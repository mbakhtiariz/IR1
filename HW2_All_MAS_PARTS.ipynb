{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval 1#\n",
    "## Assignment 2: Retrieval models [100 points] ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will get familiar with basic and advanced information retrieval concepts. You will implement different information retrieval ranking models and evaluate their performance.\n",
    "\n",
    "We provide you with a Indri index. To query the index, you'll use a Python package ([pyndri](https://github.com/cvangysel/pyndri)) that allows easy access to the underlying document statistics.\n",
    "\n",
    "For evaluation you'll use the [TREC Eval](https://github.com/usnistgov/trec_eval) utility, provided by the National Institute of Standards and Technology of the United States. TREC Eval is the de facto standard way to compute Information Retrieval measures and is frequently referenced in scientific papers.\n",
    "\n",
    "This is a **groups-of-three assignment**, the deadline is **Wednesday, January 31st**. Code quality, informative comments and convincing analysis of the results will be considered when grading. Submission should be done through blackboard, questions can be asked on the course [Piazza](piazza.com/university_of_amsterdam/spring2018/52041inr6y/home).\n",
    "\n",
    "### Technicalities (must-read!) ###\n",
    "\n",
    "The assignment directory is organized as follows:\n",
    "   * `./assignment.ipynb` (this file): the description of the assignment.\n",
    "   * `./index/`: the index we prepared for you.\n",
    "   * `./ap_88_90/`: directory with ground-truth and evaluation sets:\n",
    "      * `qrel_test`: test query relevance collection (**test set**).\n",
    "      * `qrel_validation`: validation query relevance collection (**validation set**).\n",
    "      * `topics_title`: semicolon-separated file with query identifiers and terms.\n",
    "\n",
    "You will need the following software packages (tested with Python 3.5 inside [Anaconda](https://conda.io/docs/user-guide/install/index.html)):\n",
    "   * Python 3.5 and Jupyter\n",
    "   * Indri + Pyndri (Follow the installation instructions [here](https://github.com/nickvosk/pyndri/blob/master/README.md))\n",
    "   * gensim [link](https://radimrehurek.com/gensim/install.html)\n",
    "   * TREC Eval [link](https://github.com/usnistgov/trec_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TREC Eval primer ###\n",
    "The TREC Eval utility can be downloaded and compiled as follows:\n",
    "\n",
    "    git clone https://github.com/usnistgov/trec_eval.git\n",
    "    cd trec_eval\n",
    "    make\n",
    "\n",
    "TREC Eval computes evaluation scores given two files: ground-truth information regarding relevant documents, named *query relevance* or *qrel*, and a ranking of documents for a set of queries, referred to as a *run*. The *qrel* will be supplied by us and should not be changed. For every retrieval model (or combinations thereof) you will generate a run of the top-1000 documents for every query. The format of the *run* file is as follows:\n",
    "\n",
    "    $query_identifier Q0 $document_identifier $rank_of_document_for_query $query_document_similarity $run_identifier\n",
    "    \n",
    "where\n",
    "   * `$query_identifier` is the unique identifier corresponding to a query (usually this follows a sequential numbering).\n",
    "   * `Q0` is a legacy field that you can ignore.\n",
    "   * `$document_identifier` corresponds to the unique identifier of a document (e.g., APXXXXXXX where AP denotes the collection and the Xs correspond to a unique numerical identifier).\n",
    "   * `$rank_of_document_for_query` denotes the rank of the document for the particular query. This field is ignored by TREC Eval and is only maintained for legacy support. The ranks are computed by TREC Eval itself using the `$query_document_similarity` field (see next). However, it remains good practice to correctly compute this field.\n",
    "   * `$query_document_similarity` is a score indicating the similarity between query and document where a higher score denotes greater similarity.\n",
    "   * `$run_identifier` is an identifier of the run. This field is for your own convenience and has no purpose beyond bookkeeping.\n",
    "   \n",
    "For example, say we have two queries: `Q1` and `Q2` and we rank three documents (`DOC1`, `DOC2`, `DOC3`). For query `Q1`, we find the following similarity scores `score(Q1, DOC1) = 1.0`, `score(Q1, DOC2) = 0.5`, `score(Q1, DOC3) = 0.75`; and for `Q2`: `score(Q2, DOC1) = -0.1`, `score(Q2, DOC2) = 1.25`, `score(Q1, DOC3) = 0.0`. We can generate run using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 Q0 DOC1 1 1.0 example\n",
      "Q1 Q0 DOC3 2 0.75 example\n",
      "Q1 Q0 DOC2 3 0.5 example\n",
      "Q2 Q0 DOC2 1 1.25 example\n",
      "Q2 Q0 DOC3 2 0.0 example\n",
      "Q2 Q0 DOC1 3 -0.1 example\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def write_run(model_name, data, out_f,\n",
    "              max_objects_per_query=sys.maxsize,\n",
    "              skip_sorting=False):\n",
    "    \"\"\"\n",
    "    Write a run to an output file.\n",
    "    Parameters:\n",
    "        - model_name: identifier of run.\n",
    "        - data: dictionary mapping topic_id to object_assesments;\n",
    "            object_assesments is an iterable (list or tuple) of\n",
    "            (relevance, object_id) pairs.\n",
    "            The object_assesments iterable is sorted by decreasing order.\n",
    "        - out_f: output file stream.\n",
    "        - max_objects_per_query: cut-off for number of objects per query.\n",
    "    \"\"\"\n",
    "    for subject_id, object_assesments in data.items():\n",
    "        if not object_assesments:\n",
    "            logging.warning('Received empty ranking for %s; ignoring.',\n",
    "                            subject_id)\n",
    "\n",
    "            continue\n",
    "\n",
    "        # Probe types, to make sure everything goes alright.\n",
    "        # assert isinstance(object_assesments[0][0], float) or \\\n",
    "        #     isinstance(object_assesments[0][0], np.float32)\n",
    "        assert isinstance(object_assesments[0][1], str) or \\\n",
    "            isinstance(object_assesments[0][1], bytes)\n",
    "\n",
    "        if not skip_sorting:\n",
    "            object_assesments = sorted(object_assesments, reverse=True)\n",
    "\n",
    "        if max_objects_per_query < sys.maxsize:\n",
    "            object_assesments = object_assesments[:max_objects_per_query]\n",
    "\n",
    "        if isinstance(subject_id, bytes):\n",
    "            subject_id = subject_id.decode('utf8')\n",
    "\n",
    "        for rank, (relevance, object_id) in enumerate(object_assesments):\n",
    "            if isinstance(object_id, bytes):\n",
    "                object_id = object_id.decode('utf8')\n",
    "\n",
    "            out_f.write(\n",
    "                '{subject} Q0 {object} {rank} {relevance} '\n",
    "                '{model_name}\\n'.format(\n",
    "                    subject=subject_id,\n",
    "                    object=object_id,\n",
    "                    rank=rank + 1,\n",
    "                    relevance=relevance,\n",
    "                    model_name=model_name))\n",
    "            \n",
    "# The following writes the run to standard output.\n",
    "# In your code, you should write the runs to local\n",
    "# storage in order to pass them to trec_eval.\n",
    "write_run(\n",
    "    model_name='example',\n",
    "    data={\n",
    "        'Q1': ((1.0, 'DOC1'), (0.5, 'DOC2'), (0.75, 'DOC3')),\n",
    "        'Q2': ((-0.1, 'DOC1'), (1.25, 'DOC2'), (0.0, 'DOC3')),\n",
    "    },\n",
    "    out_f=sys.stdout,\n",
    "    max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, imagine that we know that `DOC1` is relevant and `DOC3` is non-relevant for `Q1`. In addition, for `Q2` we only know of the relevance of `DOC3`. The query relevance file looks like:\n",
    "\n",
    "    Q1 0 DOC1 1\n",
    "    Q1 0 DOC3 0\n",
    "    Q2 0 DOC3 1\n",
    "    \n",
    "We store the run and qrel in files `example.run` and `example.qrel` respectively on disk. We can now use TREC Eval to compute evaluation measures. In this example, we're only interested in Mean Average Precision and we'll only show this below for brevity. However, TREC Eval outputs much more information such as NDCG, recall, precision, etc.\n",
    "\n",
    "    $ trec_eval -m all_trec -q example.qrel example.run | grep -E \"^map\\s\"\n",
    "    > map                   \tQ1\t1.0000\n",
    "    > map                   \tQ2\t0.5000\n",
    "    > map                   \tall\t0.7500\n",
    "    \n",
    "Now that we've discussed the output format of rankings and how you can compute evaluation measures from these rankings, we'll now proceed with an overview of the indexing framework you'll use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyndri primer ###\n",
    "For this assignment you will use [Pyndri](https://github.com/cvangysel/pyndri) [[1](https://arxiv.org/abs/1701.00749)], a python interface for [Indri](https://www.lemurproject.org/indri.php). We have indexed the document collection and you can query the index using Pyndri. We will start by giving you some examples of what Pyndri can do:\n",
    "\n",
    "First we read the document collection index with Pyndri:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyndri\n",
    "\n",
    "index = pyndri.Index('index/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded index can be used to access a collection of documents in an easy manner. We'll give you some examples to get some idea of what it can do, it is up to you to figure out how to use it for the remainder of the assignment.\n",
    "\n",
    "First let's look at the number of documents, since Pyndri indexes the documents using incremental identifiers we can simply take the lowest index and the maximum document and consider the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 164597 documents in this collection.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are %d documents in this collection.\" % (index.maximum_document() - index.document_base()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the first document out of the collection and take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AP890425-0001', (1360, 192, 363, 0, 880, 0, 200, 0, 894, 412, 92160, 3, 192, 0, 363, 34, 1441, 0, 174134, 0, 200, 0, 894, 412, 2652, 0, 810, 107, 49, 4903, 420, 0, 1, 48, 35, 489, 0, 35, 687, 192, 243, 0, 249311, 1877, 0, 1651, 1174, 0, 2701, 117, 412, 0, 810, 391, 245233, 1225, 5838, 16, 0, 233156, 3496, 0, 393, 17, 0, 2435, 4819, 930, 0, 0, 200, 0, 894, 0, 22, 398, 145, 0, 3, 271, 115, 0, 1176, 2777, 292, 0, 725, 192, 0, 0, 50046, 0, 1901, 1130, 0, 192, 0, 408, 0, 243779, 0, 0, 553, 192, 0, 363, 0, 3747, 0, 0, 0, 0, 1176, 0, 1239, 0, 0, 1115, 17, 0, 0, 585, 192, 1963, 0, 0, 412, 54356, 0, 773, 0, 0, 0, 192, 0, 0, 1130, 0, 363, 0, 545, 192, 0, 1174, 1901, 1130, 0, 4, 398, 145, 39, 0, 577, 0, 355, 0, 491, 0, 6025, 0, 0, 193156, 88, 34, 437, 0, 0, 1852, 0, 828, 0, 1588, 0, 0, 0, 2615, 0, 0, 107, 49, 420, 0, 0, 190, 7, 714, 2701, 0, 237, 192, 157, 0, 412, 34, 437, 0, 0, 200, 6025, 26, 0, 0, 0, 0, 363, 0, 22, 398, 145, 0, 200, 638, 126222, 6018, 0, 880, 0, 0, 161, 0, 0, 319, 894, 2701, 0, 0, 0, 301, 1200, 0, 363, 251, 430, 0, 207, 0, 76143, 1773, 0, 243779, 0, 0, 72030, 0, 55, 4903, 420, 0, 2701, 1496, 420, 0, 25480, 0, 420, 0, 0, 200, 0, 392, 2949, 0, 1738, 0, 61, 0, 71, 79, 0, 200, 903, 0, 188, 53, 6, 0, 476, 2, 0, 2028, 97, 334, 0, 0, 200, 178, 0, 0, 107, 49, 0, 214, 0, 0, 0, 114, 3866, 1505, 195, 79893, 574, 0, 198, 2160, 0, 192, 0, 420, 0, 384, 0, 2701, 0, 114, 6025, 1549, 74627, 0, 238, 0, 0, 0, 3729, 0, 192, 0, 79893, 0, 0, 729, 3141, 129, 0, 192, 196764, 39, 0, 0, 714, 63, 0, 55, 420, 3356, 0, 0, 117, 412, 0, 0, 79758, 0, 1901, 1130, 4067, 2133, 0, 0, 875, 72, 0, 0, 336, 2789, 0, 0, 25, 920, 121, 104, 0, 3162, 0, 0, 420, 0, 2178, 0, 0, 386, 192545, 159306, 0, 0, 0, 1914, 0, 200, 0, 1794, 0, 2654, 0, 0, 25480, 420, 0, 2795, 0, 0, 229690, 0, 32559, 0, 0, 392, 253919, 0, 0, 0, 0, 379, 0, 0, 114, 0, 553, 10, 0, 1128, 0, 23610, 248, 151, 0, 418, 0, 651, 0, 36, 0, 0, 645, 0, 0, 513, 0, 0, 25480, 420, 34, 0, 0, 0, 15, 0, 3348, 0, 3496, 0, 35, 687, 0, 1, 48, 0, 0, 2803, 0, 0, 714, 1274, 0, 114, 62, 1006, 70268, 1200, 2357, 0, 497, 0, 497, 125, 0, 913, 4647, 3985, 0, 0, 3370, 245233, 0, 0, 687, 0, 4, 1288, 0, 0, 0, 0, 715, 0, 0, 687, 583, 0, 0, 1627, 0, 0, 11, 357, 1359, 0, 849, 0, 0, 1518, 462, 245233, 0, 0, 0, 0, 0, 0, 171, 70268, 0))\n"
     ]
    }
   ],
   "source": [
    "example_document = index.document(index.document_base())\n",
    "print(example_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a document consists of two things, a string representing the external document identifier and an integer list representing the identifiers of words that make up the document. Pyndri uses integer representations for words or terms, thus a token_id is an integer that represents a word whereas the token is the actual text of the word/term. Every id has a unique token and vice versa with the exception of stop words: words so common that there are uninformative, all of these receive the zero id.\n",
    "\n",
    "To see what some ids and their matching tokens we take a look at the dictionary of the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'new'), (2, 'percent'), (3, 'two'), (4, '1'), (5, 'people'), (6, 'million'), (7, '000'), (8, 'government'), (9, 'president'), (10, 'years'), (11, 'state'), (12, '2'), (13, 'states'), (14, 'three'), (15, 'time'), (16, 'officials'), (17, 'police'), (18, 'bush'), (19, 'soviet'), (20, 'united'), (21, 'national'), (22, '5'), (23, '3'), (24, 'house'), (25, 'american'), (26, 'told'), (27, '10'), (28, 'billion'), (29, 'today'), (30, 'federal'), (31, 'court'), (32, 'made'), (33, 'party'), (34, 'tuesday'), (35, 'city'), (36, 'wednesday'), (37, 'says'), (38, 'company'), (39, 'monday'), (40, 'say'), (41, 'thursday'), (42, 'old'), (43, '4'), (44, 'news'), (45, 'west'), (46, 'did'), (47, 'friday'), (48, 'york'), (49, 'department'), (50, 'four'), (51, 'group'), (52, 'report'), (53, '6'), (54, 'south'), (55, 'military'), (56, 'former'), (57, 'market'), (58, 'called'), (59, 'reported'), (60, 'home'), (61, 'make'), (62, 'spokesman'), (63, 'union'), (64, 'five'), (65, 'month'), (66, '7'), (67, '30'), (68, 'high'), (69, 'foreign'), (70, 'back'), (71, 'work'), (72, 'members'), (73, 'world'), (74, '8'), (75, 'stock'), (76, '20'), (77, 'law'), (78, 'political'), (79, 'public'), (80, 'long'), (81, 'going'), (82, 'war'), (83, 'dont'), (84, 'general'), (85, 'official'), (86, 'think'), (87, 'killed'), (88, 'office'), (89, 'take'), (90, 'country'), (91, 'days'), (92, 'nations'), (93, 'reagan'), (94, 'drug'), (95, 'way'), (96, 'part'), (97, 'money'), (98, 'found'), (99, '15'), (100, 'months'), (101, 'dukakis'), (102, 'washington'), (103, 'air'), (104, 'committee'), (105, 'case'), (106, 'white'), (107, 'defense'), (108, 'six'), (109, 'prices'), (110, 'congress'), (111, 'ago'), (112, 'north'), (113, 'based'), (114, 'school'), (115, 'later'), (116, 'sunday'), (117, 'administration'), (118, 'campaign'), (119, 'program'), (120, 'fire'), (121, 'service'), (122, '12'), (123, 'east'), (124, 'business'), (125, 'meeting'), (126, 'man'), (127, 'minister'), (128, 'economic'), (129, 'began'), (130, 'major'), (131, '50'), (132, 'asked'), (133, '9'), (134, 'chief'), (135, '1987'), (136, 'workers'), (137, 'saying'), (138, 'end'), (139, 'late'), (140, 'bank'), (141, 'earlier'), (142, 'international'), (143, 'co'), (144, 'good'), (145, 'm'), (146, 'expected'), (147, 'early'), (148, 'help'), (149, 'plan'), (150, 'miles'), (151, 'board'), (152, 'trade'), (153, 'democratic'), (154, 'leader'), (155, 'john'), (156, 'oil'), (157, 'left'), (158, 'agency'), (159, 'secretary'), (160, 'night'), (161, 'second'), (162, 'leaders'), (163, 'support'), (164, 'chairman'), (165, 'security'), (166, 'army'), (167, 'least'), (168, 'death'), (169, 'higher'), (170, 'life'), (171, 'set'), (172, 'know'), (173, '100'), (174, 'children'), (175, 'senate'), (176, 'held'), (177, 'march'), (178, 'came'), (179, 'force'), (180, 'right'), (181, 'power'), (182, 'central'), (183, 'judge'), (184, 'bill'), (185, 'show'), (186, 'family'), (187, 'exchange'), (188, '11'), (189, 'texas'), (190, '25'), (191, 'system'), (192, 'students'), (193, 'took'), (194, 'cents'), (195, 'd'), (196, 'black'), (197, 'number'), (198, 'statement'), (199, 'near'), (200, 'university')]\n"
     ]
    }
   ],
   "source": [
    "token2id, id2token, _ = index.get_dictionary()\n",
    "print(list(id2token.items())[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this dictionary we can see the tokens for the (non-stop) words in our example document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['52', 'students', 'arrested', 'takeover', 'university', 'massachusetts', 'building', 'fifty', 'two', 'students', 'arrested', 'tuesday', 'evening', 'occupying', 'university', 'massachusetts', 'building', 'overnight', 'protest', 'defense', 'department', 'funded', 'research', 'new', 'york', 'city', 'thousands', 'city', 'college', 'students', 'got', 'unscheduled', 'holiday', 'demonstrators', 'occupied', 'campus', 'administration', 'building', 'protest', 'possible', 'tuition', 'increases', 'prompting', 'officials', 'suspend', 'classes', '60', 'police', 'riot', 'gear', 'arrived', 'university', 'massachusetts', '5', 'p', 'm', 'two', 'hours', 'later', 'bus', 'drove', 'away', '29', 'students', 'camped', 'memorial', 'hall', 'students', 'charged', 'trespassing', '23', 'students', 'arrested', 'lying', 'bus', 'prevent', 'leaving', 'police', '300', 'students', 'stood', 'building', 'chanting', 'looking', 'students', 'hall', 'arrested', '35', 'students', 'occupied', 'memorial', 'hall', '1', 'p', 'm', 'monday', 'declined', 'offer', 'meet', 'administrators', 'provosts', 'office', 'tuesday', 'morning', 'presented', 'list', 'demands', 'halt', 'defense', 'department', 'research', '25', '000', 'student', 'campus', '40', 'students', 'left', 'building', 'tuesday', 'morning', 'university', 'administrators', 'told', 'arrested', '5', 'p', 'm', 'university', 'spokeswoman', 'jeanne', 'hopkins', 'takeover', 'second', 'western', 'massachusetts', 'campus', 'seven', 'protesters', 'arrested', 'april', '19', 'charges', 'disorderly', 'conduct', 'trespassing', 'demonstrating', 'military', 'funded', 'research', 'campus', 'particularly', 'research', 'anthrax', 'research', 'university', 'non', 'classified', 'researchers', 'make', 'work', 'public', 'university', 'rules', '11', '6', 'million', '22', 'percent', 'grant', 'money', 'received', 'university', 'came', 'defense', 'department', '1988', 'school', 'chancellor', 'joseph', 'd', 'duffey', 'issued', 'statement', 'telling', 'students', 'research', 'continue', 'campus', 'school', 'administrators', 'decide', 'differently', 'policy', 'negotiated', 'students', 'duffey', 'latest', 'occupation', 'began', 'students', 'rallying', 'monday', 'student', 'union', 'military', 'research', 'marched', 'administration', 'building', 'ducked', 'memorial', 'hall', 'en', 'route', 'followed', 'members', 'local', 'chapter', 'american', 'friends', 'service', 'committee', 'contended', 'research', 'dangerous', 'town', 'promotes', 'militarism', 'banned', 'university', 'argued', 'purpose', 'anthrax', 'research', 'peaceful', 'strain', 'bacteria', 'non', 'virulent', 'study', 'school', '23', 'years', 'incident', 'amherst', 'health', 'board', 'scheduled', 'hearing', 'wednesday', 'question', 'safety', 'anthrax', 'research', 'tuesday', 'time', '1969', 'classes', 'city', 'college', 'new', 'york', 'canceled', 'student', 'protests', 'school', 'spokesman', 'charles', 'deciccio', 'protesters', 'demanding', 'face', 'face', 'meeting', 'gov', 'mario', 'cuomo', 'feared', 'tuition', 'college', '1', '250', 'increased', 'college', 'staff', 'reduced', 'state', 'budget', 'cuts', 'governor', 'immediate', 'comment', 'tuition', 'set', 'deciccio']\n"
     ]
    }
   ],
   "source": [
    "print([id2token[word_id] for word_id in example_document[1] if word_id > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reverse can also be done, say we want to look for news about the \"University of Massachusetts\", the tokens of that query can be converted to ids using the reverse dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query by tokens: ['university', '', 'massachusetts']\n",
      "Query by ids with stopwords: [200, 0, 894]\n",
      "Query by ids without stopwords: [200, 894]\n"
     ]
    }
   ],
   "source": [
    "query_tokens = index.tokenize(\"University of Massachusetts\")\n",
    "print(\"Query by tokens:\", query_tokens)\n",
    "query_id_tokens = [token2id.get(query_token,0) for query_token in query_tokens]\n",
    "print(\"Query by ids with stopwords:\", query_id_tokens)\n",
    "query_id_tokens = [word_id for word_id in query_id_tokens if word_id > 0]\n",
    "print(\"Query by ids without stopwords:\", query_id_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally we can now match the document and query in the id space, let's see how often a word from the query occurs in our example document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document AP890425-0001 has 13 word matches with query: \"university  massachusetts\".\n",
      "Document AP890425-0001 and query \"university  massachusetts\" have a 2.5% overlap.\n"
     ]
    }
   ],
   "source": [
    "matching_words = sum([True for word_id in example_document[1] if word_id in query_id_tokens])\n",
    "print(\"Document %s has %d word matches with query: \\\"%s\\\".\" % (example_document[0], matching_words, ' '.join(query_tokens)))\n",
    "print(\"Document %s and query \\\"%s\\\" have a %.01f%% overlap.\" % (example_document[0], ' '.join(query_tokens),matching_words/float(len(example_document[1]))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is certainly not everything Pyndri can do, it should give you an idea of how to use it. Please take a look at the [examples](https://github.com/cvangysel/pyndri) as it will help you a lot with this assignment.\n",
    "\n",
    "**CAUTION**: Avoid printing out the whole index in this Notebook as it will generate a lot of output and is likely to corrupt the Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the query file\n",
    "You can parse the query file (`ap_88_89/topics_title`) using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('51', 'Airbus Subsidies'), ('52', 'South African Sanctions'), ('53', 'Leveraged Buyouts'), ('54', 'Satellite Launch Contracts'), ('55', 'Insider Trading'), ('56', 'Prime (Lending) Rate Moves, Predictions'), ('57', 'MCI'), ('58', 'Rail Strikes'), ('59', 'Weather Related Fatalities'), ('60', 'Merit-Pay vs. Seniority'), ('61', 'Israeli Role in Iran-Contra Affair'), ('62', \"Military Coups D'etat\"), ('63', 'Machine Translation'), ('64', 'Hostage-Taking'), ('65', 'Information Retrieval Systems'), ('66', 'Natural Language Processing'), ('67', 'Politically Motivated Civil Disturbances'), ('68', 'Health Hazards from Fine-Diameter Fibers'), ('69', 'Attempts to Revive the SALT II Treaty'), ('70', 'Surrogate Motherhood'), ('71', 'Border Incursions'), ('72', 'Demographic Shifts in the U.S.'), ('73', 'Demographic Shifts across National Boundaries'), ('74', 'Conflicting Policy'), ('75', 'Automation'), ('76', 'U.S. Constitution - Original Intent'), ('77', 'Poaching'), ('78', 'Greenpeace'), ('79', 'FRG Political Party Positions'), ('80', '1988 Presidential Candidates Platforms'), ('81', 'Financial crunch for televangelists in the wake of the PTL scandal'), ('82', 'Genetic Engineering'), ('83', 'Measures to Protect the Atmosphere'), ('84', 'Alternative/renewable Energy Plant & Equipment Installation'), ('85', 'Official Corruption'), ('86', 'Bank Failures'), ('87', 'Criminal Actions Against Officers of Failed Financial Institutions'), ('88', 'Crude Oil Price Trends'), ('89', '\"Downstream\" Investments by OPEC Member States'), ('90', 'Data on Proven Reserves of Oil & Natural Gas Producers'), ('91', 'U.S. Army Acquisition of Advanced Weapons Systems'), ('92', 'International Military Equipment Sales'), ('93', 'What Backing Does the National Rifle Association Have?'), ('94', 'Computer-aided Crime'), ('95', 'Computer-aided Crime Detection'), ('96', 'Computer-Aided Medical Diagnosis'), ('97', 'Fiber Optics Applications'), ('98', 'Fiber Optics Equipment Manufacturers'), ('99', 'Iran-Contra Affair'), ('100', 'Controlling the Transfer of High Technology'), ('101', 'Design of the \"Star Wars\" Anti-missile Defense System'), ('102', \"Laser Research Applicable to the U.S.'s Strategic Defense Initiative\"), ('103', 'Welfare Reform'), ('104', 'Catastrophic Health Insurance'), ('105', '\"Black Monday\"'), ('106', 'U.S. Control of Insider Trading'), ('107', 'Japanese Regulation of Insider Trading'), ('108', 'Japanese Protectionist Measures'), ('109', 'Find Innovative Companies'), ('110', 'Black Resistance Against the South African Government'), ('111', 'Nuclear Proliferation'), ('112', 'Funding Biotechnology'), ('113', 'New Space Satellite Applications'), ('114', 'Non-commercial Satellite Launches'), ('115', 'Impact of the 1986 Immigration Law'), ('116', 'Generic Drug Substitutions'), ('117', 'Capacity of the U.S. Cellular Telephone Network'), ('118', 'International Terrorists'), ('119', 'Actions Against International Terrorists'), ('120', 'Economic Impact of International Terrorism'), ('121', 'Death from Cancer'), ('122', 'RDT&E of New Cancer Fighting Drugs'), ('123', 'Research into & Control of Carcinogens'), ('124', 'Alternatives to Traditional Cancer Therapies'), ('125', 'Anti-smoking Actions by Government'), ('126', 'Medical Ethics and Modern Technology'), ('127', 'U.S.-U.S.S.R. Arms Control Agreements'), ('128', 'Privatization of State Assets'), ('129', 'Soviet Spying on the U.S.'), ('130', 'Jewish Emigration and U.S.-USSR Relations'), ('131', 'McDonnell Douglas Contracts for Military Aircraft'), ('132', '\"Stealth\" Aircraft'), ('133', 'Hubble Space Telescope'), ('134', 'The Human Genome Project'), ('135', 'Possible Contributions of Gene Mapping to Medicine'), ('136', 'Diversification by Pacific Telesis'), ('137', 'Expansion in the U.S. Theme Park Industry'), ('138', 'Iranian Support for Lebanese Hostage-takers'), ('139', \"Iran's Islamic Revolution - Domestic and Foreign Social Consequences\"), ('140', 'Political Impact of Islamic Fundamentalism'), ('141', \"Japan's Handling of its Trade Surplus with the U.S.\"), ('142', 'Impact of Government Regulated Grain Farming on International Relations'), ('143', 'Why Protect U.S. Farmers?'), ('144', 'Management Problems at the United Nations'), ('145', 'Influence of the \"Pro-Israel Lobby\"'), ('146', 'Negotiating an End to the Nicaraguan Civil War'), ('147', 'Productivity Trends in the U.S. Economy'), ('148', 'Conflict in the Horn of Africa'), ('149', 'Industrial Espionage'), ('150', 'U.S. Political Campaign Financing'), ('151', 'Coping with overcrowded prisons'), ('152', 'Accusations of Cheating by Contractors on U.S. Defense Projects'), ('153', 'Insurance Coverage which pays for Long Term Care'), ('154', 'Oil Spills'), ('155', 'Right Wing Christian Fundamentalism in U.S.'), ('156', 'Efforts to enact Gun Control Legislation'), ('157', 'Causes and treatments of multiple sclerosis (MS)'), ('158', 'Term limitations for members of the U.S. Congress'), ('159', 'Electric Car Development'), ('160', 'Vitamins - The Cure for or Cause of Human Ailments'), ('161', 'Acid Rain'), ('162', 'Automobile Recalls'), ('163', 'Vietnam Veterans and Agent Orange'), ('164', 'Generic Drugs - Illegal Activities by Manufacturers'), ('165', 'Tobacco company advertising and the young'), ('166', 'Standardized testing and cultural bias'), ('167', 'Regulation of the showing of violence and explicit sex in motion picture theaters, on television, and on video cassettes.'), ('168', 'Financing AMTRAK'), ('169', 'Cost of Garbage/Trash Removal'), ('170', 'The Consequences of Implantation of Silicone Gel Breast Devices'), ('171', \"Use of Mutual Funds in an Individual's Retirement Strategy\"), ('172', 'The Effectiveness of Medical Products and Related Programs Utilized in the Cessation of Smoking.'), ('173', 'Smoking Bans'), ('174', 'Hazardous Waste Cleanup'), ('175', 'NRA Prevention of Gun Control Legislation'), ('176', 'Real-life private investigators'), ('177', 'English as the Official Language in U.S.'), ('178', 'Dog Maulings'), ('179', 'U. S. Restaurants in Foreign Lands'), ('180', 'Ineffectiveness of U.S. Embargoes/Sanctions'), ('181', 'Abuse of the Elderly by Family Members, and Medical and Nonmedical Personnel, and Initiatives Being Taken to Minimize This Mistreatment'), ('182', 'Commercial Overfishing Creates Food Fish Deficit'), ('183', 'Asbestos Related Lawsuits'), ('184', 'Corporate Pension Plans/Funds'), ('185', 'Reform of the U.S. Welfare System'), ('186', 'Difference of Learning Levels Among Inner City and More Suburban School Students'), ('187', 'Signs of the Demise of Independent Publishing'), ('188', 'Beachfront Erosion'), ('189', 'Real Motives for Murder'), ('190', 'Instances of Fraud Involving the Use of a Computer'), ('191', 'Efforts to Improve U.S. Schooling'), ('192', 'Oil Spill Cleanup'), ('193', 'Toys R Dangerous'), ('194', 'The Amount of Money Earned by Writers'), ('195', 'Stock Market Perturbations Attributable to Computer Initiated Trading'), ('196', 'School Choice Voucher System and its effects upon the entire U.S. educational program'), ('197', 'Reform of the jurisprudence system to stop juries from granting unreasonable monetary awards'), ('198', 'Gene Therapy and Its Benefits to Humankind'), ('199', 'Legality of Medically Assisted Suicides'), ('200', 'Impact of foreign textile imports on U.S. textile industry')])\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import io\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "def parse_topics(file_or_files,\n",
    "                 max_topics=sys.maxsize, delimiter=';'):\n",
    "    assert max_topics >= 0 or max_topics is None\n",
    "\n",
    "    topics = collections.OrderedDict()\n",
    "\n",
    "    if not isinstance(file_or_files, list) and \\\n",
    "            not isinstance(file_or_files, tuple):\n",
    "        if hasattr(file_or_files, '__iter__'):\n",
    "            file_or_files = list(file_or_files)\n",
    "        else:\n",
    "            file_or_files = [file_or_files]\n",
    "\n",
    "    for f in file_or_files:\n",
    "        assert isinstance(f, io.IOBase)\n",
    "\n",
    "        for line in f:\n",
    "            assert(isinstance(line, str))\n",
    "\n",
    "            line = line.strip()\n",
    "\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            topic_id, terms = line.split(delimiter, 1)\n",
    "\n",
    "            if topic_id in topics and (topics[topic_id] != terms):\n",
    "                    logging.error('Duplicate topic \"%s\" (%s vs. %s).',\n",
    "                                  topic_id,\n",
    "                                  topics[topic_id],\n",
    "                                  terms)\n",
    "\n",
    "            topics[topic_id] = terms\n",
    "\n",
    "            if max_topics > 0 and len(topics) >= max_topics:\n",
    "                break\n",
    "\n",
    "    return topics\n",
    "\n",
    "with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "    print(parse_topics([f_topics]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement and compare lexical IR methods [35 points] ### \n",
    "\n",
    "In this task you will implement a number of lexical methods for IR using the **Pyndri** framework. Then you will evaluate these methods on the dataset we have provided using **TREC Eval**.\n",
    "\n",
    "Use the **Pyndri** framework to get statistics of the documents (term frequency, document frequency, collection frequency; **you are not allowed to use the query functionality of Pyndri**) and implement the following scoring methods in **Python**:\n",
    "\n",
    "- [TF-IDF](http://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html) and \n",
    "- [BM25](http://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html) with k1=1.2 and b=0.75. **[5 points]**\n",
    "- Language models ([survey](https://drive.google.com/file/d/0B-zklbckv9CHc0c3b245UW90NE0/view))\n",
    "    - Jelinek-Mercer (explore different values of ùõå in the range [0.1, 0.5, 0.9]). **[5 points]**\n",
    "    - Dirichlet Prior (explore different values of ùõç [500, 1000, 1500]). **[5 points]**\n",
    "    - Absolute discounting (explore different values of ùõÖ in the range [0.1, 0.5, 0.9]). **[5 points]**\n",
    "    - [Positional Language Models](http://sifaka.cs.uiuc.edu/~ylv2/pub/sigir09-plm.pdf) define a language model for each position of a document, and score a document based on the scores of its PLMs. The PLM is estimated based on propagated counts of words within a document through a proximity-based density function, which both captures proximity heuristics and achieves an effect of ‚Äúsoft‚Äù passage retrieval. Implement the PLM, all five kernels, but only the Best position strategy to score documents. Use ùõî equal to 50, and Dirichlet smoothing with ùõç optimized on the validation set (decide how to optimize this value yourself and motivate your decision in the report). **[10 points]**\n",
    "    \n",
    "Implement the above methods and report evaluation measures (on the test set) using the hyper parameter values you optimized on the validation set (also report the values of the hyper parameters). Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "For the language models, create plots showing `NDCG@10` with varying values of the parameters. You can do this by chaining small scripts using shell scripting (preferred) or execute trec_eval using Python's `subprocess`.\n",
    "\n",
    "Compute significance of the results using a [two-tailed paired Student t-test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html) **[5 points]**. Be wary of false rejection of the null hypothesis caused by the [multiple comparisons problem](https://en.wikipedia.org/wiki/Multiple_comparisons_problem). There are multiple ways to mitigate this problem and it is up to you to choose one.\n",
    "\n",
    "Analyse the results by identifying specific queries where different methods succeed or fail and discuss possible reasons that cause these differences. This is *very important* in order to understand who the different retrieval functions behave.\n",
    "\n",
    "**NOTE**: Don‚Äôt forget to use log computations in your calculations to avoid underflows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: You should structure your code around the helper functions we provide below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import collections\n",
    "from collections import Counter\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering statistics about 456 terms.\n",
      "Inverted index creation took 47.43765616416931 seconds.\n"
     ]
    }
   ],
   "source": [
    "#with open('./ap_88_89/test_topic', 'r') as f_topics:\n",
    "with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "    queries = parse_topics([f_topics])\n",
    "\n",
    "index = pyndri.Index('index/')\n",
    "\n",
    "num_documents = index.maximum_document() - index.document_base()\n",
    "\n",
    "dictionary = pyndri.extract_dictionary(index)\n",
    "\n",
    "tokenized_queries = {\n",
    "    query_id: [dictionary.translate_token(token)\n",
    "               for token in index.tokenize(query_string)\n",
    "               if dictionary.has_token(token)]\n",
    "    for query_id, query_string in queries.items()}\n",
    "\n",
    "query_term_ids = set(\n",
    "    query_term_id\n",
    "    for query_term_ids in tokenized_queries.values()\n",
    "    for query_term_id in query_term_ids)\n",
    "\n",
    "print('Gathering statistics about', len(query_term_ids), 'terms.')\n",
    "\n",
    "# inverted index creation.\n",
    "\n",
    "document_lengths = {}\n",
    "unique_terms_per_document = {}\n",
    "\n",
    "inverted_index = collections.defaultdict(dict)\n",
    "collection_frequencies = collections.defaultdict(int)\n",
    "\n",
    "total_terms = 0\n",
    "\n",
    "#______________________we added a list to keep the ext_doc and the tokens in every doc___________\n",
    "doc_list = []\n",
    "max_doc_len = 0\n",
    "ex_to_id = {}\n",
    "\n",
    "for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "    ext_doc_id, doc_token_ids = index.document(int_doc_id)\n",
    "    \n",
    "    #--------------------------------------\n",
    "    doc_list.append((ext_doc_id,doc_token_ids))\n",
    "    if(len(doc_token_ids) > max_doc_len):\n",
    "        max_doc_len = len(doc_token_ids)\n",
    "    ex_to_id[ext_doc_id] = int_doc_id\n",
    "    #--------------------------------------\n",
    "    document_bow = collections.Counter(\n",
    "        token_id for token_id in doc_token_ids\n",
    "        if token_id > 0)\n",
    "    document_length = sum(document_bow.values())\n",
    "\n",
    "    document_lengths[int_doc_id] = document_length\n",
    "    total_terms += document_length\n",
    "\n",
    "    unique_terms_per_document[int_doc_id] = len(document_bow)\n",
    "\n",
    "    for query_term_id in query_term_ids:\n",
    "        assert query_term_id is not None\n",
    "\n",
    "        document_term_frequency = document_bow.get(query_term_id, 0)\n",
    "\n",
    "        if document_term_frequency == 0:\n",
    "            continue\n",
    "\n",
    "        collection_frequencies[query_term_id] += document_term_frequency\n",
    "        inverted_index[query_term_id][int_doc_id] = document_term_frequency\n",
    "\n",
    "avg_doc_length = total_terms / num_documents\n",
    "\n",
    "print('Inverted index creation took', time.time() - start_time, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_retrieval(model_name, score_fn, hyper_param):\n",
    "    \"\"\"\n",
    "    Runs a retrieval method for all the queries and writes the TREC-friendly results in a file.\n",
    "    \n",
    "    :param model_name: the name of the model (a string)\n",
    "    :param score_fn: the scoring function (a function - see below for an example) \n",
    "    \"\"\"\n",
    "    #-----------------------------------------\n",
    "    if model_name != 'tfidf' and model_name != 'BM25':\n",
    "        model_name = model_name + str(hyper_param)\n",
    "    #-----------------------------------------\n",
    "    run_out_path = str('models/eval/') + '{}.run'.format(model_name)\n",
    "\n",
    "    if os.path.exists(run_out_path):\n",
    "        return\n",
    "\n",
    "    retrieval_start_time = time.time()\n",
    "\n",
    "\n",
    "    print('Retrieving using', model_name)\n",
    "\n",
    "    data = collections.defaultdict(list)\n",
    "\n",
    "    # TODO: fill the data dictionary. \n",
    "    # The dictionary data should have the form: query_id --> (document_score, external_doc_id)\n",
    "    \n",
    "    #for query_id, _ in queries.items():\n",
    "    for query_id, query_terms in tokenized_queries.items():\n",
    "        \n",
    "        score_per_doc = Counter()\n",
    "        '''\n",
    "        We do not go through all documents, we just go through the all documents relate to query term.\n",
    "        For example if we have 5 docs and Q1 = w1 w2 and w1 is inside the doc1 & doc2, w2 is inside doc3\n",
    "        , then we go through doc1, doc2, doc3\n",
    "        '''        \n",
    "        #------------------------docs_to_check = sum of all documents for q:-------------------------\n",
    "        \n",
    "        docs_to_check = set([inverted_index[term_id].keys() for term_id in query_terms][0])\n",
    "        \n",
    "        '''\n",
    "        We check the score for every term of a query for every doc related to that query and assign some score\n",
    "        to them.\n",
    "        In tf-idf when the term is not inside the that doc, we return 0.\n",
    "        In the smoothing methods, we assign some probability to the unseen terms.\n",
    "        '''\n",
    "        #--------------------------find the score(query, doc)-----------------------------------------\n",
    "        #for query_term_id in tokenized_queries[query_id]:\n",
    "        for query_term_id in query_terms:\n",
    "            for int_doc_id in docs_to_check:\n",
    "                document_term_freq = get_tf(int_doc_id, query_term_id)\n",
    "                score_per_doc[int_doc_id] += score_fn(int_doc_id, query_term_id, document_term_freq,hyper_param)\n",
    "                \n",
    "        #------------------------make data set to write in run file------------------------------------        \n",
    "        for int_doc_id in score_per_doc:\n",
    "            data[query_id].append((score_per_doc[int_doc_id], doc_list[int_doc_id - 1][0]))\n",
    "\n",
    "    \n",
    "    with open(run_out_path, 'w') as f_out:\n",
    "        write_run(\n",
    "            model_name=model_name,\n",
    "            data=data,\n",
    "            out_f=f_out,\n",
    "            max_objects_per_query=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf(int_document_id, query_term_id):\n",
    "    '''\n",
    "    Returns term frequency (tf_t) for a document\n",
    "    https://docs.quantifiedcode.com/python-anti-patterns/correctness/not_using_get_to_return_a_default_value_from_a_dictionary.html    \n",
    "    '''\n",
    "    if len(inverted_index[query_term_id]) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return float(inverted_index.get(query_term_id, 0).get(int_document_id, 0))\n",
    "\n",
    "def collection_freq(query_term_id):\n",
    "    #Returns collection frequency\n",
    "    return collection_frequencies.get(query_term_id, 0)\n",
    "\n",
    "def calc_p_wc(query_term_id):\n",
    "    p_wc = collection_freq(query_term_id) / total_terms\n",
    "    return p_wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF without normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(int_document_id, query_term_id, document_term_freq,num_docs):\n",
    "    \"\"\"\n",
    "    Scoring function for a document and a query term\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query_token_id: the query term id (assuming you have split the query to tokens)\n",
    "    :param document_term_freq: the document term frequency of the query term \n",
    "    \"\"\"    \n",
    "    score = document_term_freq * math.log10(num_docs/len(inverted_index[query_term_id]))\n",
    "    return score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_documents = index.maximum_document() - index.document_base()\n",
    "run_retrieval('tfidf', tfidf,num_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BM25 with k1=1.2 and b=0.75:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BM25(int_document_id, query_term_id, document_term_freq,hyper_param):\n",
    "\n",
    "    '''\n",
    "    BM25 with document length normalization\n",
    "    tf = document_term_freq\n",
    "    l_avg = avg_doc_length\n",
    "    l_d = document_lengths[int_document_id]\n",
    "    idf = math.log10(num_documents/len(inverted_index[query_term_id])\n",
    "    '''\n",
    "    k1=hyper_param[0]\n",
    "    b=hyper_param[1]\n",
    "\n",
    "\n",
    "    w_t_numerator = (k1+1) * document_term_freq * math.log10(num_documents / len(inverted_index[query_term_id]))\n",
    "    w_t_denominator = (k1 * (1-b + (b * document_lengths[int_document_id] / avg_doc_length))) + document_term_freq\n",
    "\n",
    "    w_t =  w_t_numerator/w_t_denominator\n",
    "    \n",
    "    return w_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''    \n",
    "k1=1.2\n",
    "b=0.75\n",
    "'''\n",
    "hyper_param = [1.2,0.75]\n",
    "run_retrieval('BM25', BM25,hyper_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jelinek-Mercer at ùõå = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jelinek_mercer(int_document_id, query_term_id, document_term_freq,lmbda):\n",
    "    \n",
    "    tf_wd = document_term_freq\n",
    "    d = document_lengths[int_document_id]\n",
    "    p_wC = calc_p_wc(query_term_id)\n",
    "    \n",
    "    pbt_lambda = (1 - lmbda)*(p_wC) + lmbda*(tf_wd/d)\n",
    "    return np.log10(pbt_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hyper_param in np.arange(0, 1, 0.1): \n",
    "    hyper_param = round(hyper_param, 1)\n",
    "    run_retrieval('jelinek_mercer', jelinek_mercer,hyper_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Dirichelet at ùõç = [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dirichlet(int_document_id, query_term_id, document_term_freq, mu):\n",
    "    '''\n",
    "    score(d,q) = log p(q|d) = sum(log p_mu(w_i|d))\n",
    "    p_mu(w_i|d) = [tf[w,d] + mu*p(w|C)] / [len_d + mu]\n",
    "    '''    \n",
    "    p_wd_numerator = document_term_freq + (mu * calc_p_wc(query_term_id)) \n",
    "    p_wd_denominator = document_lengths[int_document_id] + mu #led_d + mu\n",
    "    p_wd = p_wd_numerator / p_wd_denominator    \n",
    "    score =  math.log10(p_wd)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hyper_param in range(500,5500, 500):\n",
    "    run_retrieval('dirichlet', dirichlet, hyper_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Absolute Discounting at ùõÖ =  [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#formula from : https://dl.acm.org/citation.cfm?id=384019\n",
    "def absolute_discounting(int_document_id, query_term_id, document_term_freq, delta):\n",
    "    \n",
    "    tf_wd = document_term_freq\n",
    "    d = document_lengths[int_document_id]\n",
    "    #d_u = number of unique words in d\n",
    "    d_u = unique_terms_per_document[int_document_id]\n",
    "    p_wC = calc_p_wc(query_term_id)\n",
    "    \n",
    "    #seen words probability\n",
    "    seen_words = max((tf_wd - delta), 0)/d\n",
    "    \n",
    "    #unseen words probability\n",
    "    unseen_words = delta*(d_u/d)*p_wC\n",
    "\n",
    "    return (seen_words + unseen_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hyper_param in np.arange(0, 1, 0.1):\n",
    "    hyper_param = round(hyper_param, 1)\n",
    "    run_retrieval('absolute_discounting', absolute_discounting,hyper_param)\n",
    "#run_retrieval('absolute_discounting 0.5', absolute_discounting_05)\n",
    "#run_retrieval('absolute_discounting 0.9', absolute_discounting_09)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PLM with ùõî = 50, using Dirichlet smoothing with ùõç = 3000, scoring with KL-divergence and Best position strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gaus_kernel  creation took 0.1229560375213623 seconds.\n",
      "triangle_kernel creation took 0.0016252994537353516 seconds.\n",
      "cosine_kernel creation took 0.0015964508056640625 seconds.\n",
      "circle_kernel creation took 0.0015833377838134766 seconds.\n",
      "passage_kernel creation took 0.001497507095336914 seconds.\n",
      "p_wQ_dict creation took 0.0011429786682128906 seconds.\n"
     ]
    }
   ],
   "source": [
    "#precalc kernels:\n",
    "def precaculate_kernel(kernel_func,max_doc_len, sig):\n",
    "    precomputed_list = []\n",
    "    for delta_position in range(max_doc_len):\n",
    "        precomputed_list.append(kernel_func(delta_position,sig))\n",
    "    return precomputed_list\n",
    "        \n",
    "def guas_kernel(delta_position,sig):\n",
    "    k = math.exp(-1 * ((delta_position)**2) / (2*(sig**2)))\n",
    "    return k        \n",
    "        \n",
    "    \n",
    "def triangle_kernel(delta_position,sig):\n",
    "    if delta_position <= sig:\n",
    "        return (1 - (delta_position/float(sig)))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def cosine_kernel(delta_position,sig):\n",
    "    if delta_position <= sig:\n",
    "        return 0.5 * (1 + math.cos(delta_position *math.pi/float(sig)))\n",
    "    else:\n",
    "        return 0  \n",
    "    \n",
    "def circle_kernel(delta_position,sig):\n",
    "    if delta_position <= sig:\n",
    "        return math.sqrt(1 - (delta_position/float(sig))**2)\n",
    "    else:\n",
    "        return 0    \n",
    "    \n",
    "    \n",
    "def passage_kernel(delta_position,sig):\n",
    "    if delta_position <= sig:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "#---------------------------------------------------------------------------------------\n",
    "#--------make a dict of dicts containing query_id : p(w|Q) for the that query-----------\n",
    "def make_p_WQ_dict(query_tokens):\n",
    "    p_wQ_dict = {}\n",
    "    for query_id, query_terms in query_tokens.items(): \n",
    "        p_wQ_dict[query_id] = {query_term_id : float(query_terms.count(query_term_id)/len(query_terms)) \n",
    "                               for query_term_id in query_terms}\n",
    "    return p_wQ_dict\n",
    "#---------------------------------------------------------------------------------------\n",
    "sig = 50\n",
    "\n",
    "start_time = time.time()\n",
    "precomputed_gaus = precaculate_kernel(guas_kernel,max_doc_len, sig)\n",
    "print('gaus_kernel  creation took', time.time() - start_time, 'seconds.')\n",
    "\n",
    "start_time = time.time()\n",
    "precomputed_triangle = precaculate_kernel(triangle_kernel,max_doc_len, sig)\n",
    "print('triangle_kernel creation took', time.time() - start_time, 'seconds.')\n",
    "\n",
    "start_time = time.time()\n",
    "precomputed_cosine = precaculate_kernel(cosine_kernel,max_doc_len, sig)\n",
    "print('cosine_kernel creation took', time.time() - start_time, 'seconds.')\n",
    "\n",
    "start_time = time.time()\n",
    "precomputed_circle = precaculate_kernel(circle_kernel,max_doc_len, sig)\n",
    "print('circle_kernel creation took', time.time() - start_time, 'seconds.')\n",
    "\n",
    "start_time = time.time()\n",
    "precomputed_passage = precaculate_kernel(passage_kernel,max_doc_len, sig)\n",
    "print('passage_kernel creation took', time.time() - start_time, 'seconds.')\n",
    "#---------------------------------------------------------------------------\n",
    "start_time = time.time()\n",
    "p_wQ_dict = make_p_WQ_dict(tokenized_queries)\n",
    "print('p_wQ_dict creation took', time.time() - start_time, 'seconds.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_position_score(word_pos_in_doc, current_position , mu, p_wQ, precomputed_kernel):\n",
    "    \n",
    "    cwi_prime_matrix = Counter()    \n",
    "    for doc_info in word_pos_in_doc:\n",
    "        j = doc_info[0]\n",
    "        delta_position = abs(current_position - j)\n",
    "        cwi_prime_matrix[doc_info[1]] += precomputed_kernel[delta_position]\n",
    "    z_i = sum(cwi_prime_matrix.values())    \n",
    "\n",
    "    score_q_i =0        \n",
    "    for word_id in p_wQ.keys():\n",
    "        p_wd_numerator = cwi_prime_matrix.get(word_id,0) + (mu * calc_p_wc(word_id)) \n",
    "        p_wd_denominator = z_i + mu #led_d + mu\n",
    "        p_wd_i = p_wd_numerator / p_wd_denominator\n",
    "        score_q_i += (p_wQ[word_id] * np.log10(p_wQ[word_id]/p_wd_i))\n",
    "    score_q_i = -1 * score_q_i\n",
    "    return score_q_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on baseline in: https://pdfs.semanticscholar.org/ebf5/8dbe6d58845f3253d00e25e8c7280797fc80.pdf\n",
    "def plm_run_retrieval(model_name, kernel_list,hyper_param, doc_list,p_wQ_dict):\n",
    "    \"\"\"\n",
    "    Runs a retrieval method for all the queries and writes the TREC-friendly results in a file.\n",
    "    \n",
    "    :param model_name: the name of the model (a string)\n",
    "    :param kernel_fn: the scoring function (a function - see below for an example) \n",
    "    \"\"\"\n",
    "    model_name = model_name + \"_\" +str(hyper_param)\n",
    "    #-----------------------------------------\n",
    "    run_out_path = str('models/eval/') + '{}.run'.format(model_name)\n",
    "    if os.path.exists(run_out_path):\n",
    "        return\n",
    "    retrieval_start_time = time.time()\n",
    "    print('Retrieving using', model_name)\n",
    "    \n",
    "    data = collections.defaultdict(list)   \n",
    "    \n",
    "    '''query= ['133','93']\n",
    "    terms = tokenized_queries['133'],tokenized_queries['93']\n",
    "    for i in range(len(query)):\n",
    "        query_id = query[i]\n",
    "        query_terms = terms[i]'''\n",
    "    \n",
    "    for query_id, query_terms in tokenized_queries.items(): \n",
    "        score_per_doc = {}\n",
    "        docs_to_check = set([inverted_index[term_id].keys() for term_id in tokenized_queries[query_id]][0])\n",
    "        print(query_id,len(docs_to_check))\n",
    "        for int_doc_id in docs_to_check:\n",
    "            score_pos = []\n",
    "            word_pos_in_doc=[(j,w) for w in query_terms \n",
    "                               for j in np.where(np.array(doc_list[int_doc_id-1][1]) == w)[0]]\n",
    "            for pos in range(len(doc_list[int_doc_id-1][1])):\n",
    "                score_pos.append(find_position_score(word_pos_in_doc, pos , hyper_param, p_wQ_dict[query_id],kernel_list))\n",
    "            score_per_doc[int_doc_id] = max(score_pos)\n",
    "\n",
    "\n",
    "        #------------------------make data set to write in run file------------------------------------        \n",
    "        for int_doc_id in score_per_doc:\n",
    "            data[query_id].append((score_per_doc[int_doc_id], doc_list[int_doc_id-1][0]))\n",
    "\n",
    "    \n",
    "    with open(run_out_path, 'w') as f_out:\n",
    "        write_run(\n",
    "            model_name=model_name,\n",
    "            data=data,\n",
    "            out_f=f_out,\n",
    "            max_objects_per_query=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 3000\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "plm_run_retrieval(\"plm_gaus\", precomputed_gaus ,mu, doc_list,p_wQ_dict)\n",
    "print('precomputed_gaus creation took', time.time() - start_time, 'seconds.')\n",
    "\n",
    "start_time = time.time()\n",
    "plm_run_retrieval(\"plm_triangle\", precomputed_triangle ,mu, doc_list,p_wQ_dict)\n",
    "print('precomputed_triangle creation took', time.time() - start_time, 'seconds.')\n",
    "\n",
    "start_time = time.time()\n",
    "plm_run_retrieval(\"plm_cosine\", precomputed_cosine ,mu, doc_list,p_wQ_dict)\n",
    "print('cosine plm creation took', time.time() - start_time, 'seconds.')\n",
    "\n",
    "start_time = time.time()\n",
    "plm_run_retrieval(\"plm_circle\", precomputed_circle ,mu, doc_list,p_wQ_dict)\n",
    "print('circle plm creation took', time.time() - start_time, 'seconds.')\n",
    "\n",
    "start_time = time.time()\n",
    "plm_run_retrieval(\"passage\", precomputed_passage ,mu, doc_list,p_wQ_dict)\n",
    "print('passage creation took', time.time() - start_time, 'seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3:  Word embeddings for ranking [20 points] (open-ended) ###\n",
    "\n",
    "First create word embeddings on the corpus we provided using [word2vec](http://arxiv.org/abs/1411.2738) -- [gensim implementation](https://radimrehurek.com/gensim/models/word2vec.html). You should extract the indexed documents using pyndri and provide them to gensim for training a model (see example [here](https://github.com/nickvosk/pyndri/blob/master/examples/word2vec.py)).\n",
    "   \n",
    "This is an open-ended task. It is left up you to decide how you will combine word embeddings to derive query and document representations. Note that since we provide the implementation for training word2vec, you will be graded based on your creativity on combining word embeddings for building query and document representations.\n",
    "\n",
    "Note: If you want to experiment with pre-trained word embeddings on a different corpus, you can use the word embeddings we provide alongside the assignment (./data/reduced_vectors_google.txt.tar.gz). These are the [google word2vec word embeddings](https://code.google.com/archive/p/word2vec/), reduced to only the words that appear in the document collection we use in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:test\n"
     ]
    }
   ],
   "source": [
    "# Lib:\n",
    "import copy\n",
    "import gensim\n",
    "import logging\n",
    "import pyndri\n",
    "import pyndri.compat\n",
    "from operator import itemgetter\n",
    "from scipy import spatial\n",
    "from scipy import stats\n",
    "import pickle\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.info(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS:\n",
    "\n",
    "def save_to_file(my_obj, path,obj_name):\n",
    "    with open(str(path)+str(obj_name)+'.pkl', 'wb') as f:\n",
    "        pickle.dump(my_obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_from_file(path,obj_name):\n",
    "    with open(str(path)+str(obj_name), 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "    return b\n",
    "\n",
    "\n",
    "def generate_model(model_id,hyper_param,sentences):\n",
    "    logging.info('Initializing word2vec.')\n",
    "    \n",
    "    word2vec_init = gensim.models.Word2Vec(\n",
    "    size = hyper_param['size'][model_id],             # Embedding size\n",
    "    window = hyper_param['window'][model_id],         # One-sided window size\n",
    "    sg = hyper_param['sg'][model_id],                 # Skip-gram.\n",
    "    min_count = hyper_param['min_count'][model_id],   # Minimum word frequency.\n",
    "    sample = hyper_param['sample'][model_id],         # Sub-sample threshold.\n",
    "    hs = hyper_param['hs'][model_id],                 # Hierarchical softmax.\n",
    "    negative = hyper_param['negative'][model_id],     # Number of negative examples.\n",
    "    iter = hyper_param['iter'][model_id],             # Number of iterations.\n",
    "    workers  =hyper_param['workers'][model_id],       # Number of workers.\n",
    "    )\n",
    "    \n",
    "    word2vec_init.build_vocab(sentences, trim_rule=None)\n",
    "    models = [word2vec_init]\n",
    "    for epoch in range(1, 6):\n",
    "        logging.info('Epoch %d', epoch)\n",
    "\n",
    "        model = copy.deepcopy(models[-1])\n",
    "        model.train(sentences,total_examples=164597,epochs=model.iter)\n",
    "    logging.info('Trained models: %s', model)    \n",
    "    model_name = str(model_id)+\".model\"    \n",
    "    model.save(\"models/word2vec_models/\"+model_name)    \n",
    "\n",
    "# train word2vec model and save the adress:\n",
    "def train_word2vec_model(model_id,hyper_param,sentences):\n",
    "    model_adrs_dict = {}\n",
    "    for model_id in range(model_num):\n",
    "        model_name = str(model_id)+\".model\"\n",
    "        model_adrs_dict[model_id] = \"models/word2vec_models/\"+model_name\n",
    "        if os.path.exists(model_adrs_dict[model_id]):\n",
    "            continue\n",
    "        else:\n",
    "            generate_model(model_id,hyper_param,sentences)\n",
    "        \n",
    "    return model_adrs_dict    \n",
    "\n",
    "def parse_data_set(path):\n",
    "    file = open(path, 'r')    \n",
    "    top1000 = collections.defaultdict(list)\n",
    "    top_docs_set = set()\n",
    "    for line in file:\n",
    "        info =line.split()\n",
    "        top1000[info[0]].append((info[2],info[4]))\n",
    "        top_docs_set.add(info[2])\n",
    "    return top1000,top_docs_set\n",
    "\n",
    "\n",
    "def generate_query2vec(model,top1000,embed_size):\n",
    "    query2vec = collections.defaultdict(list)\n",
    "    #------------make vector for the query------------------\n",
    "    query_vec = {}\n",
    "    for query_id, query_term in tokenized_queries.items():\n",
    "        sum_vec = np.zeros(embed_size)\n",
    "        counter = 0\n",
    "        for term_id in query_term:\n",
    "            if term_id > 0 and id2token[term_id] in model :\n",
    "                sum_vec += np.copy(model[id2token[int(term_id)]])\n",
    "                counter +=1\n",
    "        if(counter):\n",
    "            query_vec[query_id] = sum_vec/counter\n",
    "    return query_vec \n",
    "\n",
    "def generate_doc2vec(model,top_docs_set,embed_size):\n",
    "    doc_vec = {}\n",
    "    for doc_ex_id in top_docs_set:\n",
    "        \n",
    "        #---------------find doc words---------------\n",
    "        for indx, doc_info in enumerate(doc_list):\n",
    "            if doc_info[0] == doc_ex_id:\n",
    "                doc_terms = doc_info[1]\n",
    "                doc_id = indx+1\n",
    "                break                \n",
    "\n",
    "        sum_vec = np.zeros(embed_size)\n",
    "        counter = 0\n",
    "        for term_id in doc_terms:\n",
    "            if term_id > 0 and id2token[term_id] in model :\n",
    "                sum_vec += np.copy(model[id2token[int(term_id)]])\n",
    "                counter +=1\n",
    "        if(counter):\n",
    "            doc_vec[doc_ex_id] = sum_vec/counter\n",
    "    return doc_vec \n",
    "\n",
    "def find_cosine_similarity(top1000_tfidf, document2vec, query2vec):\n",
    "    word2vec_cosine_similarity = collections.defaultdict(list)\n",
    "    ''' query_id = '93'\n",
    "    related_docs = top1000_tfidf['93']'''\n",
    "    for query_id, related_docs in top1000_tfidf.items():\n",
    "        for doc_info in related_docs: \n",
    "            if query_id in query2vec and doc_info[0] in document2vec:\n",
    "                sim = (1 - spatial.distance.cosine(query2vec[query_id],document2vec[doc_info[0]]),doc_info[0])\n",
    "                #query_id --> (document_score, external_doc_id)\n",
    "                word2vec_cosine_similarity[query_id].append((sim))\n",
    "    return word2vec_cosine_similarity\n",
    "\n",
    "def write_to_file_word2vec_doc2vec(top1000_list,top_docs_set,model_path,model_num,path_to_save):\n",
    "    word2vecmodel = {}\n",
    "    for model_id in range(model_num):\n",
    "        print(model_id)\n",
    "        word2vecmodel[model_id]=gensim.models.Word2Vec.load(model_path + model_adrs_dict[model_id])#load model\n",
    "        \n",
    "        \n",
    "        if not os.path.exists(str(path_to_save)+ str(model_id)+\"_query2vec\"):\n",
    "            query2vec = generate_query2vec(word2vecmodel[model_id],top1000_list,hyper_param['size'][model_id])#make query vectors\n",
    "            save_to_file(query2vec, path_to_save ,str(model_id)+\"_query2vec\")\n",
    "            \n",
    "            \n",
    "        if not os.path.exists(str(path_to_save)+ str(model_id)+\"_document2vec\"):\n",
    "            document2vec= generate_doc2vec(word2vecmodel[model_id],top_docs_set,hyper_param['size'][model_id])#make doc vectors\n",
    "            save_to_file(document2vec, path_to_save,str(model_id)+\"_document2vec\")\n",
    "        \n",
    "def generate_word2vec_similarity_file(top1000_list,top_docs_set,model_path,model_num,query_doc_path):   \n",
    "    for model_id in range(model_num):\n",
    "        \n",
    "        model_name = str(model_id)+ '_sim'\n",
    "        run_out_path = str('models/word2vec_models/') + '{}.run'.format(model_name)\n",
    "        if os.path.exists(run_out_path):\n",
    "            return\n",
    "        \n",
    "        query2vec = load_from_file(query_doc_path, str(model_id)+'_query2vec.pkl')#load query vectors\n",
    "        document2vec = load_from_file(query_doc_path, str(model_id)+'_document2vec.pkl')#load doc vector\n",
    "        word2vec_cosine_similarity = find_cosine_similarity(top1000_list, document2vec, query2vec)#find similarities between queries and docs        \n",
    "\n",
    "        with open(run_out_path, 'w') as f_out:\n",
    "            write_run(\n",
    "                model_name=model_name,\n",
    "                data=word2vec_cosine_similarity,\n",
    "                out_f=f_out,\n",
    "                max_objects_per_query=1000) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Initialization #############\n",
    "\n",
    "hyper_param = {\n",
    "'size' : [32, 32, 32, 32, 64]\n",
    ",'window' : [20, 10, 20, 5, 15]\n",
    ",'sg' : [False, True, True, False, False]\n",
    ",'min_count' : [5, 5, 5, 5, 5]\n",
    ",'sample' : [1e-3, 1e-3, 1e-3, 1e-3, 1e-3]\n",
    ",'hs' : [True,True, True, True, True]\n",
    ", 'negative' : [10, 10, 10, 10, 10]\n",
    ", 'iter' : [1, 1, 1, 1, 1]\n",
    ", 'workers' : [8, 8, 8, 8, 8]\n",
    "}\n",
    "\n",
    "\n",
    "data_path = 'models/tfidf.run' \n",
    "model_path = \"models/word2vec_models/\"\n",
    "model_num = 5 #amount of models we have\n",
    "query_doc_path = 'models/word2vec_models/query_doc_info/'\n",
    "\n",
    "\n",
    "dictionary = pyndri.extract_dictionary(index)\n",
    "sentences = pyndri.compat.IndriSentences(index, dictionary)\n",
    "logging.info('Initializing word2vec.')\n",
    "\n",
    "###### BUILD MODELS #######\n",
    "train_word2vec_model(model_id,hyper_param,doc,sentences)\n",
    "\n",
    "\n",
    "###### creat word2vec and doc2vec & find similarities ########\n",
    "top1000_tfidf,top_docs_set = parse_data_set(data_path) \n",
    "write_to_file_word2vec_doc2vec(top1000_tfidf,top_docs_set,model_path,model_num,query_doc_path) \n",
    "generate_word2vec_similarity_file(top1000_tfidf,top_docs_set,model_path,model_num,query_doc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST FOR FUN:\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "model_adrs_dict = {\n",
    "    0:\"0.model\"\n",
    "    ,1: \"1.model\"\n",
    "    ,2:\"2.model\"\n",
    "    ,3:\"3.model\"\n",
    "    , 4:\"4.model\"\n",
    "    }\n",
    "\n",
    "'''word2vecmodel=gensim.models.Word2Vec.load('word2vec_models/0.model')'''\n",
    "print(id2token[1],word2vecmodel[0][id2token[1]])\n",
    "word2vecmodel[0].most_similar(id2token[1])\n",
    "query_doc_path = 'models/word2vec_models/query_doc_info/'\n",
    "#query2vec = load_from_file(query_doc_path, str(model_id)+'_query2vec.pkl')#load query vectors\n",
    "query2vec = generate_query2vec(word2vecmodel[0],top1000_tfidf,hyper_param['size'][0])\n",
    "document2vec = load_from_file(query_doc_path, str(model_id)+'_document2vec.pkl')#load doc vector\n",
    "word2vec_cosine_similarity = find_cosine_similarity(top1000_tfidf, document2vec, query2vec)\n",
    "\n",
    "#https://www.youtube.com/watch?v=ERibwqs9p38\n",
    "#BOW: sentence vector = sum(verctor[each word in sentence])\n",
    "\n",
    "#another method: vec_sent = len_sent * sum_on_all_words_in_sent( [alpha/(alpha + p(w))] * vec_w)\n",
    "#Then v_s = v_s - u * u_T * v_s\n",
    "\n",
    "\n",
    "'''\n",
    "https://ciir-publications.cs.umass.edu/pub/web/getpdf.php?id=1248\n",
    "\n",
    " Similarly,\n",
    "Zuccon et al. [33] leveraged word embeddings for the estima-\n",
    "tion of translation probability between words, and combine\n",
    "d\n",
    "the neural translation language model with collection back\n",
    "-\n",
    "ground probabilities using the Dirichlet smoothing strate\n",
    "gy.\n",
    "All the existing work shows that employing word embed-\n",
    "ding for IR can improve retrieval effectiveness.  However,\n",
    "this is achieved by linearly combining an embedding based\n",
    "model with traditional retrieval models.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "def parse_results(results_file):\n",
    "    results_lines = [l.strip('\\n') for l in open(results_file, 'r').readlines()]\n",
    "    eval_measure = None\n",
    "    results_by_scoring = defaultdict(lambda: defaultdict(dict))\n",
    "    results_by_eval = defaultdict(lambda: defaultdict(dict))\n",
    "    means = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    for line in results_lines:\n",
    "        if line.startswith('MODEL:'):\n",
    "            scoring_method = line.split(': ')[1]\n",
    "            continue\n",
    "        else:\n",
    "            eval_measure, query, result = line.split()\n",
    "            if query == 'all':\n",
    "                means[eval_measure][scoring_method] = result\n",
    "            else:\n",
    "                results_by_scoring[scoring_method][eval_measure][int(query)] = float(result)\n",
    "                \n",
    "    for scoring_method in results_by_scoring.keys():\n",
    "        for eval_measure in results_by_scoring[scoring_method].keys():\n",
    "            for query, result in results_by_scoring[scoring_method][eval_measure].items():\n",
    "                results_by_eval[eval_measure][scoring_method][query] = float(result)\n",
    "\n",
    "    return means, results_by_scoring, results_by_eval\n",
    "    \n",
    "means, scoring_dict, eval_dict = parse_results('results_word2vec_validation.txt')\n",
    "\n",
    "\n",
    "\n",
    "language_models = {\n",
    "    '_sim' : [setting for setting in list(means['ndcg_cut_10']) \n",
    "                              if setting.startswith('0_sim') or setting.startswith('1_sim') or setting.startswith('2_sim') or setting.startswith('3_sim') or setting.startswith('4_sim')]\n",
    "}\n",
    "\n",
    "\n",
    "#pprint(language_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from itertools import combinations\n",
    "from scipy import stats\n",
    "from statsmodels.sandbox.stats import multicomp\n",
    "np.random.seed(12345678)\n",
    "\n",
    "relevance_grades  = ['s0', 's1', 's2', 's3', 's4']\n",
    "rankings = [ranking for ranking in combinations(relevance_grades, 2)]\n",
    "print(rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#['ndcg_cut_10', 'map_cut_1000', 'recall_1000', 'P_5']\n",
    "#['4_sim', '3_sim', '2_sim', '0_sim', '1_sim']\n",
    "#print(eval_dict['ndcg_cut_10']['0_sim'])\n",
    "'''\n",
    "S0 AND S4 RECEIVE A NAN PVALUE. WE THINK THIS IS BECAUSE THE NDCG@10 VALUES DON'T SEEM TO BE SIGNIFICANTLY DIFFERENT. \n",
    "SO WE CONVERT THIS TO 0 BEFORE PASSING FOR BONFERRONI TEST. AND EVENTUALLY THIS TURNS OUT TO BE THE BEST MODEL.\n",
    "IN THE END WE CHOOSE S0 (DUE TO 32 EMBEDDING SIZE AND 20 WINDOW SIZE(?)) [CHOOSE THE MODEL WITH LESS FEATURES]\n",
    "'''\n",
    "s0_ndcg = np.array(list(eval_dict['ndcg_cut_10']['0_sim'].values()), dtype = float)\n",
    "s1_ndcg = np.array(list(eval_dict['ndcg_cut_10']['1_sim'].values()), dtype = float)\n",
    "s2_ndcg = np.array(list(eval_dict['ndcg_cut_10']['2_sim'].values()), dtype = float)\n",
    "s3_ndcg = np.array(list(eval_dict['ndcg_cut_10']['3_sim'].values()), dtype = float)\n",
    "s4_ndcg = np.array(list(eval_dict['ndcg_cut_10']['4_sim'].values()), dtype = float)\n",
    "\n",
    "models_available  = [s0_ndcg, s1_ndcg, s2_ndcg, s3_ndcg, s4_ndcg]\n",
    "model_combination = [ranking for ranking in combinations(models_available, 2)]\n",
    "multiple_p_values = [] \n",
    "\n",
    "#print(model_combination[0][0], '\\n',model_combination[0][1])\n",
    "\n",
    "for item in model_combination:\n",
    "    t_test = stats.ttest_rel(item[0],item[1])\n",
    "    if str(t_test.pvalue) != 'nan':\n",
    "        multiple_p_values.append(t_test.pvalue)\n",
    "    else:\n",
    "        multiple_p_values.append(int(0))\n",
    "multiple_p_values = np.array(multiple_p_values)\n",
    "\n",
    "print(multiple_p_values)\n",
    "\n",
    "#multiple comparisons problem\n",
    "multiple_comparison = multicomp.multipletests(multiple_p_values, alpha = 0.05, method = 'bonferroni')\n",
    "\n",
    "print(multiple_comparison)\n",
    "print(Counter(eval_dict['ndcg_cut_10']['0_sim'].values())[0.0])\n",
    "print(Counter(eval_dict['ndcg_cut_10']['4_sim'].values())[0.0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "means_df = pd.DataFrame.from_dict(means)\n",
    "means_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for model in language_models.keys():\n",
    "    dicts = { setting : eval_dict['ndcg_cut_10'][setting] for setting in language_models[model] }\n",
    "    ndcg_df = pd.DataFrame.from_dict(dicts) \n",
    "    ndcg_df.plot.bar(figsize=(20,5))\n",
    "    plt.title(model + ' ndcg plot')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Learning to rank (LTR) [15 points] (open-ended) ###\n",
    "\n",
    "In this task you will get an introduction into learning to rank for information retrieval.\n",
    "\n",
    "You can explore different ways for devising features for the model. Obviously, you can use the retrieval methods you implemented in Task 1, Task 2 and Task 3 as features. Think about other features you can use (e.g. query/document length). Creativity on devising new features and providing motivation for them will be taken into account when grading.\n",
    "\n",
    "For every query, first create a document candidate set using the top-1000 documents using TF-IDF, and subsequently compute features given a query and a document. Note that the feature values of different retrieval methods are likely to be distributed differently.\n",
    "\n",
    "You are adviced to start some pointwise learning to rank algorithm e.g. logistic regression, implemented in [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "Train your LTR model using 10-fold cross validation on the test set. More advanced learning to rank algorithms will be appreciated when grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "def get_query_score(score_fn,query_terms, int_doc_id,hyper_param):\n",
    "    score = 0\n",
    "    for term_id in query_terms:\n",
    "        document_term_freq = get_tf(int_doc_id, term_id)\n",
    "        score += score_fn(int_doc_id, term_id, document_term_freq,hyper_param)\n",
    "    return score\n",
    "\n",
    "def load_data(path_file):\n",
    "    \n",
    "    file = open(path_file,'r')    \n",
    "    num_docs = index.maximum_document() - index.document_base()    \n",
    "    #data_set = collections.defaultdict(dict)\n",
    "    data_list = []\n",
    "    for line in file:\n",
    "        info = line.split()\n",
    "        query_id = info[0]\n",
    "        label = int(info[3])\n",
    "        query_terms = tokenized_queries[query_id]\n",
    "        int_doc_id = ex_to_id.get(str(info[2]),0)\n",
    "        if int_doc_id != 0:\n",
    "            data_list.append([float(label), query_id, str(info[2]), get_query_score(tfidf,query_terms, int_doc_id,num_docs),\n",
    "                                        get_query_score(BM25,query_terms, int_doc_id,[1.2,0.75]),\n",
    "                                        get_query_score(jelinek_mercer,query_terms, int_doc_id,0.7),\n",
    "                                        get_query_score(dirichlet,query_terms, int_doc_id,3000),\n",
    "                                        get_query_score(absolute_discounting,query_terms, int_doc_id,0.7),\n",
    "                                        len(query_terms),\n",
    "                                        float(document_lengths[int_doc_id])])\n",
    "            #data_set[query_id][info[2]] = data_list[-1][3:]            \n",
    "    return data_list\n",
    "\n",
    "\n",
    "def normalize_feautures(data_list):\n",
    "    new_list = list(map(list, zip(*data_list)))\n",
    "    means = []\n",
    "    stdv = []    \n",
    "    \n",
    "    for i in range(3,len(new_list)):\n",
    "        means.append(np.mean(new_list[i]))\n",
    "        stdv.append(np.std(new_list[i])) \n",
    "        new_list[i] = np.subtract(new_list[i], float(means[-1]))\n",
    "        new_list[i] = np.divide(new_list[i], float(stdv[-1]))\n",
    "    print(means,stdv)\n",
    "    return means,stdv,list(map(list, zip(*new_list)))\n",
    "\n",
    "def train_classifier(data_train, label_train, data_valid, label_valid, save_name):\n",
    "    #clf = MLPClassifier(solver='lbfgs', alpha=1e-4, hidden_layer_sizes=(100, 100, 20, 2), random_state=1)\n",
    "    clf = MLPClassifier(solver='adam', alpha=1e-4, hidden_layer_sizes=(100,50,2), random_state=1)\n",
    "    #clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(data_train, label_train)\n",
    "    predictions = np.array(clf.predict(data_valid))\n",
    "    khar = clf.predict_proba(data_train) \n",
    "    #for i in range(10, 20):\n",
    "    #    print(khar[i, 1], label_train[i], data_train[i])\n",
    "    correct = np.sum(predictions == label_valid)\n",
    "    acc = 100.0*float(correct) / ((len(data_valid)))\n",
    "    #error = np.mean(np.abs(np.subtract(predictions, label_valid)))\n",
    "    joblib.dump(clf, save_name)\n",
    "    print(\"acc: %f\" % acc)\n",
    "    return acc\n",
    "    \n",
    "\n",
    "def train_regressor(data_train, label_train, data_valid, label_valid, save_name):\n",
    "    reg = MLPRegressor(solver='adam', alpha=1e-5, hidden_layer_sizes=(100, 100, 20, 2), random_state=1)\n",
    "    reg.fit(data_train, label_train)\n",
    "    predictions = np.array(reg.predict(data_valid))\n",
    "    error = np.mean(np.abs(np.subtract(predictions, label_valid)))\n",
    "    joblib.dump(reg, \"reg_\"+save_name)\n",
    "    print(\"MSE: %f\" % error)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.1375445477876198, 2.6974360204760783, -16.44675042354265, -16.018241993603226, 0.013188075585199702, 4.2079331329276339, 323.035413153457] [9.141247295711004, 2.3257832377296586, 9.8136145404114234, 9.0690051295584286, 0.017421626706747489, 2.1784240443907934, 143.45037661258701]\n"
     ]
    }
   ],
   "source": [
    "############### Training and finding accuracy for validation set #################\n",
    "\n",
    "whole_data_path = './ap_88_89/qrel_validation'\n",
    "whole_data_list = load_data(whole_data_path)\n",
    "means,stdv,whole_data_normalized_list = normalize_feautures(whole_data_list)\n",
    "np.random.shuffle(whole_data_normalized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 84.886280\n",
      "acc: 84.005869\n",
      "acc: 83.418929\n",
      "acc: 84.005869\n",
      "acc: 84.372707\n",
      "acc: 83.492296\n",
      "acc: 81.217902\n",
      "acc: 82.465150\n",
      "acc: 84.372707\n",
      "acc: 83.859134\n",
      "ave: 83.6096845194\n"
     ]
    }
   ],
   "source": [
    "########### fold data #############\n",
    "#def cross_validation_training(whole_data_normalized_list, itr_num, hyper_param, ):\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, cross_validation\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "folding_index = int(0.1*(len(whole_data_normalized_list)))\n",
    "\n",
    "end = folding_index\n",
    "begin = 0\n",
    "validation_acc = []\n",
    "\n",
    "for i in range(10):\n",
    "    validation_data = np.array(whole_data_normalized_list[begin:end])\n",
    "    if begin == 0:   \n",
    "        train_data = np.array(whole_data_normalized_list[end:])\n",
    "    else:\n",
    "        train_data = np.concatenate((np.array(whole_data_normalized_list[:begin]) \n",
    "                                 , np.array(whole_data_normalized_list[end:])), axis=0)\n",
    "    end += folding_index\n",
    "    begin += folding_index\n",
    "\n",
    "    train_features = np.array(train_data[:,3:],dtype =float)#(44573, 7)\n",
    "    train_labels = np.array(train_data[:,0],dtype =float)#(44573, 1)\n",
    "    #print(train_labels)\n",
    "    valid_features = np.array(validation_data[:,3:],dtype =float)#(44573, 7)\n",
    "    valid_labels = np.array(validation_data[:,0],dtype =float)#(44573, 1) '''\n",
    "\n",
    "    validation_acc.append(train_classifier(train_features,train_labels,valid_features,valid_labels, str(i)+\"___label_train.pkl\"))\n",
    "print(\"ave:\",np.mean(np.array(validation_acc)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7114243023866252\n",
      "[ 0.  1.  1. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "test_path = './ap_88_89/qrel_test'\n",
    "test_data_list = load_data(test_path)\n",
    "\n",
    "new_list = list(map(list, zip(*test_data_list)))\n",
    "for i in range(3,len(new_list)):\n",
    "    new_list[i] = np.subtract(new_list[i], float(means[i-3]))\n",
    "    new_list[i] = np.divide(new_list[i], float(stdv[i-3]))\n",
    "test_normalized_list = list(map(list, zip(*new_list)))\n",
    "\n",
    "test_data = np.array(test_normalized_list)\n",
    "\n",
    "test_features = np.array(np.array(test_data)[:,3:],dtype =float)#(44573, 7)\n",
    "test_labels = np.array(np.array(test_data)[:,0],dtype =float)#(44573, 1) \n",
    "\n",
    "pred = np.zeros((len(test_labels)), dtype=np.float32)\n",
    "for i in range(10):\n",
    "    test_clf = joblib.load(str(i)+'___label_train.pkl')\n",
    "    pred += test_clf.predict_proba(test_features)[:,1]\n",
    "pred /= 10.0\n",
    "th = 0.5\n",
    "pred[pred>th] = 1\n",
    "pred[pred<=th] = 0\n",
    "\n",
    "#for i in range(100):\n",
    "#    print (pred[i], test_labels[i])\n",
    "correct = np.sum(pred == test_labels)\n",
    "acc = float(correct) / (len(pred))\n",
    "print(acc)\n",
    "\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pred),len(test_normalized_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = collections.defaultdict(list)\n",
    "for indx, item in enumerate(test_normalized_list):\n",
    "    query_id = item[1]\n",
    "    int_doc_id = ex_to_id[item[2]]\n",
    "    data[query_id].append((pred[indx], doc_list[int_doc_id - 1][0]))\n",
    "\n",
    "    \n",
    "model_name = 'ltr_test_set_prediction'\n",
    "#str('models/eval/') +\n",
    "run_out_path =  '{}.run'.format(model_name)\n",
    "\n",
    "'''if os.path.exists(run_out_path):\n",
    "    return'''\n",
    "with open(run_out_path, 'w') as f_out:\n",
    "    write_run(\n",
    "        model_name=model_name,\n",
    "        data=data,\n",
    "        out_f=f_out,\n",
    "        max_objects_per_query=1000)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'models/eval/tfidf.run' \n",
    "top1000_tfidf,top_docs_set = parse_data_set(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my idea:\n",
    "#use the pairwise ranknet loss function for the document ranking: we use scores of every doc instead of rank\n",
    "# l_ranknet = log(1+ e** -lmbda*(sj-si))\n",
    "#normalize freatures from different evaluations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (<ipython-input-93-8ae2469b3a6b>, line 57)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-93-8ae2469b3a6b>\"\u001b[0;36m, line \u001b[0;32m57\u001b[0m\n\u001b[0;31m    print(metrics.classification_report(y_test, predicted))\u001b[0m\n\u001b[0m                                                           \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, cross_validation\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Create Logisticregression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "#Load test set\n",
    "# Format is Query_id, 0, external_document_id, 'label'\n",
    "testset = pd.read_table('./ap_88_89/qrel_test', sep=' ', names=['query_id', '0', 'ext_doc_id', 'label'])\n",
    "\n",
    "#load tf_idf results from the run file \n",
    "# Format is Query_id, Q0, external_document_id, 'rank', 'score', 'methof'\n",
    "\n",
    "tfidf_results = pd.read_table('./models/eval/tfidf.run', sep=' ', names=['query_id', 'Q0', 'ext_doc_id', 'rank', 'tfidf', 'method'])\n",
    "\n",
    "#Merge the two tables to into one on query_id and external_document_id\n",
    "df = pd.merge(tfidf_results[['query_id', 'ext_doc_id', 'tfidf']], testset[['query_id', 'ext_doc_id', 'label']], how='left', on=['query_id', 'ext_doc_id'])\n",
    "df['label'].fillna(0.0, inplace=True)\n",
    "\n",
    "#Match external_doc_ids to internal_doc_ids\n",
    "df['int_doc_id'] = df['ext_doc_id'].apply(lambda ext_doc_id: ex_to_id[ext_doc_id])\n",
    "\n",
    "df['query_length'] = df['query_id'].apply(lambda query_id: len(tokenized_queries[str(query_id)]))\n",
    "df['doc_length'] = df['int_doc_id'].apply(lambda int_doc_id: document_lengths[int_doc_id])\n",
    "\n",
    "'''df['BM25'] = df.apply(get_BM25, axis=1)\n",
    "df['jelinek_merce'] = df.apply(get_jelinek_mercer, axis=1)\n",
    "df['dirichlet_prior'] = df.apply(get_dirichlet_prior, axis=1)\n",
    "df['absolute_discounting'] = df.apply(get_absolute_discounting, axis=1)'''\n",
    "'''\n",
    "#Prepare X and Y for Regression\n",
    "X = df.drop(['ext_doc_id', 'int_doc_id', 'label'], axis=1)#x = just features\n",
    "y = df['label']#just relevancy lables\n",
    "\n",
    "#Divide into train/test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "model = LogisticRegression()\n",
    "\n",
    "#Fit the data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Make predictions\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "#Scores\n",
    "scores=cross_val_score(model,X_test,y_test,cv=10)\n",
    "\n",
    "#Accuracy score\n",
    "print(metrics.accuracy_score(y_test, predicted))\n",
    "\n",
    "scores.mean()\n",
    "\n",
    "print(metrics.classification_report(y_test, predicted))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
