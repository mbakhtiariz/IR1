{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework is made by:\n",
    "* Masoumeh Bakhtiariziabari (11813105)\n",
    "* Marianne de Heer Kloots (11138351)\n",
    "* Tharangni Sivaji (XXXXXXXX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Part [15 pts]\n",
    "\n",
    "## 1. Hypothesis Testing ‚Äì The problem of multiple comparisons [5 points]\n",
    "Experimentation in AI often happens like this: \n",
    "1. Modify/Build an algorithm\n",
    "2. Compare the algorithm to a baseline by running a hypothesis test.\n",
    "3. If not significant, go back to step A\n",
    "4. If significant, start writing a paper. \n",
    "\n",
    "How many hypothesis tests, m, does it take to get to (with Type I error for each test = Œ±):\n",
    "* P(m<sup>th</sup> experiment gives significant result | m experiments lacking power to reject H<sub>0</sub>)?\n",
    "* P(at least one significant result | m experiments lacking power to reject H<sub>0</sub>)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "<ol>\n",
    "<li><ul>\n",
    "        <li> $$\n",
    "        P(m^{\\text{th}}\\text{ experiment gives significant result} \\mid m \\text{ experiments lacking power to reject } H_0) \\\\\n",
    "        \\approx P(m^{\\text{th}}\\text{ experiment gives significant result} \\mid H_0 \\text{ is true in all m experiments}) \\\\\n",
    "        \\text{(i.e. only the } m^{\\text{th}} \\text{ result is significant whereas } (m-1) \\text{ results are not significant)} \\\\\n",
    "        = \\boldsymbol{((1 - \\alpha)^{m-1})\\cdot\\alpha}\n",
    "        $$<br><br>\n",
    "        <li> $$\n",
    "        P(\\text{at least one significant result} \\mid m \\text{ experiments lacking power to reject } H_0)\\\\\n",
    "        = 1 - P(\\text{no significant result})\\\\\n",
    "        = \\boldsymbol{1 - (1 - \\alpha)^m}\n",
    "        $$\n",
    "    </ul>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bias and unfairness in Interleaving experiments [10 points]\n",
    "Balance interleaving has been shown to be biased in a number of corner cases. An example was given during the lecture with two ranked lists of length 3 being interleaved, and a randomly clicking population of users that resulted in algorithm A winning ‚Öî of the time, even though in theory the percentage of wins should be 50% for both algorithms. Can you come up with a situation of two ranked lists of length 3 and a distribution of clicks over them for which Team-draft interleaving is unfair to the better algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "answer"
    ]
   },
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "*answer here*\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Part [85 pts]\n",
    "Commercial search engines use both offline and online approach in evaluating a new search algorithm: they first use an offline test collection to compare the production algorithm (P) with the new experimental algorithm (E); if *E* statistically significantly outperforms *P* with respect to the evaluation measure of their interest, the two algorithms are then compared online through an interleaving experiment.\n",
    "\n",
    "For the purpose of this homework we will assume that the evaluation measures of interest are:\n",
    "1. Binary evaluation measures\n",
    "    1. Precision at rank k,\n",
    "    2. Recall at rank k,\n",
    "    3. Average Precision,\n",
    "2. Multi-graded evaluation measures\n",
    "    1. Normalized Discounted Cumulative Gain at rank k (nDCG@k),\n",
    "    2. Expected Reciprocal Rank (ERR).\n",
    "\n",
    "Further, for the purpose of this homework we will assume that the interleaving algorithms of interest are:\n",
    "Team-Draft Interleaving (Joachims. \"Evaluating retrieval performance using clickthrough data\". Text Mining 2003.),\n",
    "Probabilistic Interleaving (Hofmann, Whiteson, and de Rijke. \"A probabilistic method for inferring preferences from clicks.\" CIKM 2011.).\n",
    " \n",
    "In an interleaving experiment the ranked results of *P* and *E* (against a user query) are interleaved in a single ranked list which is presented to a user. The user then clicks on the results and the algorithm that receives most of the clicks wins the comparison. The experiment is repeated for a number of times (impressions) and the total wins for *P* and *E* are computed. \n",
    "\n",
    "A Sign/Binomial Test is then run to examine whether the difference in wins between the two algorithms is statistically significant (or due to chance). Alternatively one can calculate the proportion of times the *E* wins and test whether this proportion, *p*, is greater than *p<sub>0</sub>=*0.5. This is called an 1-sample 1-sided proportion test.\n",
    "\n",
    "One of the key questions however is **whether offline evaluation and online evaluation outcomes agree with each other**. In this homework you will determine the degree of agreement between offline evaluation measures and interleaving outcomes, by the means of simulations. A similar analysis using actual online data can be found at Chapelle et al. ‚ÄúLarge-Scale Validation and Analysis of Interleaved Search Evaluation‚Äù."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='purple'>[Based on Lecture 1]</font>\n",
    "### Step 1: <font color='darkred'>Simulate Rankings of Relevance for *E* and *P* *(5 points)*</font>\n",
    "\n",
    "In the first step you will generate pairs of rankings of relevance, for the production *P* and experimental *E*, respectively, for a hypothetical query **q**. Assume a 3-graded relevance, i.e. `{N, R, HR}`. Construct all possible *P* and *E* ranking pairs of length 5. This step should give you about.\n",
    "\n",
    "Example:\n",
    "\n",
    "    P: {N N N N N}\n",
    "    E: {N N N N R}\n",
    "    ‚Ä¶\n",
    "    P: {HR HR HR HR R}\n",
    "    E: {HR HR HR HR HR}\n",
    "\n",
    "(Note 1: If you do not have enough computational power, sample 5000 pair uniformly at random to show your work.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of combinations: 59049\n",
      "number of non-equal combinations: 58806\n",
      "[(('P', ('HR', 'N', 'HR', 'R', 'R')), ('E', ('HR', 'N', 'R', 'HR', 'R'))),\n",
      " (('P', ('N', 'R', 'HR', 'N', 'N')), ('E', ('N', 'HR', 'HR', 'N', 'N'))),\n",
      " (('P', ('N', 'HR', 'N', 'N', 'R')), ('E', ('HR', 'HR', 'R', 'HR', 'HR'))),\n",
      " (('P', ('N', 'HR', 'HR', 'R', 'HR')), ('E', ('HR', 'HR', 'N', 'HR', 'HR'))),\n",
      " (('P', ('HR', 'R', 'N', 'R', 'R')), ('E', ('R', 'N', 'R', 'HR', 'N'))),\n",
      " (('P', ('R', 'R', 'R', 'HR', 'HR')), ('E', ('R', 'N', 'HR', 'HR', 'R'))),\n",
      " (('P', ('R', 'HR', 'N', 'N', 'HR')), ('E', ('R', 'N', 'R', 'N', 'N'))),\n",
      " (('P', ('HR', 'R', 'N', 'N', 'R')), ('E', ('HR', 'R', 'R', 'N', 'N'))),\n",
      " (('P', ('R', 'R', 'R', 'N', 'R')), ('E', ('HR', 'R', 'HR', 'R', 'HR'))),\n",
      " (('P', ('R', 'N', 'R', 'R', 'HR')), ('E', ('R', 'R', 'HR', 'HR', 'N')))]\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from pprint import pprint\n",
    "import random\n",
    "\n",
    "# define collections of algorithms and relevance grades\n",
    "algorithms = ['P', 'E']\n",
    "relevance_grades = ['N', 'HR', 'R']\n",
    "\n",
    "# all possible ranking sequences\n",
    "# list of rankings [('HR', 'HR', 'HR', 'HR', 'HR') ... ('N', 'N', 'N', 'N', 'N')]\n",
    "rankings = [ranking for ranking in product(relevance_grades, repeat=5)]\n",
    "\n",
    "# all algorithms paired with all rankings \n",
    "# (list of lists with elements e.g. ('P', ('HR', 'HR', 'HR', 'HR', 'HR')))\n",
    "algorithm_rankings = [list(product(alg, rankings)) for alg in algorithms]\n",
    "\n",
    "# all possible pairs of P and E with their rankings\n",
    "all_ranking_pairs = [pair for pair in product(*algorithm_rankings)]\n",
    "\n",
    "# all ranking pairs except equals\n",
    "ranking_pairs = [pair for pair in product(*algorithm_rankings) if pair[0][1] != pair[1][1]]\n",
    "\n",
    "# pretty print\n",
    "print('number of combinations:', len(all_ranking_pairs))\n",
    "print('number of non-equal combinations:', len(ranking_pairs))\n",
    "pprint(random.sample(ranking_pairs, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "<p>We find 59049 different ranking pairs. This makes sense: each ranking pair consists of 10 relevance values (5 produced by each algorithm), all of which can take on any of the 3 grades (N, R, HR). So there should be 3<sup>10</sup> = 59049 different combinations, which matches our finding.</p>\n",
    "\n",
    "<p>We then exclude all ranking pairs which have exactly the same relevance grade sequences, which leaves us with 58806 different combinations.</p>\n",
    "\n",
    "<p>We have printed a randomly selected sample of 10 ranking pairs as an example.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: <font color='darkred'>Implement Evaluation Measures *(10 points)*</font>\n",
    "Implement 1 binary and 2 multi-graded evaluation measures out of the 7 measures mentioned above. \n",
    "\n",
    "(Note 2: Some of the aforementioned measures require the total number of relevant and highly relevant documents in the entire collection ‚Äì pay extra attention on how to find this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query 796 : \t (0.0, 0.38888888888888884)\n",
      "query 48467 : \t (1.0, 0.38888888888888884)\n",
      "query 55823 : \t (1.0, 0.3333333333333333)\n",
      "query 33520 : \t (0.6666666666666666, 1.0)\n",
      "query 4130 : \t (0.1111111111111111, 0.1111111111111111)\n",
      "query 31832 : \t (1.0, 1.0)\n",
      "query 50725 : \t (1.0, 1.0)\n",
      "query 7356 : \t (0.16666666666666666, 0.5555555555555555)\n",
      "query 28468 : \t (1.0, 1.0)\n",
      "query 3511 : \t (0.1111111111111111, 1.0)\n"
     ]
    }
   ],
   "source": [
    "# precision at rank k\n",
    "def precision(k,_ranking_pairs):\n",
    "    \n",
    "    def calc_precision(i,x):\n",
    "        rel_counter = 0.0\n",
    "        prec = 0.0\n",
    "        for j in range(k):\n",
    "            if _ranking_pairs[i][x][1][j] == 'HR' or _ranking_pairs[i][x][1][j] == 'R':\n",
    "                rel_counter += 1\n",
    "                prec += rel_counter/(1.0+j)\n",
    "        return prec\n",
    "    \n",
    "    prec_list = []\n",
    "    for i in range(len(_ranking_pairs)):\n",
    "        p_prec = calc_precision(i,0)\n",
    "        e_prec = calc_precision(i,1)\n",
    "        prec_list.append((p_prec/k,e_prec/k))\n",
    "        #print(p_prec,e_prec)\n",
    "    \n",
    "    return prec_list\n",
    "\n",
    "# results for k = 3\n",
    "prec = precision(3,ranking_pairs)\n",
    "\n",
    "random_query_index_list = random.sample(range(len(ranking_pairs)), 10)\n",
    "for i in random_query_index_list:\n",
    "    print(\"query\", i, \": \\t\", prec[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query  37361 : \t ('P', ('HR', 'R', 'R', 'N', 'HR')) \t 0.902 \t ('E', ('HR', 'N', 'HR', 'HR', 'N')) \t 0.629\n",
      "query  34992 : \t ('P', ('HR', 'R', 'HR', 'N', 'N')) \t 0.870 \t ('E', ('HR', 'R', 'HR', 'N', 'HR')) \t 0.902\n",
      "query  42315 : \t ('P', ('R', 'N', 'HR', 'HR', 'N')) \t 0.629 \t ('E', ('R', 'HR', 'R', 'N', 'HR')) \t 0.902\n",
      "query  38297 : \t ('P', ('HR', 'R', 'R', 'HR', 'R')) \t 1.000 \t ('E', ('N', 'R', 'N', 'R', 'HR')) \t 0.197\n",
      "query  58617 : \t ('P', ('R', 'R', 'R', 'R', 'R')) \t 1.000 \t ('E', ('N', 'HR', 'R', 'R', 'R')) \t 0.332\n",
      "query  27042 : \t ('P', ('HR', 'HR', 'N', 'HR', 'N')) \t 0.781 \t ('E', ('R', 'N', 'R', 'N', 'HR')) \t 0.585\n",
      "query  57705 : \t ('P', ('R', 'R', 'R', 'HR', 'HR')) \t 1.000 \t ('E', ('HR', 'HR', 'N', 'N', 'HR')) \t 0.737\n",
      "query  10752 : \t ('P', ('N', 'HR', 'HR', 'R', 'R')) \t 0.332 \t ('E', ('HR', 'N', 'R', 'R', 'N')) \t 0.629\n",
      "query  6957 : \t ('P', ('N', 'HR', 'N', 'N', 'HR')) \t 0.144 \t ('E', ('R', 'N', 'R', 'N', 'R')) \t 0.585\n",
      "query  27007 : \t ('P', ('HR', 'HR', 'N', 'HR', 'N')) \t 0.781 \t ('E', ('HR', 'R', 'HR', 'N', 'R')) \t 0.902\n"
     ]
    }
   ],
   "source": [
    "# average precision\n",
    "import numpy as np\n",
    "\n",
    "def average_prec(k_max,_ranking_pairs):\n",
    "    temp_list = []\n",
    "    ap_list = [[0,0] for i in range(len(_ranking_pairs))]\n",
    "    for k in range(1,k_max+1):\n",
    "        temp_list = precision(k,_ranking_pairs)\n",
    "        #print(k,temp_list)\n",
    "        for index,item in enumerate(temp_list):\n",
    "            ap_list[index] = (np.array(ap_list[index])+np.array(item)).tolist()\n",
    "    #print(ap_list)\n",
    "    for item in ap_list:\n",
    "        item[:] = [i / k_max for i in item]\n",
    "    return ap_list \n",
    "\n",
    "k_max = 5\n",
    "avg_prec = average_prec(k_max,ranking_pairs)\n",
    "\n",
    "for i in random_query_index_list:\n",
    "    print(\"query \",i,\": \\t\",ranking_pairs[i][0], \"\\t\",'%.3f'%avg_prec[i][0],'\\t',ranking_pairs[i][1],'\\t','%.3f'%avg_prec[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query 37361 : ('P', ('HR', 'R', 'R', 'N', 'HR')) \t 0.598 ('E', ('HR', 'N', 'HR', 'HR', 'N')) \t 0.655\n",
      "query 34992 : ('P', ('HR', 'R', 'HR', 'N', 'N')) \t 0.580 ('E', ('HR', 'R', 'HR', 'N', 'HR')) \t 0.711\n",
      "query 42315 : ('P', ('R', 'N', 'HR', 'HR', 'N')) \t 0.429 ('E', ('R', 'HR', 'R', 'N', 'HR')) \t 0.515\n",
      "query 38297 : ('P', ('HR', 'R', 'R', 'HR', 'R')) \t 0.657 ('E', ('N', 'R', 'N', 'R', 'HR')) \t 0.251\n",
      "query 58617 : ('P', ('R', 'R', 'R', 'R', 'R')) \t 0.333 ('E', ('N', 'HR', 'R', 'R', 'R')) \t 0.363\n",
      "query 27042 : ('P', ('HR', 'HR', 'N', 'HR', 'N')) \t 0.699 ('E', ('R', 'N', 'R', 'N', 'HR')) \t 0.301\n",
      "query 57705 : ('P', ('R', 'R', 'R', 'HR', 'HR')) \t 0.518 ('E', ('HR', 'HR', 'N', 'N', 'HR')) \t 0.684\n",
      "query 10752 : ('P', ('N', 'HR', 'HR', 'R', 'R')) \t 0.476 ('E', ('HR', 'N', 'R', 'R', 'N')) \t 0.444\n",
      "query 6957 : ('P', ('N', 'HR', 'N', 'N', 'HR')) \t 0.345 ('E', ('R', 'N', 'R', 'N', 'R')) \t 0.213\n",
      "query 27007 : ('P', ('HR', 'HR', 'N', 'HR', 'N')) \t 0.699 ('E', ('HR', 'R', 'HR', 'N', 'R')) \t 0.624\n"
     ]
    }
   ],
   "source": [
    "# nDCG\n",
    "\n",
    "#-------------calc ideal dcg based on ground truth [10 HR, 10 R, 10 N]\n",
    "def calc_ideal_dgc(k):\n",
    "    idcg_rel = [2]*10 + [1]*10 + [0]*10\n",
    "    idcg = 0\n",
    "    for index in range(min(k, len(idcg_rel))):\n",
    "        idcg += ((2**idcg_rel[index]) - 1)/(math.log2(2+index))\n",
    "    return idcg\n",
    "\n",
    "# I used the second formula of DCG from slide 6 of http://www.cs.cornell.edu/courses/cs4300/2013fa/lectures/evaluation-1-4pp.pdf\n",
    "#nDCG is a way to calculate this measure across many independent ____queries____(http://curtis.ml.cmu.edu/w/courses/index.php/Normalized_discounted_cumulative_gain)\n",
    "#Normalize DCG at rank n by the DCG value at rank n of the ideal ranking(stanford slide)\n",
    "\n",
    "import math\n",
    "def ndcg(max_k,query_index,method):    \n",
    "    all_dcg_k = []\n",
    "    \n",
    "    #--------------------make dcg list for all k-----------------------------\n",
    "    for r in range(0,max_k):\n",
    "        if ranking_pairs[query_index][method][1][r] == 'HR':\n",
    "            rel_num = 2\n",
    "        elif ranking_pairs[query_index][method][1][r] == 'R':\n",
    "            rel_num = 1\n",
    "        else:\n",
    "            rel_num = 0\n",
    "            \n",
    "        if len(all_dcg_k) > 0:   \n",
    "            all_dcg_k.append((((2**rel_num) - 1)/(math.log2(2+r))) + all_dcg_k[-1])\n",
    "        else:\n",
    "            all_dcg_k.append(((2**rel_num) - 1)/(math.log2(2+r)))\n",
    "            \n",
    "            \n",
    "    #----------------calculate ideal dcg------------------------------------      \n",
    "    idcg = calc_ideal_dgc(max_k)\n",
    "        \n",
    "    #--------------------------convert dcg to ndcg--------------------------   \n",
    "    if(idcg == 0):\n",
    "        all_dcg_k[:] = [0 for x in all_dcg_k]\n",
    "    else:\n",
    "        all_dcg_k[:] = [x / idcg for x in all_dcg_k]\n",
    "     \n",
    "    return all_dcg_k\n",
    "\n",
    "#-----------------------call ndcg function for all 59000 queries------------------\n",
    "ndcg_list = []\n",
    "max_k = 5\n",
    "for i in range(len(ranking_pairs)):\n",
    "    p_dcg = ndcg(max_k,i,0)\n",
    "    e_dcg = ndcg(max_k,i,1)\n",
    "    ndcg_list.append((p_dcg[-1],e_dcg[-1]))\n",
    "    \n",
    "for index in random_query_index_list:\n",
    "    print(\"query %d\"%index,\":\", ranking_pairs[index][0], \"\\t\", '%.3f'%ndcg_list[index][0], ranking_pairs[index][1], \"\\t\", '%.3f'%ndcg_list[index][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of all ERR values for P & E (should be equal):\n",
      " [ 0.55613292  0.55613292]\n",
      "mean of a random sample of 100 pairs:\n",
      " [ 0.5534362   0.55497852]\n",
      "query  37361 : \t ('P', ('HR', 'R', 'R', 'N', 'HR')) \t 0.818 \t ('E', ('HR', 'N', 'HR', 'HR', 'N')) \t 0.824\n",
      "query  34992 : \t ('P', ('HR', 'R', 'HR', 'N', 'N')) \t 0.828 \t ('E', ('HR', 'R', 'HR', 'N', 'HR')) \t 0.835\n",
      "query  42315 : \t ('P', ('R', 'N', 'HR', 'HR', 'N')) \t 0.473 \t ('E', ('R', 'HR', 'R', 'N', 'HR')) \t 0.568\n",
      "query  38297 : \t ('P', ('HR', 'R', 'R', 'HR', 'R')) \t 0.825 \t ('E', ('N', 'R', 'N', 'R', 'HR')) \t 0.256\n",
      "query  58617 : \t ('P', ('R', 'R', 'R', 'R', 'R')) \t 0.689 \t ('E', ('N', 'HR', 'R', 'R', 'R')) \t 0.415\n",
      "query  27042 : \t ('P', ('HR', 'HR', 'N', 'HR', 'N')) \t 0.855 \t ('E', ('R', 'N', 'R', 'N', 'HR')) \t 0.397\n",
      "query  57705 : \t ('P', ('R', 'R', 'R', 'HR', 'HR')) \t 0.486 \t ('E', ('HR', 'HR', 'N', 'N', 'HR')) \t 0.853\n",
      "query  10752 : \t ('P', ('N', 'HR', 'HR', 'R', 'R')) \t 0.444 \t ('E', ('HR', 'N', 'R', 'R', 'N')) \t 0.783\n",
      "query  6957 : \t ('P', ('N', 'HR', 'N', 'N', 'HR')) \t 0.412 \t ('E', ('R', 'N', 'R', 'N', 'R')) \t 0.608\n",
      "query  27007 : \t ('P', ('HR', 'HR', 'N', 'HR', 'N')) \t 0.855 \t ('E', ('HR', 'R', 'HR', 'N', 'R')) \t 0.830\n"
     ]
    }
   ],
   "source": [
    "# ERR\n",
    "import numpy as np\n",
    "\n",
    "def numerical(relevance_sequence):\n",
    "    \"\"\"\n",
    "    Convert a relevance grade sequence to a sequence of numerical \n",
    "    values, based on the relevance grades given.\n",
    "    E.g. ['HR', 'HR', 'HR', 'R', 'N'] returns [2, 2, 2, 1, 0]\n",
    "    \"\"\"\n",
    "    numerical_relevance_sequence = [1 if grade == 'R' \\\n",
    "                                    else 2 if grade == 'HR' else 0 \\\n",
    "                                    for grade in relevance_sequence]\n",
    "    return numerical_relevance_sequence\n",
    "\n",
    "def R_function(g, g_max):\n",
    "    return (2**g - 1)/(2**g_max)\n",
    "\n",
    "def ERR(relevance_sequence):\n",
    "    \"\"\"\n",
    "    Compute the ERR based on Algorithm 2 in \n",
    "    https://pdfs.semanticscholar.org/7e3c/f6492128f915112ca01dcb77c766129e65cb.pdf\n",
    "    \"\"\"\n",
    "    p = 1\n",
    "    ERR = 0\n",
    "    n = len(relevance_sequence)\n",
    "    g_max = max(relevance_sequence)\n",
    "    \n",
    "    for r in range(1, n + 1):\n",
    "        g = relevance_sequence[r - 1]\n",
    "        R = R_function(g, g_max)\n",
    "        ERR = ERR + p * (R/r)\n",
    "        p = p * (1 - R)\n",
    "    return ERR\n",
    "\n",
    "ERR_list = []\n",
    "for P, E in ranking_pairs:\n",
    "    ERR_P = ERR(numerical(P[1]))\n",
    "    ERR_E = ERR(numerical(E[1]))\n",
    "    ERR_list.append((ERR_P, ERR_E))\n",
    "    \n",
    "print('mean of all ERR values for P & E (should be equal):\\n', \n",
    "      np.mean(ERR_list, axis=0))\n",
    "print('mean of a random sample of 100 pairs:\\n', \n",
    "      np.mean(random.sample(ERR_list, 100), axis=0))\n",
    "for i in random_query_index_list:\n",
    "    print(\"query \",i,\": \\t\",ranking_pairs[i][0], \"\\t\",'%.3f'%ERR_list[i][0],'\\t',ranking_pairs[i][1],'\\t','%.3f'%ERR_list[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "explanation/analysis?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: <font color='darkred'>Calculate the ùõ•measure *(0 points)*</font>\n",
    "For the three measures and all *P* and *E* ranking pairs constructed above calculate the difference: ùõ•measure = measure<sub>E</sub>-measure<sub>P</sub>. Consider only those pairs for which *E* outperforms *P*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24462 27962 29376 29374\n"
     ]
    }
   ],
   "source": [
    "# code\n",
    "diff_precision = [precision_E - precision_P for precision_P, precision_E in prec if precision_E > precision_P]\n",
    "diff_avgprec = [avgprec_E - avgprec_P for avgprec_P, avgprec_E in avg_prec if avgprec_E > avgprec_P]\n",
    "diff_nDCG = [nDCG_E - nDCG_P for nDCG_P, nDCG_E in ndcg_list if nDCG_E > nDCG_P]\n",
    "diff_ERR = [ERR_E - ERR_P for ERR_P, ERR_E in ERR_list if ERR_E > ERR_P]\n",
    "print(len(diff_precision), len(diff_avgprec), len(diff_nDCG), len(diff_ERR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "explanation/analysis?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='purple'>[Based on Lecture 2]</font>\n",
    "### Step 4: <font color='darkred'>Implement Interleaving *(15 points)*</font>\n",
    "Implement 2 interleaving algorithms: (1) Team-Draft Interleaving OR Balanced Interleaving, AND (2), Probabilistic Interleaving. The interleaving algorithms should (a) given two rankings of relevance interleave them into a single ranking, and (b) given the users clicks on the interleaved ranking assign credit to the algorithms that produced the rankings.\n",
    "\n",
    "(Note 4: Note here that as opposed to a normal interleaving experiment where rankings consists of urls or docids, in our case the rankings consist of relevance labels. Hence in this case (a) you will assume that E and P return different documents, (b) the interleaved ranking will also be a ranking of labels.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "explanation/analysis?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='purple'>[Based on Lecture 3]</font>\n",
    "### Step 5: <font color='darkred'>Implement User Clicks Simulation *(15 points)*</font>\n",
    "Having interleaved all the ranking pairs an online experiment could be ran. However, given that we do not have any users (and the entire homework is a big simulation) we will simulate user clicks.\n",
    "\n",
    "We have considered a number of click models including:\n",
    "1. Random Click Model (RCM)\n",
    "2. Position-Based Model (PBM)\n",
    "3. Simple Dependent Click Model (SDCM)\n",
    "4. Simple Dynamic Bayesian Network (SDBN)\n",
    "\n",
    "Consider two different click models, (a) the Random Click Model (RCM), and (b) one out of the remaining 3 aforementioned models. The parameters of some of these models can be estimated using the Maximum Likelihood Estimation (MLE) method, while others require using the Expectation-Maximization (EM) method. Implement the two models so that (a) there is a method that learns the parameters of the model given a set of training data, (b) there is a method that predicts the click probability given a ranked list of relevance labels, (c) there is a method that decides - stochastically - whether a document is clicked based on these probabilities.\n",
    "\n",
    "Having implemented the two click models, estimate the model parameters using the Yandex Click Log [[file]](https://drive.google.com/file/d/1tqMptjHvAisN1CJ35oCEZ9_lb0cEJwV0/view).\n",
    "\n",
    "(Note 6: Do not learn the attractiveness parameter *a*<sub>uq</sub>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "explanation/analysis?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: <font color='darkred'>Simulate Interleaving Experiment *(10 points)*</font>\n",
    "Having implemented the click models, it is time to run the simulated experiment.\n",
    "\n",
    "For each of interleaved ranking run N simulations for each one of the click models implemented and measure the proportion *p* of wins for E.\n",
    "(Note 7: Some of the models above include an attractiveness parameter *a*<sub>uq</sub>. Use the relevance label to assign this parameter by setting *a*<sub>uq</sub> for a document u in the ranked list accordingly. (See [Click Models for Web Search](http://clickmodels.weebly.com/uploads/5/2/2/5/52257029/mc2015-clickmodels.pdf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "explanation/analysis?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: <font color='darkred'>Results and Analysis *(30 points)*</font>\n",
    "Compare the results of the offline experiments (i.e. the values of the ùõ•measure) with the results of the online experiment (i.e. proportion of wins), analyze them and reach your conclusions regarding their agreement.\n",
    "* Use easy to read and comprehend visuals to demonstrate the results;\n",
    "* Analyze the results on the basis of\n",
    "    * the evaluation measure used,\n",
    "    * the interleaving method used,\n",
    "    * the click model used.\n",
    "* Report and ground your conclusions.\n",
    "\n",
    "(Note 8: This is the place where you need to demonstrate your deeper understanding of what you have implemented so far; hence the large number of points assigned. Make sure you clearly do that so that the examiner of your work can grade it accordingly.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average precision vs. precision\n",
      "0.385633681425 0.440397350993 \t Ttest_indResult(statistic=-24.810331399370998, pvalue=4.1906255886023189e-135) \n",
      "\n",
      "average precision vs. ERR\n",
      "0.385633681425 0.252210726855 \t Ttest_indResult(statistic=71.148169032468587, pvalue=0.0) \n",
      "\n",
      "average precision vs. nDCG\n",
      "0.385633681425 0.227499954393 \t Ttest_indResult(statistic=87.264614427180462, pvalue=0.0) \n",
      "\n",
      "precision vs. ERR\n",
      "0.440397350993 0.252210726855 \t Ttest_indResult(statistic=102.07668528704501, pvalue=0.0) \n",
      "\n",
      "precision vs. nDCG\n",
      "0.440397350993 0.227499954393 \t Ttest_indResult(statistic=120.28697419447488, pvalue=0.0) \n",
      "\n",
      "ERR vs. nDCG\n",
      "0.252210726855 0.227499954393 \t Ttest_indResult(statistic=17.076224869103331, pvalue=3.2076576398410232e-65) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "np.random.seed(123)\n",
    "\n",
    "diff_measure_lists = {'precision': diff_precision, 'average precision': diff_avgprec, \n",
    "                      'nDCG': diff_nDCG, 'ERR': diff_ERR}\n",
    "\n",
    "for measure_1, measure_2 in combinations(diff_measure_lists.keys(), 2):\n",
    "    print(measure_1, 'vs.', measure_2)\n",
    "    measure_1 = diff_measure_lists[measure_1]\n",
    "    measure_2 = diff_measure_lists[measure_2]\n",
    "    print(np.mean(measure_1), np.mean(measure_2), '\\t', stats.ttest_ind(measure_1, measure_2), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "explanation/analysis?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<u>Yandex Click Log File</u>:\n",
    "\n",
    "The dataset includes user sessions extracted from Yandex logs, with queries, URL rankings and clicks. To allay privacy concerns the user data is fully anonymized. So, only meaningless numeric IDs of queries, sessions, and URLs are released. The queries are grouped only by sessions and no user IDs are provided. The dataset consists of several parts. Logs represent a set of rows, where each row represents one of the possible user actions: query or click.\n",
    "\n",
    "In the case of a Query:\n",
    "\n",
    "    SessionID TimePassed TypeOfAction QueryID RegionID ListOfURLs\n",
    "\n",
    "\n",
    "In the case of a Click:\n",
    "\n",
    "    SessionID TimePassed TypeOfAction URLID\n",
    "\n",
    "\n",
    "* `SessionID` - the unique identifier of the user session.\n",
    "* `TimePassed` - the time elapsed since the beginning of the current session in standard time units.\n",
    "* `TypeOfAction` - type of user action. This may be either a query (Q), or a click (C).\n",
    "* `QueryID` - the unique identifier of the request.\n",
    "* `RegionID` - the unique identifier of the country from which a given query. This identifier may take four values.\n",
    "* `URLID` - the unique identifier of the document.\n",
    "* `ListOfURLs` - the list of documents from left to right as they have been shown to users on the page extradition Yandex (top to bottom)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
