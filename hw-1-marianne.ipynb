{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework is made by:\n",
    "* Masoumeh Bakhtiariziabari (11813105)\n",
    "* Marianne de Heer Kloots (11138351)\n",
    "* Tharangni Sivaji (11611065)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Part [15 pts]\n",
    "\n",
    "## 1. Hypothesis Testing ‚Äì The problem of multiple comparisons [5 points]\n",
    "How many hypothesis tests, m, does it take to get to (with Type I error for each test = Œ±):\n",
    "* P(m<sup>th</sup> experiment gives significant result | m experiments lacking power to reject H<sub>0</sub>)?\n",
    "* P(at least one significant result | m experiments lacking power to reject H<sub>0</sub>)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "<ol>\n",
    "<li><ul>\n",
    "        <li> $$\n",
    "        P(m^{\\text{th}}\\text{ experiment gives significant result} \\mid m \\text{ experiments lacking power to reject } H_0) \\\\\n",
    "        \\approx P(m^{\\text{th}}\\text{ experiment gives significant result} \\mid H_0 \\text{ is true in all m experiments}) \\\\\n",
    "        \\text{(i.e. only the } m^{\\text{th}} \\text{ result is significant whereas } (m-1) \\text{ results are not significant)} \\\\\n",
    "        = \\boldsymbol{((1 - \\alpha)^{m-1})\\cdot\\alpha}\n",
    "        $$<br><br>\n",
    "        <li> $$\n",
    "        P(\\text{at least one significant result} \\mid m \\text{ experiments lacking power to reject } H_0)\\\\\n",
    "        = 1 - P(\\text{no significant result})\\\\\n",
    "        = \\boldsymbol{1 - (1 - \\alpha)^m}\n",
    "        $$\n",
    "    </ul>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bias and unfairness in Interleaving experiments [10 points]\n",
    "Balance interleaving has been shown to be biased in a number of corner cases. An example was given during the lecture with two ranked lists of length 3 being interleaved, and a randomly clicking population of users that resulted in algorithm A winning ‚Öî of the time, even though in theory the percentage of wins should be 50% for both algorithms. Can you come up with a situation of two ranked lists of length 3 and a distribution of clicks over them for which Team-draft interleaving is unfair to the better algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/latex",
    "tags": [
     "answer"
    ]
   },
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "<ul>\n",
    "<li> All interleaving comparison methods will fail to be sensitive if user clicks are not correlated with relevance.\n",
    "<li> However, Team Draft Interleaving can suffer from insensitivity even when such correlations exist.\n",
    "<li> Consider two lists $A$ and $B$ such that $A \\in \\{a, b, c\\}$ and $B \\in \\{b, c, d\\}$. \n",
    "<li> Assume $c$ is considered as the most relevant document. Then, since rank of $c^B > c^A \\Rightarrow$ B should win.\n",
    "<li> The interleaved comparisions for both the lists is given below:\n",
    "<table><tr><th>a)</th><th></th><th></th><th>b)</th><th></th><th></th></tr><tr><td></td><td>a</td><td>A</td><td></td><td>b</td><td>B</td></tr><tr><td></td><td>b</td><td>B</td><td></td><td>a</td><td>A</td></tr><tr><td></td><td>c</td><td>A</td><td></td><td>c</td><td>B</td></tr><tr><th>c)</th><td></td><td></td><th>d)</th><td></td><td></td></tr><tr><td></td><td>b</td><td>B</td><td></td><td>a</td><td>A</td></tr><tr><td></td><td>a</td><td>A</td><td></td><td>b</td><td>B</td></tr><tr><td></td><td>c</td><td>A</td><td></td><td>c</td><td>B</td></tr></table>\n",
    "<li> However, all possible 4 interleaved lists place $c$ at the same rank, i.e. third.\n",
    "<li> This results in a tie.\n",
    "<li> Hence Team Draft Interleaving fails to detect the preference for $B$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Part [85 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from pprint import pprint\n",
    "import random\n",
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from decimal import Decimal\n",
    "random.seed(11138351)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='purple'>[Based on Lecture 1]</font>\n",
    "### Step 1: <font color='darkred'>Simulate Rankings of Relevance for *E* and *P* *(5 points)*</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define collections of algorithms and relevance grades\n",
    "algorithms = ['P', 'E']\n",
    "relevance_grades = ['N', 'HR', 'R']\n",
    "\n",
    "# all possible ranking sequences\n",
    "# list of rankings [('HR', 'HR', 'HR', 'HR', 'HR') ... ('N', 'N', 'N', 'N', 'N')]\n",
    "rankings = [ranking for ranking in product(relevance_grades, repeat=5)]\n",
    "\n",
    "# all algorithms paired with all rankings \n",
    "# (list of lists with elements e.g. ('P', ('HR', 'HR', 'HR', 'HR', 'HR')))\n",
    "algorithm_rankings = [list(product(alg, rankings)) for alg in algorithms]\n",
    "\n",
    "# all possible pairs of P and E with their rankings\n",
    "all_ranking_pairs = [pair for pair in product(*algorithm_rankings)]\n",
    "\n",
    "# all ranking pairs except equals\n",
    "ranking_pairs = [pair for pair in product(*algorithm_rankings) if pair[0][1] != pair[1][1]]\n",
    "\n",
    "# pretty print\n",
    "print('number of combinations:', len(all_ranking_pairs))\n",
    "print('number of non-equal combinations:', len(ranking_pairs))\n",
    "pprint(random.sample(ranking_pairs, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**explanation/analysis:**\n",
    "\n",
    "We find 59049 different ranking pairs. This makes sense: each ranking pair consists of 10 relevance values (5 produced by each algorithm), all of which can take on any of the 3 grades (N, R, HR). So there should be 310 = 59049 different combinations, which matches our finding.\n",
    "\n",
    "We then exclude all ranking pairs which have exactly the same relevance grade sequences, which leaves us with 58806 different combinations.\n",
    "\n",
    "We have printed a randomly selected sample of 10 ranking pairs as an example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: <font color='darkred'>Implement Evaluation Measures *(10 points)*</font>\n",
    "Implement 1 binary and 2 multi-graded evaluation measures out of the 7 measures mentioned above. \n",
    "\n",
    "(Note 2: Some of the aforementioned measures require the total number of relevant and highly relevant documents in the entire collection ‚Äì pay extra attention on how to find this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#p@K function\n",
    "\n",
    "def precision(k,_ranking_pairs):\n",
    "    \n",
    "    def calc_precision(i,x):\n",
    "        rel_counter = 0.0\n",
    "        prec = 0.0\n",
    "        for j in range(k):\n",
    "            if _ranking_pairs[i][x][1][j] == 'HR' or _ranking_pairs[i][x][1][j] == 'R':\n",
    "                rel_counter += 1\n",
    "                prec += rel_counter/(1.0+j)\n",
    "        return prec\n",
    "    \n",
    "    prec_list = []\n",
    "    for i in range(len(_ranking_pairs)):\n",
    "        p_prec = calc_precision(i,0)\n",
    "        e_prec = calc_precision(i,1)\n",
    "        prec_list.append((p_prec/k,e_prec/k))\n",
    "        #print(p_prec,e_prec)\n",
    "    \n",
    "    return prec_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = precision(3,ranking_pairs)\n",
    "\n",
    "print('mean of all precision values for P & E (should be equal):\\n', \n",
    "      np.mean(prec, axis=0),'\\n')\n",
    "print('mean of a random sample of 100 pairs:\\n', \n",
    "      np.mean(random.sample(prec, 100), axis=0),'\\n')\n",
    "\n",
    "random_query_index_list = random.sample(range(len(ranking_pairs)), 10)\n",
    "print(\"Precision at K=3 for 10 random queries:\")\n",
    "print(\"query number \\t P \\t E\")\n",
    "for i in random_query_index_list:\n",
    "    print(\"query \",i,\": \\t\",'%.3f'%prec[i][0],\"\\t\",'%.3f'%prec[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_prec(k_max,_ranking_pairs):\n",
    "    temp_list = []\n",
    "    ap_list = [[0,0] for i in range(len(_ranking_pairs))]\n",
    "    for k in range(1,k_max+1):\n",
    "        temp_list = precision(k,_ranking_pairs)\n",
    "        #print(k,temp_list)\n",
    "        for index,item in enumerate(temp_list):\n",
    "            ap_list[index] = (np.array(ap_list[index])+np.array(item)).tolist()\n",
    "    #print(ap_list)\n",
    "    for item in ap_list:\n",
    "        item[:] = [i / k_max for i in item]\n",
    "    return ap_list       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_max = 5\n",
    "avg_prec = average_prec(k_max,ranking_pairs)\n",
    "\n",
    "print('mean of all AP values for P & E (should be equal):\\n', \n",
    "      np.mean(avg_prec, axis=0),'\\n')\n",
    "print('mean of a random sample of 100 pairs:\\n', \n",
    "      np.mean(random.sample(avg_prec, 100), axis=0),'\\n')\n",
    "\n",
    "random_query_index_list = random.sample(range(len(ranking_pairs)), 10)\n",
    "print(\"Average Precision for 10 random queries:\")\n",
    "print(\"query number \\t P \\t\\t\\t\\t AP \\t E \\t\\t\\t\\t AP\")\n",
    "for i in random_query_index_list:\n",
    "    print(\"query \",i,\": \\t\",ranking_pairs[i][0][1], \"\\t\",'%.3f'%avg_prec[i][0],'\\t',ranking_pairs[i][1][1],'\\t','%.3f'%avg_prec[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "explanation/analysis?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------calc ideal dcg based on ground truth [10 HR, 10 R, 10 N]\n",
    "def calc_ideal_dgc(k):\n",
    "    idcg_rel = [0.9]*10 + [0.6]*10 + [0.1]*10\n",
    "    idcg = 0\n",
    "    for index in range(min(k, len(idcg_rel))):\n",
    "        idcg += ((2**idcg_rel[index]) - 1)/(math.log2(2+index))\n",
    "    return idcg\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I used the second formula of DCG from slide 6 of http://www.cs.cornell.edu/courses/cs4300/2013fa/lectures/evaluation-1-4pp.pdf\n",
    "#nDCG is a way to calculate this measure across many independent ____queries____(http://curtis.ml.cmu.edu/w/courses/index.php/Normalized_discounted_cumulative_gain)\n",
    "#Normalize DCG at rank n by the DCG value at rank n of the ideal ranking(stanford slide)\n",
    "\n",
    "def ndcg(max_k,query_index,method):    \n",
    "    all_dcg_k = []\n",
    "    \n",
    "    #--------------------make dcg list for all k-----------------------------\n",
    "    for r in range(0,max_k):\n",
    "        if ranking_pairs[query_index][method][1][r] == 'HR':\n",
    "            rel_num = 0.9\n",
    "        elif ranking_pairs[query_index][method][1][r] == 'R':\n",
    "            rel_num = 0.6\n",
    "        else:\n",
    "            rel_num = 0.1\n",
    "            \n",
    "        if len(all_dcg_k) > 0:   \n",
    "            all_dcg_k.append((((2**rel_num) - 1)/(math.log2(2+r))) + all_dcg_k[-1])\n",
    "        else:\n",
    "            all_dcg_k.append(((2**rel_num) - 1)/(math.log2(2+r)))\n",
    "            \n",
    "            \n",
    "    #----------------calculate ideal dcg------------------------------------      \n",
    "    idcg = calc_ideal_dgc(max_k)\n",
    "        \n",
    "    #--------------------------convert dcg to ndcg--------------------------   \n",
    "    if(idcg == 0):\n",
    "        all_dcg_k[:] = [0 for x in all_dcg_k]\n",
    "    else:\n",
    "        all_dcg_k[:] = [x / idcg for x in all_dcg_k]\n",
    "     \n",
    "    return all_dcg_k\n",
    "\n",
    "#-----------------------call ndcg function for all 59000 queries------------------\n",
    "ndcg_list = []\n",
    "max_k = 5\n",
    "for i in range(len(ranking_pairs)):\n",
    "    p_dcg = ndcg(max_k,i,0)\n",
    "    e_dcg = ndcg(max_k,i,1)\n",
    "    ndcg_list.append((p_dcg[-1],e_dcg[-1]))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean of all NDCD values for P & E (should be equal):\\n', \n",
    "      np.mean(ndcg_list, axis=0),'\\n')\n",
    "print('mean of a random sample of 100 pairs:\\n', \n",
    "      np.mean(random.sample(ndcg_list, 100), axis=0),'\\n')\n",
    "random_query_index_list = random.sample(range(len(ranking_pairs)), 10)\n",
    "print(\"NDCG@5 for 10 random queries:\")\n",
    "print(\"query number \\t P \\t\\t\\t\\t NDCG \\t E \\t\\t\\t\\t NDCG\")\n",
    "for index in random_query_index_list:\n",
    "    print(\"query %d\"%index,\":\",'\\t',ranking_pairs[index][0][1], \"\\t\", '%.3f'%ndcg_list[index][0],'\\t',ranking_pairs[index][1][1], \"\\t\",'%.3f'%ndcg_list[index][1],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERR\n",
    "\n",
    "def numerical(relevance_sequence):\n",
    "    \"\"\"\n",
    "    Convert a relevance grade sequence to a sequence of numerical \n",
    "    values, based on the relevance grades given.\n",
    "    E.g. ['HR', 'HR', 'HR', 'R', 'N'] returns [2, 2, 2, 1, 0]\n",
    "    \"\"\"\n",
    "    numerical_relevance_sequence = [0.6 if grade == 'R' \\\n",
    "                                    else 0.9 if grade == 'HR' else 0.1 \\\n",
    "                                    for grade in relevance_sequence]\n",
    "    return numerical_relevance_sequence\n",
    "\n",
    "def R_function(g, g_max):\n",
    "    return (2**g - 1)/(2**g_max)\n",
    "\n",
    "def ERR(relevance_sequence):\n",
    "    \"\"\"\n",
    "    Compute the ERR based on Algorithm 2 in \n",
    "    https://pdfs.semanticscholar.org/7e3c/f6492128f915112ca01dcb77c766129e65cb.pdf\n",
    "    \"\"\"\n",
    "    p = 1\n",
    "    ERR = 0\n",
    "    n = len(relevance_sequence)\n",
    "    g_max = max(relevance_sequence)\n",
    "    \n",
    "    for r in range(1, n + 1):\n",
    "        g = relevance_sequence[r - 1]\n",
    "        R = R_function(g, g_max)\n",
    "        ERR = ERR + p * (R/r)\n",
    "        p = p * (1 - R)\n",
    "    return ERR\n",
    "\n",
    "ERR_list = []\n",
    "for P, E in ranking_pairs:\n",
    "    ERR_P = ERR(numerical(P[1]))\n",
    "    ERR_E = ERR(numerical(E[1]))\n",
    "    ERR_list.append((ERR_P, ERR_E))\n",
    "    \n",
    "print('mean of all ERR values for P & E (should be equal):\\n', \n",
    "      np.mean(ERR_list, axis=0))\n",
    "print('mean of a random sample of 100 pairs:\\n', \n",
    "      np.mean(random.sample(ERR_list, 100), axis=0),'\\n')\n",
    "\n",
    "random_query_index_list = random.sample(range(len(ranking_pairs)), 10)\n",
    "print(\"ERR for 10 random queries:\")\n",
    "print(\"query number \\t P \\t\\t\\t\\t ERR \\t E \\t\\t\\t\\t ERR\")\n",
    "for index in random_query_index_list:\n",
    "    print(\"query %d\"%index,\":\",'\\t',ranking_pairs[index][0][1], \"\\t\", '%.3f'%ERR_list[index][0],'\\t',ranking_pairs[index][1][1], \"\\t\",'%.3f'%ERR_list[index][1],'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: <font color='darkred'>Calculate the ùõ•measure *(0 points)*</font>\n",
    "For the three measures and all *P* and *E* ranking pairs constructed above calculate the difference: ùõ•measure = measure<sub>E</sub>-measure<sub>P</sub>. Consider only those pairs for which *E* outperforms *P*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def measure_diff(measure_list, _ranking_pairs, query_index_list):\n",
    "    pos_result = []\n",
    "    diff_list = []\n",
    "    for i in query_index_list:\n",
    "        diff = measure_list[i][1] - measure_list[i][0]\n",
    "        if diff > 0:\n",
    "            pos_result.append(_ranking_pairs[i])\n",
    "            diff_list.append(diff)\n",
    "    return pos_result, diff_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_query_index_list = [i for i in range(len(ranking_pairs))]\n",
    "#for ap:\n",
    "print(\"Based on ùõ•measure for AP: \")\n",
    "ap_pos_result,diff_avgprec = measure_diff(avg_prec, ranking_pairs, random_query_index_list)\n",
    "print(\"There are \",len(ap_pos_result),\" experiments that E out perform P\")\n",
    "print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "#for nDCG:\n",
    "print(\"Based on ùõ•measure for nDCG: \")\n",
    "ndcg_pos_result,diff_nDCG = measure_diff(ndcg_list, ranking_pairs, random_query_index_list)\n",
    "print(\"There are \",len(ndcg_pos_result),\" experiments that E out perform P\")\n",
    "print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "#for ERR:\n",
    "print(\"Based on ùõ•measure for ERR: \")\n",
    "err_pos_result,diff_ERR = measure_diff(ERR_list, ranking_pairs, random_query_index_list)\n",
    "print(\"There are \",len(err_pos_result),\" experiments that E out perform P\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(diff_avgprec), len(diff_nDCG), len(diff_ERR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "explanation/analysis?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='purple'>[Based on Lecture 2]</font>\n",
    "### Step 4: <font color='darkred'>Implement Interleaving *(15 points)*</font>\n",
    "Implement 2 interleaving algorithms: (1) Team-Draft Interleaving OR Balanced Interleaving, AND (2), Probabilistic Interleaving. The interleaving algorithms should (a) given two rankings of relevance interleave them into a single ranking, and (b) given the users clicks on the interleaved ranking assign credit to the algorithms that produced the rankings.\n",
    "\n",
    "(Note 4: Note here that as opposed to a normal interleaving experiment where rankings consists of urls or docids, in our case the rankings consist of relevance labels. Hence in this case (a) you will assume that E and P return different documents, (b) the interleaved ranking will also be a ranking of labels.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Team-Draft Interleaving\n",
    "#P and E as two interleaving lists\n",
    "#as output we will have \n",
    "\n",
    "def team_draft_interleaving(_ranking_pairs):\n",
    "    \"\"\"\n",
    "    Input: [[(documents for P for query 1,documents for E for query 1)]\n",
    "            ,[(documents for P for query 2,documents for E for query 2)]\n",
    "            ,[(documents for P for query 3,documents for E for query 3)],...]\n",
    "            \n",
    "    output: [[combined documents of P and E for query 1]\n",
    "            ,[combined documents of P and E for query 2]\n",
    "            ,[combined documents of P and E for query 3],...]\n",
    "            \n",
    "    Here we assume all of documents in A and B are unique and independent otherwise we should change the\n",
    "    code to eliminate the document from both A and B when we insert it to I.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_I = []\n",
    "    for i in range(len(_ranking_pairs)):\n",
    "        A = _ranking_pairs[i][0][1]\n",
    "        B = _ranking_pairs[i][1][1]\n",
    "        team_A = []\n",
    "        team_B = []\n",
    "        I = []\n",
    "        while len(team_A) < len(A) or len(team_B) < len(B):\n",
    "            RandBit = random.getrandbits(1)\n",
    "            #pick from A\n",
    "            if len(team_A) < len(team_B) or (len(team_A) == len(team_B) and RandBit == 1):\n",
    "                I.append((A[len(team_A)],0))\n",
    "                team_A.append(I[-1])        \n",
    "            else:\n",
    "                #pick from B\n",
    "                I.append((B[len(team_B)],1))\n",
    "                team_B.append(I[-1])\n",
    "        all_I.append(I[:5])\n",
    "    return all_I\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_winner(_click,_all_I):\n",
    "    winner_list = []\n",
    "    for query_index in range(len(_all_I)):\n",
    "        h_a = 0\n",
    "        h_b = 0\n",
    "        for index, item in enumerate(_all_I[query_index]):\n",
    "            if _click[query_index][index] > 0:\n",
    "                if item[1] == 0:\n",
    "                    h_a += 1\n",
    "                if item[1] == 1:\n",
    "                    h_b += 1\n",
    "        if(h_a > h_b):\n",
    "            winner_list.append('P')\n",
    "        elif(h_a == h_b):\n",
    "            winner_list.append('NoPref')\n",
    "        else:\n",
    "            winner_list.append('E')\n",
    "    return winner_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "explanation/analysis?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='purple'>[Based on Lecture 3]</font>\n",
    "### Step 5: <font color='darkred'>Implement User Clicks Simulation *(15 points)*</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn_rcm_param(_train_file):\n",
    "    c = 0\n",
    "    query_num = 0.0\n",
    "    click_num = 0.0\n",
    "    doc = Counter()\n",
    "    for line in _train_file:\n",
    "        info = line.split()\n",
    "        #print(info[2] == 'Q')\n",
    "        if info[2] == 'Q':\n",
    "            #query_num += 1\n",
    "            for i in info[5:]:\n",
    "                doc[i] += 1\n",
    "        if info[2] == 'C':\n",
    "            click_num += 1\n",
    "        \n",
    "    #print(doc)\n",
    "    doc_num = len(doc)#query_num*10\n",
    "    #print(doc_num)\n",
    "    _rcm_param = click_num/doc_num\n",
    "    return _rcm_param\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = open(\"YandexRelPredChallenge.txt\",\"r\")\n",
    "rho = learn_rcm_param(train_file)\n",
    "rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SDBM:\n",
    "def learn_sdbm_param(_train_file):        \n",
    "\n",
    "    prev_action = ''\n",
    "    satisfy_num = 0\n",
    "    click_num = 0\n",
    "    \n",
    "    for line in _train_file:\n",
    "        \n",
    "        info = line.split()        \n",
    "        if info[2] == 'C':\n",
    "            click_num += 1\n",
    "\n",
    "        if info[2] == 'Q' and prev_action == 'C':\n",
    "            satisfy_num += 1\n",
    "        prev_action = info[2]\n",
    "    if prev_action == 'C':\n",
    "        satisfy_num += 1\n",
    "    sigma = satisfy_num/click_num\n",
    "    \n",
    "    return sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = open(\"YandexRelPredChallenge.txt\",\"r\")\n",
    "sigma = learn_sdbm_param(train_file)\n",
    "sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "explanation/analysis?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: <font color='darkred'>Simulate Interleaving Experiment *(10 points)*</font>\n",
    "Having implemented the click models, it is time to run the simulated experiment.\n",
    "\n",
    "For each of interleaved ranking run N simulations for each one of the click models implemented and measure the proportion *p* of wins for E.\n",
    "(Note 7: Some of the models above include an attractiveness parameter *a*<sub>uq</sub>. Use the relevance label to assign this parameter by setting *a*<sub>uq</sub> for a document u in the ranked list accordingly. (See [Click Models for Web Search](http://clickmodels.weebly.com/uploads/5/2/2/5/52257029/mc2015-clickmodels.pdf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file = open(\"YandexRelPredChallenge.txt\",\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate_rcm_click(_I,_ru):\n",
    "    simulated_click = []\n",
    "    for i in range(len(_I)):\n",
    "        if(random.random() < _ru):\n",
    "            simulated_click.append(1)\n",
    "        else:\n",
    "            simulated_click.append(0)\n",
    "    return simulated_click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate_sdbm_click(_I,_sigma):\n",
    "    simulated_click = [0]*len(_I)\n",
    "    attraction = {'HR':0.9,'R':0.6,'N':0.1}\n",
    "    for index,item in enumerate(_I):\n",
    "        if random.random() < attraction[item[0]]:\n",
    "            simulated_click[index]= 1\n",
    "            if random.random() < _sigma:\n",
    "                return simulated_click\n",
    "    return simulated_click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proportion(winner):\n",
    "    \"\"\"\n",
    "    input:   a counter including {'E':E_count,'P':P_count,'NoPref':NoPref_count}\n",
    "    output:  ratio of choosing E over P\n",
    "    \"\"\"\n",
    "    ratio = winner['E']/(winner['P'] + winner['E'])\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_simulation(data_set,train_file,method,iteration_num):\n",
    "    #-------------learn param-------------------------\n",
    "    if method == 'rcm':\n",
    "        param = learn_rcm_param(train_file)#estimate rho\n",
    "    else:\n",
    "        param = learn_sdbm_param(train_file)#estimate sigma    \n",
    "    \n",
    "    avg_ratio_E = 0\n",
    "    ratio_list = []\n",
    "    winner_matrix = []\n",
    "    for i in range(iteration_num):\n",
    "        interleaved_set = team_draft_interleaving(data_set)\n",
    "        if method == 'rcm':\n",
    "            click_list = [simulate_rcm_click(interleaved_set[i],param) for i in range(len(interleaved_set))]\n",
    "        else:\n",
    "            click_list = [simulate_sdbm_click(interleaved_set[i],param) for i in range(len(interleaved_set))]\n",
    "\n",
    "        winner_list = find_winner(click_list, interleaved_set)\n",
    "        winner_counter = Counter(winner_list)\n",
    "        prop = proportion(winner_counter)\n",
    "        avg_ratio_E += prop\n",
    "        ratio_list.append(prop)\n",
    "        winner_matrix.append(winner_list)\n",
    "\n",
    "    avg_ratio_E /= iteration_num\n",
    "    \n",
    "    query_winner = list(map(list, zip(*winner_matrix)))\n",
    "    #print(winners_per_query[:10])\n",
    "    qwinner_ratio = []\n",
    "    \n",
    "    for item in query_winner:\n",
    "        qwinner_counter = Counter(item)\n",
    "        qwinner_ratio.append(proportion(qwinner_counter))\n",
    "        \n",
    "    return avg_ratio_E,ratio_list, qwinner_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change iteration_num (= number of samples generated) if you want to run less iterations to save time\n",
    "# running the simulation for 100 iterations takes a bit more than 1 minute wall clock time for us\n",
    "# so around 7 minutes for all conditions in this cell \n",
    "iteration_num = 100\n",
    "\n",
    "train_file.seek(0)\n",
    "%time ap_rcm_avg_ratio_E ,ap_rcm_ratio_list, ap_rcm_qwinner_ratio = run_simulation(ap_pos_result,train_file,'rcm',iteration_num)\n",
    "print(\"Applying rcm on AP:\",'\\n',ap_rcm_avg_ratio_E)\n",
    "\n",
    "train_file.seek(0)\n",
    "ap_sdbm_avg_ratio_E, ap_sdbm_ratio_list, ap_sdbm_qwinner_ratio= run_simulation(ap_pos_result,train_file,'sdbm',iteration_num)\n",
    "print(\"Applying sdbm on AP:\",'\\n',ap_sdbm_avg_ratio_E)\n",
    "\n",
    "train_file.seek(0)\n",
    "ndcg_rcm_avg_ratio_E, ndcg_rcm_ratio_list, ndcg_rcm_qwinner_ratio= run_simulation(ndcg_pos_result,train_file,'rcm',iteration_num)\n",
    "print(\"Applying rcm on NDCG:\",'\\n',ndcg_rcm_avg_ratio_E)\n",
    "\n",
    "train_file.seek(0)\n",
    "ndcg_sdbm_avg_ratio_E, ndcg_sdbm_ratio_list, ndcg_sdbm_qwinner_ratio = run_simulation(ndcg_pos_result,train_file,'sdbm',iteration_num)\n",
    "print(\"Applying sdbm on NDCG:\",'\\n',ndcg_sdbm_avg_ratio_E)\n",
    "\n",
    "train_file.seek(0)\n",
    "err_rcm_avg_ratio_E, err_rcm_ratio_list,err_rcm_qwinner_ratio= run_simulation(err_pos_result,train_file,'rcm',iteration_num)\n",
    "print(\"Applying rcm on ERR:\",'\\n',err_rcm_avg_ratio_E)\n",
    "\n",
    "train_file.seek(0)\n",
    "err_sdbm_avg_ratio_E, err_sdbm_ratio_list,err_sdbm_qwinner_ratio= run_simulation(err_pos_result,train_file,'sdbm',iteration_num)\n",
    "print(\"Applying sdbm on ERR:\",'\\n',err_sdbm_avg_ratio_E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: <font color='darkred'>Results and Analysis *(30 points)*</font>\n",
    "Compare the results of the offline experiments (i.e. the values of the ùõ•measure) with the results of the online experiment (i.e. proportion of wins), analyze them and reach your conclusions regarding their agreement.\n",
    "* Use easy to read and comprehend visuals to demonstrate the results;\n",
    "* Analyze the results on the basis of\n",
    "    * the evaluation measure used,\n",
    "    * the interleaving method used,\n",
    "    * the click model used.\n",
    "* Report and ground your conclusions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis based on click model used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# t-tests between click models\n",
    "proportion_lists = {\n",
    "    'RCM': {\n",
    "        'AP': ap_rcm_ratio_list,\n",
    "        'ERR': err_rcm_ratio_list,\n",
    "        'nDCG': ndcg_rcm_ratio_list,\n",
    "    },\n",
    "    'SDBM': {\n",
    "        'AP': ap_sdbm_ratio_list,\n",
    "        'ERR': err_sdbm_ratio_list,\n",
    "        'nDCG': ndcg_sdbm_ratio_list,\n",
    "    }\n",
    "}\n",
    "\n",
    "ttest_results = {}\n",
    "\n",
    "for eval_measure in ['AP', 'ERR', 'nDCG']:\n",
    "    ttest_results[eval_measure] = stats.ttest_ind(proportion_lists['RCM'][eval_measure], proportion_lists['SDBM'][eval_measure])\n",
    "    print(eval_measure, ttest_results[eval_measure])\n",
    "    \n",
    "    min_x = min(proportion_lists['RCM'][eval_measure] + proportion_lists['SDBM'][eval_measure])\n",
    "    max_x = max(proportion_lists['RCM'][eval_measure] + proportion_lists['SDBM'][eval_measure])\n",
    "    bins = np.linspace(min_x, max_x, 1000)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(14,7))\n",
    "    sns.distplot(proportion_lists['RCM'][eval_measure], bins, label='RCM')\n",
    "    sns.distplot(proportion_lists['SDBM'][eval_measure], bins, label='SDBM')\n",
    "    plt.title(eval_measure + '    (p = ' + '%.2e' % Decimal(ttest_results[eval_measure][1]) + ')')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.savefig('proportiondists_' + eval_measure + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "<p>Comparing between the models, we see consistently for each evaluation measure that the proportion distribution of SDBM is higher on average than the proportion distribution of RCM. This means that with the SDBM, E had higher win proportions than with the RCM. This result makes sense: in the RCM, clicks are modelled randomly, so the relevance of documents is not taken into account. However, the SDBM click model does take the relevance into account, and since we selected the queries where E outperformed P, we would expect to see higher win proportions for E than for P using the SDBM. We performed two-sided t-tests to compare this hypothesis, setting $\\alpha = 0.05$ to determine whether the distributions differ significantly.</p>\n",
    "\n",
    "<p>The proportion distributions are shown in the graphs below. The t-test results comparing both distributions for every measure are printed above. Since $p <<< 0.05$ for every measure, we conclude that our results compare the hypothesis: using the SDBM consistently we consistently get higher win proportions for E than using the RCM.</p>\n",
    "</div>\n",
    "\n",
    "<img src='proportiondists_AP.png'>\n",
    "<img src='proportiondists_ERR.png'>\n",
    "<img src='proportiondists_nDCG.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis based on evaluation method used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Offline methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code for offline methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Online methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ANOVA between evaluation measures (for each click model)\n",
    "for model in ['RCM', 'SDBM']:\n",
    "    print(model)\n",
    "    print(stats.f_oneway(proportion_lists[model]['AP'], \n",
    "                         proportion_lists[model]['ERR'], \n",
    "                         proportion_lists[model]['nDCG']))\n",
    "    \n",
    "    min_x = min(proportion_lists[model]['AP'] + proportion_lists[model]['ERR'] + proportion_lists[model]['nDCG'])\n",
    "    max_x = max(proportion_lists[model]['AP'] + proportion_lists[model]['ERR'] + proportion_lists[model]['nDCG'])\n",
    "    bins = np.linspace(min_x, max_x, 100)\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(14,7))\n",
    "    sns.distplot(proportion_lists[model]['AP'], bins, label='AP')\n",
    "    sns.distplot(proportion_lists[model]['ERR'], bins, label='ERR')\n",
    "    sns.distplot(proportion_lists[model]['nDCG'], bins, label='nDCG')\n",
    "    plt.title(model)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.savefig('proportiondists_' + model + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "We also compared between measures for each model. Here we see again that the RCM produces win proportions around ~0.5 for all measures, while for the SDBM the results differ per measure. In the graphs plotted below, we see that the distributions overlap completely in the RCM. The graph for the SDBM shows that using the ERR measure results in the highest win proportions for E, followed by AP and then nDCG. Therefore our hypothesis is that the win proportion distributions for the RCM do not differ significantly, but the win proportion distributions for the SDBM do.<br><br>\n",
    "\n",
    "We tested this using a one-way ANOVA, where we set $\\alpha = 0.05$ to determine whether the differences between distributions are significant. As expected, the difference between the win proportion distributions for the RCM was not significant ($p = 0.542$), but the differences between the win proportion distributions for de SDBM are ($p <<< 0.05$). Therefore we conclude that the win proportions for E are consistently higher if we use ERR than if we use AP, and higher if we use AP then if we use nDCG. The distribution graphs are shown below.\n",
    "</div>\n",
    "\n",
    "<img src='proportiondists_RCM.png'>\n",
    "<img src='proportiondists_SDBM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between offline and online methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_online_offline(offline_delta, online_avg, x_name, y_name, filename, title):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.scatter(offline_delta, online_avg, alpha=.1)\n",
    "    plt.xlabel(x_name)\n",
    "    plt.ylabel(y_name)\n",
    "    plt.title(title)\n",
    "    plt.savefig(filename + '.png')\n",
    "    plt.close()\n",
    "    \n",
    "pearson_correlation_coefficients = { \n",
    "    'AP': {\n",
    "        'RCM': stats.pearsonr(diff_avgprec, ap_rcm_qwinner_ratio),\n",
    "        'SDBM': stats.pearsonr(diff_avgprec, ap_sdbm_qwinner_ratio) \n",
    "    },\n",
    "    'nDCG': {\n",
    "        'RCM': stats.pearsonr(diff_nDCG, ndcg_rcm_qwinner_ratio),\n",
    "        'SDBM': stats.pearsonr(diff_nDCG, ndcg_sdbm_qwinner_ratio)\n",
    "    },\n",
    "    'ERR': {\n",
    "        'RCM': stats.pearsonr(diff_ERR, err_rcm_qwinner_ratio),\n",
    "        'SDBM': stats.pearsonr(diff_ERR, err_sdbm_qwinner_ratio)\n",
    "    }\n",
    "}\n",
    "\n",
    "plot_online_offline(diff_avgprec, \n",
    "                    ap_rcm_qwinner_ratio, \n",
    "                    \"AP Delta (E-P)\", \n",
    "                    \"RCM proportion E_num/(E_num+P_num)\", \n",
    "                    \"AP_RCM\",\n",
    "                    \"Correlation between Œî-measures and win proportions for AP, using RCM\\n\" + \n",
    "                    \"(Pearson coefficient: \" + '%.2e'%Decimal(pearson_correlation_coefficients['AP']['RCM'][0]) + \n",
    "                    \", p = \" '%.2e'%Decimal(pearson_correlation_coefficients['AP']['RCM'][1]) + ')')\n",
    "plot_online_offline(diff_avgprec,\n",
    "                    ap_sdbm_qwinner_ratio,\n",
    "                    \"AP Delta (E-P)\",\n",
    "                    \"SDBM proportion E_num/(E_num+P_num)\",\n",
    "                    \"AP_SDBM\",\n",
    "                    \"Correlation between Œî-measures and win proportions for AP, using SDBM\\n\" + \n",
    "                    \"(Pearson coefficient: \" + '%.2f'%Decimal(pearson_correlation_coefficients['AP']['SDBM'][0]) + \n",
    "                    \", p = \" '%.2e'%Decimal(pearson_correlation_coefficients['AP']['SDBM'][1]) + ')')\n",
    "plot_online_offline(diff_nDCG, \n",
    "                    ndcg_rcm_qwinner_ratio, \n",
    "                    \"NDCG Delta (E-P)\", \n",
    "                    \"RCM proportion E_num/(E_num+P_num)\",\n",
    "                    \"nDCG_RCM\",\n",
    "                    \"Correlation between Œî-measures and win proportions for nDCG, using RCM\\n\" + \n",
    "                    \"(Pearson coefficient: \" + '%.2e'%Decimal(pearson_correlation_coefficients['nDCG']['RCM'][0]) + \n",
    "                    \", p = \" '%.2e'%Decimal(pearson_correlation_coefficients['nDCG']['RCM'][1]) + ')')\n",
    "plot_online_offline(diff_nDCG, \n",
    "                    ndcg_sdbm_qwinner_ratio, \n",
    "                    \"NDCG Delta (E-P)\", \n",
    "                    \"SDBM proportion E_num/(E_num+P_num)\",\n",
    "                    \"nDCG_SDBM\",\n",
    "                    \"Correlation between Œî-measures and win proportions for nDCG, using SDBM\\n\" + \n",
    "                    \"(Pearson coefficient: \" + '%.2f'%Decimal(pearson_correlation_coefficients['nDCG']['SDBM'][0]) + \n",
    "                    \", p = \" '%.2e'%Decimal(pearson_correlation_coefficients['nDCG']['SDBM'][1]) + ')')\n",
    "plot_online_offline(diff_ERR,\n",
    "                    err_rcm_qwinner_ratio,\n",
    "                    \"ERR Delta (E-P)\",\n",
    "                    \"RCM proportion E_num/(E_num+P_num)\",\n",
    "                    \"ERR_RCM\",\n",
    "                    \"Correlation between Œî-measures and win proportions for ERR, using RCM\\n\" + \n",
    "                    \"(Pearson coefficient: \" + '%.2e'%Decimal(pearson_correlation_coefficients['ERR']['RCM'][0]) + \n",
    "                    \", p = \" '%.2e'%Decimal(pearson_correlation_coefficients['ERR']['RCM'][1]) + ')')\n",
    "plot_online_offline(diff_ERR,\n",
    "                    err_sdbm_qwinner_ratio,\n",
    "                    \"ERR Delta (E-P)\",\n",
    "                    \"SDBM proportion E_num/(E_num+P_num)\",\n",
    "                    \"ERR_SDBM\",\n",
    "                    \"Correlation between Œî-measures and win proportions for ERR, using SDBM\\n\" + \n",
    "                    \"(Pearson coefficient: \" + '%.2f'%Decimal(pearson_correlation_coefficients['ERR']['SDBM'][0]) + \n",
    "                    \", p = \" '%.2e'%Decimal(pearson_correlation_coefficients['ERR']['SDBM'][1]) + ')')\n",
    "\n",
    "for measure in pearson_correlation_coefficients.keys():\n",
    "    for model in pearson_correlation_coefficients[measure].keys():\n",
    "        print('pearson correlation coefficient between Œî-measures and win proportions for', \n",
    "              measure, 'and', model + ':')\n",
    "        print('\\t', pearson_correlation_coefficients[measure][model][0], '\\n\\t', \n",
    "              'p =', pearson_correlation_coefficients[measure][model][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "To compare the offline measures with the online measures, we wanted to see how the (offline) Œî-measures correlate with the win proportions of E. For example, if the difference in the offline measure score (AP, nDCG or ERR) between P and E is large, we expect that the win proportion of E would also be larger. Because the RCM does not take relevance into account, our hypothesis is that there is no correlation between the offline measures and the win proportions using the RCM. However, we do expect that there will be a significant positive correlation between the offline measures and the win proportions of the SDBM: we would expect that the win proportions increase when the Œî-measures increase.<br><br>\n",
    "\n",
    "To test this hypothesis, we computed the Pearson correlation coefficients and the corresponding p-values. For each measure, we computed the Pearson correlation between the Œî-measures and the win proportions using both click models. The Pearson correlation coefficient varies between -1 and +1, with 0 indicating lack of correlation. The p-value indicates the probability that an uncorrelated system would produce datasets that have a Pearson correlation at least as extreme as the one computed. We set $\\alpha = 0.05$ to determine whether the calculated Pearson correlation is significant. The Pearson correlation coefficients and their corresponding p-values are printed above.<br><br>\n",
    "\n",
    "As expected, we find no correlation between the offline and online measures using the RCM at all: we find very small Pearson correlation coefficients, but the p-values are not significant ($p > 0.05$). This makes sense, since the offline correlation compute the score based on the relevance of retrieved documents (where we have selected the ones where E outperforms P), while the RCM models random clicking and does not take any relevance scores into account. Therefore there is no relation between the offline and online measures in this case and indeed we do not expect a correlation.<br>\n",
    "For the SDBM, we do find significant positive correlations between all offline and online measures. The correlation coefficient between the offline and online measures is ~0.7 for Average Precision ($p <<< 0.05$ or even $p = 0.0$), ~0.6 for nDCG ($p <<< 0.05$ or even $p = 0.0$), and ~0.9 for ERR ($p <<< 0.05$ or even $p = 0.0$). This confirms our hypothesis, so we conclude that there is a significant positive correlation between the offline measures and the online measures using the SDBM: the higher the Œî-measures, the higher the win proportions will be. The graphs below also show this: whereas the datapoints are distributed randomly in the graphs that plot the RCM results, we see clear upwards trends in the graphs that plot the SDBM results.\n",
    "</div>\n",
    "\n",
    "<img src='AP_RCM.png'>\n",
    "<img src='AP_SDBM.png'>\n",
    "<img src='nDCG_RCM.png'>\n",
    "<img src='nDCG_SDBM.png'>\n",
    "<img src='ERR_RCM.png'>\n",
    "<img src='ERR_SDBM.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
