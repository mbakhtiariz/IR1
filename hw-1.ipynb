{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework is made by:\n",
    "* Masoumeh Bakhtiariziabari (11813105)\n",
    "* Marianne de Heer Kloots (11138351)\n",
    "* Tharangni Sivaji (11611065)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Part [15 pts]\n",
    "\n",
    "## 1. Hypothesis Testing ‚Äì The problem of multiple comparisons [5 points]\n",
    "How many hypothesis tests, m, does it take to get to (with Type I error for each test = Œ±):\n",
    "* P(m<sup>th</sup> experiment gives significant result | m experiments lacking power to reject H<sub>0</sub>)?\n",
    "* P(at least one significant result | m experiments lacking power to reject H<sub>0</sub>)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "<div style=\"background-color: lightyellow\">\n",
    "<ol>\n",
    "<li><ul>\n",
    "        <li> $$\n",
    "        P(m^{\\text{th}}\\text{ experiment gives significant result} \\mid m \\text{ experiments lacking power to reject } H_0) \\\\\n",
    "        \\approx P(m^{\\text{th}}\\text{ experiment gives significant result} \\mid H_0 \\text{ is true in all m experiments}) \\\\\n",
    "        \\text{(i.e. only the } m^{\\text{th}} \\text{ result is significant whereas } (m-1) \\text{ results are not significant)} \\\\\n",
    "        = \\boldsymbol{((1 - \\alpha)^{m-1})\\cdot\\alpha}\n",
    "        $$<br><br>\n",
    "        <li> $$\n",
    "        P(\\text{at least one significant result} \\mid m \\text{ experiments lacking power to reject } H_0)\\\\\n",
    "        = 1 - P(\\text{no significant result})\\\\\n",
    "        = \\boldsymbol{1 - (1 - \\alpha)^m}\n",
    "        $$\n",
    "    </ul>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bias and unfairness in Interleaving experiments [10 points]\n",
    "Balance interleaving has been shown to be biased in a number of corner cases. An example was given during the lecture with two ranked lists of length 3 being interleaved, and a randomly clicking population of users that resulted in algorithm A winning ‚Öî of the time, even though in theory the percentage of wins should be 50% for both algorithms. Can you come up with a situation of two ranked lists of length 3 and a distribution of clicks over them for which Team-draft interleaving is unfair to the better algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "answer"
    ]
   },
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "*answer here!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!*\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Part [85 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='purple'>[Based on Lecture 1]</font>\n",
    "### Step 1: <font color='darkred'>Simulate Rankings of Relevance for *E* and *P* *(5 points)*</font>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO: add comments\n",
    "from itertools import product\n",
    "from pprint import pprint\n",
    "import random\n",
    "\n",
    "algorithms = ['P', 'E']\n",
    "relevance_grades = {'N', 'HR', 'R'}\n",
    "rankings = [ranking for ranking in product(relevance_grades, repeat=5)]\n",
    "algorithm_rankings = [product(alg, rankings) for alg in algorithms]\n",
    "\n",
    "ranking_pairs = [pair for pair in product(*algorithm_rankings)]\n",
    "print(len(ranking_pairs))\n",
    "#pprint(ranking_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of combinations: 59049\n",
      "number of non-equal combinations: 58806\n",
      "[(('P', ('N', 'R', 'HR', 'N', 'N')), ('E', ('N', 'N', 'R', 'N', 'HR'))),\n",
      " (('P', ('N', 'N', 'N', 'R', 'R')), ('E', ('HR', 'N', 'N', 'R', 'N'))),\n",
      " (('P', ('N', 'HR', 'HR', 'HR', 'HR')), ('E', ('N', 'N', 'N', 'N', 'N'))),\n",
      " (('P', ('HR', 'N', 'R', 'N', 'R')), ('E', ('R', 'HR', 'R', 'N', 'R'))),\n",
      " (('P', ('R', 'HR', 'N', 'N', 'R')), ('E', ('HR', 'N', 'R', 'HR', 'HR'))),\n",
      " (('P', ('R', 'HR', 'N', 'N', 'R')), ('E', ('HR', 'HR', 'N', 'N', 'R'))),\n",
      " (('P', ('N', 'HR', 'N', 'HR', 'N')), ('E', ('HR', 'HR', 'N', 'HR', 'N'))),\n",
      " (('P', ('R', 'HR', 'HR', 'R', 'R')), ('E', ('HR', 'N', 'N', 'HR', 'HR'))),\n",
      " (('P', ('HR', 'N', 'R', 'R', 'N')), ('E', ('HR', 'N', 'R', 'N', 'N'))),\n",
      " (('P', ('R', 'R', 'R', 'N', 'HR')), ('E', ('HR', 'N', 'N', 'R', 'N')))]\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from pprint import pprint\n",
    "import random\n",
    "\n",
    "# define collections of algorithms and relevance grades\n",
    "algorithms = ['P', 'E']\n",
    "relevance_grades = ['N', 'HR', 'R']\n",
    "\n",
    "# all possible ranking sequences\n",
    "# list of rankings [('HR', 'HR', 'HR', 'HR', 'HR') ... ('N', 'N', 'N', 'N', 'N')]\n",
    "rankings = [ranking for ranking in product(relevance_grades, repeat=5)]\n",
    "\n",
    "# all algorithms paired with all rankings \n",
    "# (list of lists with elements e.g. ('P', ('HR', 'HR', 'HR', 'HR', 'HR')))\n",
    "algorithm_rankings = [list(product(alg, rankings)) for alg in algorithms]\n",
    "\n",
    "# all possible pairs of P and E with their rankings\n",
    "all_ranking_pairs = [pair for pair in product(*algorithm_rankings)]\n",
    "\n",
    "# all ranking pairs except equals\n",
    "ranking_pairs = [pair for pair in product(*algorithm_rankings) if pair[0][1] != pair[1][1]]\n",
    "\n",
    "# pretty print\n",
    "print('number of combinations:', len(all_ranking_pairs))\n",
    "print('number of non-equal combinations:', len(ranking_pairs))\n",
    "pprint(random.sample(ranking_pairs, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**explanation/analysis:**\n",
    "\n",
    "We find 59049 different ranking pairs. This makes sense: each ranking pair consists of 10 relevance values (5 produced by each algorithm), all of which can take on any of the 3 grades (N, R, HR). So there should be 3$^{10}$ = 59049 different combinations, which matches our finding.\n",
    "\n",
    "We then exclude all ranking pairs which have exactly the same relevance grade sequences, which leaves us with 58806 different combinations.\n",
    "\n",
    "We have printed a randomly selected sample of 10 ranking pairs as an example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: <font color='darkred'>Implement Evaluation Measures *(10 points)*</font>\n",
    "Implement 1 binary and 2 multi-graded evaluation measures out of the 7 measures mentioned above. \n",
    "\n",
    "(Note 2: Some of the aforementioned measures require the total number of relevant and highly relevant documents in the entire collection ‚Äì pay extra attention on how to find this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#p@K function\n",
    "\n",
    "def precision(k,_ranking_pairs):\n",
    "    \n",
    "    def calc_precision(i,x):\n",
    "        rel_counter = 0.0\n",
    "        prec = 0.0\n",
    "        for j in range(k):\n",
    "            if _ranking_pairs[i][x][1][j] == 'HR' or _ranking_pairs[i][x][1][j] == 'R':\n",
    "                rel_counter += 1\n",
    "                prec += rel_counter/(1.0+j)\n",
    "        return prec\n",
    "    \n",
    "    prec_list = []\n",
    "    for i in range(len(_ranking_pairs)):\n",
    "        p_prec = calc_precision(i,0)\n",
    "        e_prec = calc_precision(i,1)\n",
    "        prec_list.append((p_prec/k,e_prec/k))\n",
    "        #print(p_prec,e_prec)\n",
    "    \n",
    "    return prec_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of all precision values for P & E (should be equal):\n",
      " [ 0.58024691  0.58024691] \n",
      "\n",
      "mean of a random sample of 100 pairs:\n",
      " [ 0.57277778  0.55111111] \n",
      "\n",
      "Precision at K=3 for 10 random queries:\n",
      "query number \t P \t E\n",
      "query  25644 : \t 0.556 \t 1.000\n",
      "query  34691 : \t 0.667 \t 0.333\n",
      "query  35161 : \t 1.000 \t 0.389\n",
      "query  36101 : \t 1.000 \t 0.389\n",
      "query  47635 : \t 0.667 \t 1.000\n",
      "query  26471 : \t 0.667 \t 0.556\n",
      "query  45356 : \t 0.556 \t 0.556\n",
      "query  461 : \t 0.000 \t 0.667\n",
      "query  34524 : \t 0.667 \t 1.000\n",
      "query  40321 : \t 0.333 \t 1.000\n"
     ]
    }
   ],
   "source": [
    "prec = precision(3,ranking_pairs)\n",
    "\n",
    "print('mean of all precision values for P & E (should be equal):\\n', \n",
    "      np.mean(prec, axis=0),'\\n')\n",
    "print('mean of a random sample of 100 pairs:\\n', \n",
    "      np.mean(random.sample(prec, 100), axis=0),'\\n')\n",
    "\n",
    "random_query_index_list = random.sample(range(len(ranking_pairs)), 10)\n",
    "print(\"Precision at K=3 for 10 random queries:\")\n",
    "print(\"query number \\t P \\t E\")\n",
    "for i in random_query_index_list:\n",
    "    print(\"query \",i,\": \\t\",'%.3f'%prec[i][0],\"\\t\",'%.3f'%prec[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_prec(k_max,_ranking_pairs):\n",
    "    temp_list = []\n",
    "    ap_list = [[0,0] for i in range(len(_ranking_pairs))]\n",
    "    for k in range(1,k_max+1):\n",
    "        temp_list = precision(k,_ranking_pairs)\n",
    "        #print(k,temp_list)\n",
    "        for index,item in enumerate(temp_list):\n",
    "            ap_list[index] = (np.array(ap_list[index])+np.array(item)).tolist()\n",
    "    #print(ap_list)\n",
    "    for item in ap_list:\n",
    "        item[:] = [i / k_max for i in item]\n",
    "    return ap_list       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of all AP values for P & E (should be equal):\n",
      " [ 0.59282716  0.59282716] \n",
      "\n",
      "mean of a random sample of 100 pairs:\n",
      " [ 0.56073944  0.60135278] \n",
      "\n",
      "Average Precision for 10 random queries:\n",
      "query number \t P \t\t\t\t AP \t E \t\t\t\t AP\n",
      "query  33643 : \t ('HR', 'R', 'N', 'HR', 'HR') \t 0.813 \t ('N', 'N', 'N', 'HR', 'R') \t 0.038\n",
      "query  44172 : \t ('R', 'N', 'R', 'N', 'R') \t 0.585 \t ('HR', 'HR', 'R', 'N', 'R') \t 0.902\n",
      "query  11696 : \t ('N', 'HR', 'R', 'HR', 'N') \t 0.300 \t ('HR', 'N', 'N', 'N', 'N') \t 0.457\n",
      "query  3840 : \t ('N', 'N', 'HR', 'R', 'N') \t 0.097 \t ('R', 'HR', 'R', 'HR', 'HR') \t 1.000\n",
      "query  51565 : \t ('R', 'HR', 'R', 'R', 'N') \t 0.960 \t ('N', 'N', 'R', 'N', 'HR') \t 0.068\n",
      "query  34186 : \t ('HR', 'R', 'N', 'R', 'N') \t 0.781 \t ('N', 'R', 'HR', 'N', 'HR') \t 0.257\n",
      "query  41527 : \t ('R', 'N', 'HR', 'N', 'N') \t 0.561 \t ('HR', 'R', 'HR', 'N', 'HR') \t 0.902\n",
      "query  53694 : \t ('R', 'R', 'N', 'HR', 'R') \t 0.813 \t ('R', 'HR', 'R', 'HR', 'R') \t 1.000\n",
      "query  5208 : \t ('N', 'N', 'R', 'HR', 'N') \t 0.097 \t ('HR', 'HR', 'R', 'N', 'HR') \t 0.902\n",
      "query  46634 : \t ('R', 'HR', 'N', 'HR', 'N') \t 0.781 \t ('R', 'N', 'N', 'R', 'R') \t 0.526\n"
     ]
    }
   ],
   "source": [
    "k_max = 5\n",
    "avg_prec = average_prec(k_max,ranking_pairs)\n",
    "\n",
    "print('mean of all AP values for P & E (should be equal):\\n', \n",
    "      np.mean(avg_prec, axis=0),'\\n')\n",
    "print('mean of a random sample of 100 pairs:\\n', \n",
    "      np.mean(random.sample(avg_prec, 100), axis=0),'\\n')\n",
    "\n",
    "random_query_index_list = random.sample(range(len(ranking_pairs)), 10)\n",
    "print(\"Average Precision for 10 random queries:\")\n",
    "print(\"query number \\t P \\t\\t\\t\\t AP \\t E \\t\\t\\t\\t AP\")\n",
    "for i in random_query_index_list:\n",
    "    print(\"query \",i,\": \\t\",ranking_pairs[i][0][1], \"\\t\",'%.3f'%avg_prec[i][0],'\\t',ranking_pairs[i][1][1],'\\t','%.3f'%avg_prec[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "explanation/analysis?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------calc ideal dcg based on ground truth [10 HR, 10 R, 10 N]\n",
    "def calc_ideal_dgc(k):\n",
    "    idcg_rel = [0.9]*10 + [0.6]*10 + [0.1]*10\n",
    "    idcg = 0\n",
    "    for index in range(min(k, len(idcg_rel))):\n",
    "        idcg += ((2**idcg_rel[index]) - 1)/(math.log2(2+index))\n",
    "    return idcg\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I used the second formula of DCG from slide 6 of http://www.cs.cornell.edu/courses/cs4300/2013fa/lectures/evaluation-1-4pp.pdf\n",
    "#nDCG is a way to calculate this measure across many independent ____queries____(http://curtis.ml.cmu.edu/w/courses/index.php/Normalized_discounted_cumulative_gain)\n",
    "#Normalize DCG at rank n by the DCG value at rank n of the ideal ranking(stanford slide)\n",
    "\n",
    "import math\n",
    "def ndcg(max_k,query_index,method):    \n",
    "    all_dcg_k = []\n",
    "    \n",
    "    #--------------------make dcg list for all k-----------------------------\n",
    "    for r in range(0,max_k):\n",
    "        if ranking_pairs[query_index][method][1][r] == 'HR':\n",
    "            rel_num = 0.9\n",
    "        elif ranking_pairs[query_index][method][1][r] == 'R':\n",
    "            rel_num = 0.6\n",
    "        else:\n",
    "            rel_num = 0.1\n",
    "            \n",
    "        if len(all_dcg_k) > 0:   \n",
    "            all_dcg_k.append((((2**rel_num) - 1)/(math.log2(2+r))) + all_dcg_k[-1])\n",
    "        else:\n",
    "            all_dcg_k.append(((2**rel_num) - 1)/(math.log2(2+r)))\n",
    "            \n",
    "            \n",
    "    #----------------calculate ideal dcg------------------------------------      \n",
    "    idcg = calc_ideal_dgc(max_k)\n",
    "        \n",
    "    #--------------------------convert dcg to ndcg--------------------------   \n",
    "    if(idcg == 0):\n",
    "        all_dcg_k[:] = [0 for x in all_dcg_k]\n",
    "    else:\n",
    "        all_dcg_k[:] = [x / idcg for x in all_dcg_k]\n",
    "     \n",
    "    return all_dcg_k\n",
    "\n",
    "#-----------------------call ndcg function for all 59000 queries------------------\n",
    "ndcg_list = []\n",
    "max_k = 5\n",
    "for i in range(len(ranking_pairs)):\n",
    "    p_dcg = ndcg(max_k,i,0)\n",
    "    e_dcg = ndcg(max_k,i,1)\n",
    "    ndcg_list.append((p_dcg[-1],e_dcg[-1]))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of all NDCG values for P & E (should be equal):\n",
      " [ 0.55944776  0.55944776] \n",
      "\n",
      "mean of a random sample of 100 pairs:\n",
      " [ 0.58806767  0.56823818] \n",
      "\n",
      "NDCG@5 for 10 random queries:\n",
      "query number \t P \t\t\t\t NDCG \t E \t\t\t\t NDCG\n",
      "query 13919 : \t ('N', 'R', 'N', 'HR', 'N') \t 0.327 \t ('HR', 'HR', 'R', 'N', 'N') \t 0.677 \n",
      "\n",
      "query 13106 : \t ('N', 'R', 'N', 'N', 'N') \t 0.193 \t ('N', 'HR', 'HR', 'N', 'R') \t 0.502 \n",
      "\n",
      "query 10700 : \t ('N', 'HR', 'HR', 'R', 'R') \t 0.577 \t ('N', 'HR', 'R', 'R', 'R') \t 0.508 \n",
      "\n",
      "query 21191 : \t ('HR', 'N', 'N', 'R', 'N') \t 0.469 \t ('HR', 'R', 'N', 'HR', 'N') \t 0.638 \n",
      "\n",
      "query 25505 : \t ('HR', 'N', 'R', 'R', 'N') \t 0.556 \t ('HR', 'N', 'HR', 'HR', 'R') \t 0.751 \n",
      "\n",
      "query 58095 : \t ('R', 'R', 'R', 'R', 'N') \t 0.528 \t ('N', 'N', 'HR', 'R', 'N') \t 0.313 \n",
      "\n",
      "query 30223 : \t ('HR', 'HR', 'HR', 'R', 'HR') \t 0.941 \t ('R', 'R', 'N', 'N', 'N') \t 0.366 \n",
      "\n",
      "query 39213 : \t ('R', 'N', 'N', 'N', 'N') \t 0.257 \t ('N', 'N', 'HR', 'N', 'N') \t 0.238 \n",
      "\n",
      "query 40043 : \t ('R', 'N', 'N', 'HR', 'N') \t 0.391 \t ('HR', 'HR', 'N', 'HR', 'R') \t 0.791 \n",
      "\n",
      "query 47209 : \t ('R', 'HR', 'N', 'R', 'N') \t 0.528 \t ('N', 'N', 'R', 'N', 'HR') \t 0.290 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('mean of all NDCG values for P & E (should be equal):\\n', \n",
    "      np.mean(ndcg_list, axis=0),'\\n')\n",
    "print('mean of a random sample of 100 pairs:\\n', \n",
    "      np.mean(random.sample(ndcg_list, 100), axis=0),'\\n')\n",
    "random_query_index_list = random.sample(range(len(ranking_pairs)), 10)\n",
    "print(\"NDCG@5 for 10 random queries:\")\n",
    "print(\"query number \\t P \\t\\t\\t\\t NDCG \\t E \\t\\t\\t\\t NDCG\")\n",
    "for index in random_query_index_list:\n",
    "    print(\"query %d\"%index,\":\",'\\t',ranking_pairs[index][0][1], \"\\t\", '%.3f'%ndcg_list[index][0],'\\t',ranking_pairs[index][1][1], \"\\t\",'%.3f'%ndcg_list[index][1],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of all ERR values for P & E (should be equal):\n",
      " [ 0.45258501  0.45258501]\n",
      "mean of a random sample of 100 pairs:\n",
      " [ 0.4465199   0.43630261] \n",
      "\n",
      "ERR for 10 random queries:\n",
      "query number \t P \t\t\t\t ERR \t E \t\t\t\t ERR\n",
      "query 52741 : \t ('R', 'R', 'N', 'N', 'HR') \t 0.433 \t ('R', 'R', 'HR', 'HR', 'N') \t 0.491 \n",
      "\n",
      "query 3008 : \t ('N', 'N', 'HR', 'HR', 'N') \t 0.260 \t ('HR', 'N', 'R', 'R', 'N') \t 0.550 \n",
      "\n",
      "query 31265 : \t ('HR', 'HR', 'R', 'HR', 'N') \t 0.640 \t ('N', 'HR', 'R', 'N', 'R') \t 0.332 \n",
      "\n",
      "query 49965 : \t ('R', 'HR', 'HR', 'R', 'R') \t 0.527 \t ('HR', 'HR', 'N', 'HR', 'R') \t 0.632 \n",
      "\n",
      "query 23263 : \t ('HR', 'N', 'HR', 'R', 'N') \t 0.575 \t ('N', 'HR', 'N', 'HR', 'HR') \t 0.350 \n",
      "\n",
      "query 10100 : \t ('N', 'HR', 'HR', 'HR', 'R') \t 0.382 \t ('R', 'N', 'HR', 'R', 'R') \t 0.439 \n",
      "\n",
      "query 12024 : \t ('N', 'HR', 'R', 'HR', 'HR') \t 0.371 \t ('R', 'N', 'N', 'HR', 'R') \t 0.397 \n",
      "\n",
      "query 17882 : \t ('N', 'R', 'R', 'N', 'HR') \t 0.285 \t ('R', 'R', 'N', 'N', 'HR') \t 0.433 \n",
      "\n",
      "query 8041 : \t ('N', 'HR', 'N', 'R', 'N') \t 0.305 \t ('N', 'R', 'N', 'N', 'R') \t 0.265 \n",
      "\n",
      "query 52770 : \t ('R', 'R', 'N', 'N', 'R') \t 0.491 \t ('N', 'N', 'HR', 'HR', 'R') \t 0.272 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ERR\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def numerical(relevance_sequence):\n",
    "    \"\"\"\n",
    "    Convert a relevance grade sequence to a sequence of numerical \n",
    "    values, based on the relevance grades given.\n",
    "    E.g. ['HR', 'HR', 'HR', 'R', 'N'] returns [2, 2, 2, 1, 0]\n",
    "    \"\"\"\n",
    "    numerical_relevance_sequence = [0.6 if grade == 'R' \\\n",
    "                                    else 0.9 if grade == 'HR' else 0.1 \\\n",
    "                                    for grade in relevance_sequence]\n",
    "    return numerical_relevance_sequence\n",
    "\n",
    "def R_function(g, g_max):\n",
    "    return (2**g - 1)/(2**g_max)\n",
    "\n",
    "def ERR(relevance_sequence):\n",
    "    \"\"\"\n",
    "    Compute the ERR based on Algorithm 2 in \n",
    "    https://pdfs.semanticscholar.org/7e3c/f6492128f915112ca01dcb77c766129e65cb.pdf\n",
    "    \"\"\"\n",
    "    p = 1\n",
    "    ERR = 0\n",
    "    n = len(relevance_sequence)\n",
    "    g_max = max(relevance_sequence)\n",
    "    \n",
    "    for r in range(1, n + 1):\n",
    "        g = relevance_sequence[r - 1]\n",
    "        R = R_function(g, g_max)\n",
    "        ERR = ERR + p * (R/r)\n",
    "        p = p * (1 - R)\n",
    "    return ERR\n",
    "\n",
    "ERR_list = []\n",
    "for P, E in ranking_pairs:\n",
    "    ERR_P = ERR(numerical(P[1]))\n",
    "    ERR_E = ERR(numerical(E[1]))\n",
    "    ERR_list.append((ERR_P, ERR_E))\n",
    "    \n",
    "print('mean of all ERR values for P & E (should be equal):\\n', \n",
    "      np.mean(ERR_list, axis=0))\n",
    "print('mean of a random sample of 100 pairs:\\n', \n",
    "      np.mean(random.sample(ERR_list, 100), axis=0),'\\n')\n",
    "\n",
    "random_query_index_list = random.sample(range(len(ranking_pairs)), 10)\n",
    "print(\"ERR for 10 random queries:\")\n",
    "print(\"query number \\t P \\t\\t\\t\\t ERR \\t E \\t\\t\\t\\t ERR\")\n",
    "for index in random_query_index_list:\n",
    "    print(\"query %d\"%index,\":\",'\\t',ranking_pairs[index][0][1], \"\\t\", '%.3f'%ERR_list[index][0],'\\t',ranking_pairs[index][1][1], \"\\t\",'%.3f'%ERR_list[index][1],'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: <font color='darkred'>Calculate the ùõ•measure *(0 points)*</font>\n",
    "For the three measures and all *P* and *E* ranking pairs constructed above calculate the difference: ùõ•measure = measure<sub>E</sub>-measure<sub>P</sub>. Consider only those pairs for which *E* outperforms *P*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def measure_diff(measure_list, _ranking_pairs, query_index_list):\n",
    "    pos_result = []\n",
    "    diff_list = []\n",
    "    for i in query_index_list:\n",
    "        diff = measure_list[i][1] - measure_list[i][0]\n",
    "        if diff > 0:\n",
    "            pos_result.append(_ranking_pairs[i])\n",
    "            diff_list.append(diff)\n",
    "    return pos_result, diff_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on ùõ•measure for AP: \n",
      "There are  27962  experiments where E out perform P\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "Based on ùõ•measure for nDCG: \n",
      "There are  29403  experiments where E out perform P\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "Based on ùõ•measure for ERR: \n",
      "There are  29403  experiments where E out perform P\n"
     ]
    }
   ],
   "source": [
    "random_query_index_list = [i for i in range(len(ranking_pairs))]\n",
    "#for ap:\n",
    "print(\"Based on ùõ•measure for AP: \")\n",
    "ap_pos_result,diff_avgprec = measure_diff(avg_prec, ranking_pairs, random_query_index_list)\n",
    "print(\"There are \",len(ap_pos_result),\" experiments where E out perform P\")\n",
    "print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "#for nDCG:\n",
    "print(\"Based on ùõ•measure for nDCG: \")\n",
    "ndcg_pos_result,diff_nDCG = measure_diff(ndcg_list, ranking_pairs, random_query_index_list)\n",
    "print(\"There are \",len(ndcg_pos_result),\" experiments where E out perform P\")\n",
    "print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "#for ERR:\n",
    "print(\"Based on ùõ•measure for ERR: \")\n",
    "err_pos_result,diff_ERR = measure_diff(ERR_list, ranking_pairs, random_query_index_list)\n",
    "print(\"There are \",len(err_pos_result),\" experiments where E out perform P\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24462 27962 29403 29403\n"
     ]
    }
   ],
   "source": [
    "# code\n",
    "diff_precision = [precision_E - precision_P for precision_P, precision_E in prec if precision_E > precision_P]\n",
    "diff_avgprec = [avgprec_E - avgprec_P for avgprec_P, avgprec_E in avg_prec if avgprec_E > avgprec_P]\n",
    "diff_nDCG = [nDCG_E - nDCG_P for nDCG_P, nDCG_E in ndcg_list if nDCG_E > nDCG_P]\n",
    "diff_ERR = [ERR_E - ERR_P for ERR_P, ERR_E in ERR_list if ERR_E > ERR_P]\n",
    "print(len(diff_precision), len(diff_avgprec), len(diff_nDCG), len(diff_ERR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "explanation/analysis?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='purple'>[Based on Lecture 2]</font>\n",
    "### Step 4: <font color='darkred'>Implement Interleaving *(15 points)*</font>\n",
    "Implement 2 interleaving algorithms: (1) Team-Draft Interleaving OR Balanced Interleaving, AND (2), Probabilistic Interleaving. The interleaving algorithms should (a) given two rankings of relevance interleave them into a single ranking, and (b) given the users clicks on the interleaved ranking assign credit to the algorithms that produced the rankings.\n",
    "\n",
    "(Note 4: Note here that as opposed to a normal interleaving experiment where rankings consists of urls or docids, in our case the rankings consist of relevance labels. Hence in this case (a) you will assume that E and P return different documents, (b) the interleaved ranking will also be a ranking of labels.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Team-Draft Interleaving\n",
    "#P and E as two interleaving lists\n",
    "#as output we will have \n",
    "\n",
    "def team_draft_interleaving(_ranking_pairs):\n",
    "    \"\"\"\n",
    "    Input: [[(documents for P for query 1,documents for E for query 1)]\n",
    "            ,[(documents for P for query 2,documents for E for query 2)]\n",
    "            ,[(documents for P for query 3,documents for E for query 3)],...]\n",
    "            \n",
    "    output: [[combined documents of P and E for query 1]\n",
    "            ,[combined documents of P and E for query 2]\n",
    "            ,[combined documents of P and E for query 3],...]\n",
    "            \n",
    "    Here we assume all of documents in A and B are unique and independent otherwise we should change the\n",
    "    code to eliminate the document from both A and B when we insert it to I.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_I = []\n",
    "    for i in range(len(_ranking_pairs)):\n",
    "        A = _ranking_pairs[i][0][1]\n",
    "        B = _ranking_pairs[i][1][1]\n",
    "        team_A = []\n",
    "        team_B = []\n",
    "        I = []\n",
    "        while len(team_A) < len(A) or len(team_B) < len(B):\n",
    "            RandBit = random.getrandbits(1)\n",
    "            #pick from A\n",
    "            if len(team_A) < len(team_B) or (len(team_A) == len(team_B) and RandBit == 1):\n",
    "                I.append((A[len(team_A)],0))\n",
    "                team_A.append(I[-1])        \n",
    "            else:\n",
    "                #pick from B\n",
    "                I.append((B[len(team_B)],1))\n",
    "                team_B.append(I[-1])\n",
    "        all_I.append(I)\n",
    "    return all_I\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_winner(_click,_all_I):\n",
    "    winner_list = []\n",
    "    for query_index in range(len(_all_I)):\n",
    "        h_a = 0\n",
    "        h_b = 0\n",
    "        for index, item in enumerate(_all_I[query_index]):\n",
    "            if _click[query_index][index] > 0:\n",
    "                if item[1] == 0:\n",
    "                    h_a += 1\n",
    "                if item[1] == 1:\n",
    "                    h_b += 1\n",
    "        if(h_a > h_b):\n",
    "            winner_list.append('P')\n",
    "        elif(h_a == h_b):\n",
    "            winner_list.append('NoPref')\n",
    "        else:\n",
    "            winner_list.append('E')\n",
    "    return winner_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "explanation/analysis?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='purple'>[Based on Lecture 3]</font>\n",
    "### Step 5: <font color='darkred'>Implement User Clicks Simulation *(15 points)*</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn_rcm_param(_train_file):\n",
    "    c = 0\n",
    "    query_num = 0.0\n",
    "    click_num = 0.0\n",
    "    doc = Counter()\n",
    "    for line in _train_file:\n",
    "        info = line.split()\n",
    "        #print(info[2] == 'Q')\n",
    "        if info[2] == 'Q':\n",
    "            #query_num += 1\n",
    "            for i in info[5:]:\n",
    "                doc[i] += 1\n",
    "        if info[2] == 'C':\n",
    "            click_num += 1\n",
    "        \n",
    "    #print(doc)\n",
    "    doc_num = len(doc)#query_num*10\n",
    "    #print(doc_num)\n",
    "    _rcm_param = click_num/doc_num\n",
    "    return _rcm_param\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2830797784644546"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file = open(\"YandexRelPredChallenge.txt\",\"r\")\n",
    "ruoo = learn_rcm_param(train_file)\n",
    "ruoo"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#SDBM:\n",
    "# I have to change the formula for finiding the number of last click-------------------!!!!!!\n",
    "\n",
    "def learn_sdbm_param(_train_file):\n",
    "    last_session = '0'\n",
    "    satisfy_num = 0.0\n",
    "    last_action = ''\n",
    "    click_num = 0.0\n",
    "    \n",
    "    for line in _train_file:\n",
    "        info = line.split()\n",
    "        \n",
    "        if info[2] == 'C':\n",
    "            click_num += 1\n",
    "        \n",
    "\n",
    "        if info[0] == last_session:\n",
    "            last_action = info[2]\n",
    "        else:\n",
    "            if last_action == 'C':                \n",
    "                satisfy_num += 1\n",
    "            last_session = info[0]\n",
    "    if last_action == 'C':                \n",
    "        satisfy_num += 1     \n",
    "\n",
    "        \n",
    "    sigma = satisfy_num/click_num  \n",
    "\n",
    "    return sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SDBM:\n",
    "def learn_sdbm_param(_train_file):\n",
    "    prev_action = ''\n",
    "    satisfy_num = 0\n",
    "    click_num = 0\n",
    "    \n",
    "    for line in _train_file:\n",
    "        info = line.split()        \n",
    "        \n",
    "        if info[2] == 'C':\n",
    "            click_num += 1\n",
    "\n",
    "        if info[2] == 'Q' and prev_action == 'C':\n",
    "            satisfy_num += 1\n",
    "        prev_action = info[2]\n",
    "    if prev_action == 'C':\n",
    "        satisfy_num += 1\n",
    "    sigma = satisfy_num/click_num\n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def sdbm_predict_click_prob(_I, _sigma):\n",
    "    prediction = []\n",
    "    attraction = {'HR':0.9,'R':0.6,'N':0.1}\n",
    "    for index, item in enumerate(_I):\n",
    "        if index == 0:\n",
    "            prediction.append(attraction[item[0]])\n",
    "        else:\n",
    "            #er = prediction[index-1]*(attraction[_I[index-1]]*(1-_sigma) + (1 - attraction[_I[index-1]]))            \n",
    "            er = prediction[index-1]*(1-_sigma * attraction[_I[index-1][0]])\n",
    "            prediction.append(attraction[item[0]] * er)\n",
    "                                       \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "explanation/analysis?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: <font color='darkred'>Simulate Interleaving Experiment *(10 points)*</font>\n",
    "Having implemented the click models, it is time to run the simulated experiment.\n",
    "\n",
    "For each of interleaved ranking run N simulations for each one of the click models implemented and measure the proportion *p* of wins for E.\n",
    "(Note 7: Some of the models above include an attractiveness parameter *a*<sub>uq</sub>. Use the relevance label to assign this parameter by setting *a*<sub>uq</sub> for a document u in the ranked list accordingly. (See [Click Models for Web Search](http://clickmodels.weebly.com/uploads/5/2/2/5/52257029/mc2015-clickmodels.pdf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate_rcm_click(_I,_ru):\n",
    "    simulated_click = []\n",
    "    for i in range(len(_I)):\n",
    "        if(random.random() < _ru):\n",
    "            simulated_click.append(1)\n",
    "        else:\n",
    "            simulated_click.append(0)\n",
    "    return simulated_click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate_sdbm_click(_I,_sigma):\n",
    "    simulated_click = [0]*len(_I)\n",
    "    attraction = {'HR':0.9,'R':0.6,'N':0.1}\n",
    "    for index,item in enumerate(_I):\n",
    "        if random.random() < attraction[item[0]]:\n",
    "            simulated_click[index]= 1\n",
    "            if random.random() < _sigma:\n",
    "                return simulated_click\n",
    "    return simulated_click"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For nDCG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ndcg_I = team_draft_interleaving(ndcg_pos_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying RCM on nDCG: \n",
      "---------------------\n",
      "Winner list for first 10 queries:  ['P', 'E', 'E', 'E', 'NoPref', 'NoPref', 'NoPref', 'E', 'P', 'NoPref']\n",
      "Overall counts for nDCG_RCM:   \t Counter({'P': 10689, 'E': 10617, 'NoPref': 8097})\n",
      "Proportion of I where E > P:   \t 0.4983103351168685\n"
     ]
    }
   ],
   "source": [
    "print(\"Applying RCM on nDCG: \\n---------------------\")\n",
    "train_file = open(\"YandexRelPredChallenge.txt\",\"r\")\n",
    "\n",
    "rho = learn_rcm_param(train_file)\n",
    "\n",
    "rcm_click_list = [simulate_rcm_click(ndcg_I[i],rho) for i in range(len(ndcg_I))]\n",
    "\n",
    "ndcg_rcm_winner_list = find_winner(rcm_click_list, ndcg_I)\n",
    "\n",
    "NDCG_RCM = Counter(ndcg_rcm_winner_list)\n",
    "\n",
    "proportion_NDCG_RCM = NDCG_RCM['E']/(NDCG_RCM['P'] + NDCG_RCM['E'])\n",
    "\n",
    "print('Winner list for first 10 queries: ',ndcg_rcm_winner_list[:10])\n",
    "print(\"Overall counts for nDCG_RCM:  \",'\\t',NDCG_RCM)\n",
    "print(\"Proportion of I where E > P:  \",'\\t',proportion_NDCG_RCM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying SDBN on nDCG: \n",
      "----------------------\n",
      "Winner list for first 10 queries:  ['E', 'E', 'E', 'E', 'NoPref', 'P', 'E', 'P', 'P', 'E']\n",
      "Overall counts for nDCG_SDBN:  \t Counter({'E': 16115, 'P': 7025, 'NoPref': 6263})\n",
      "Proportion of I where E > P:   \t 0.6964131374243734\n"
     ]
    }
   ],
   "source": [
    "print(\"Applying SDBN on nDCG: \\n----------------------\")\n",
    "train_file = open(\"YandexRelPredChallenge.txt\",\"r\")\n",
    "\n",
    "sigma = learn_sdbm_param(train_file)\n",
    "\n",
    "sdbm_click_list = [simulate_sdbm_click(ndcg_I[i],sigma) for i in range(len(ndcg_I))]\n",
    "\n",
    "ndcg_sdbm_winner_list = find_winner(sdbm_click_list, ndcg_I)\n",
    "\n",
    "NDCG_SDBM = Counter(ndcg_sdbm_winner_list)\n",
    "\n",
    "proportion_NDCG_SDBM = NDCG_SDBM['E']/(NDCG_SDBM['P'] + NDCG_SDBM['E'])\n",
    "\n",
    "print('Winner list for first 10 queries: ',ndcg_sdbm_winner_list[:10])\n",
    "print(\"Overall counts for nDCG_SDBN: \",'\\t',NDCG_SDBM)\n",
    "print(\"Proportion of I where E > P:  \",'\\t',proportion_NDCG_SDBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For ERR: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "err_I = team_draft_interleaving(err_pos_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying RCM on ERR: \n",
      "--------------------\n",
      "Winner list for first 10 queries:  ['E', 'E', 'P', 'P', 'NoPref', 'E', 'P', 'P', 'NoPref', 'P']\n",
      "Overall counts for RCM_ERR:  \t Counter({'E': 10625, 'P': 10536, 'NoPref': 8242})\n",
      "Proportion of I where E > P:   \t 0.5021029251925713\n"
     ]
    }
   ],
   "source": [
    "print(\"Applying RCM on ERR: \\n--------------------\")\n",
    "train_file = open(\"YandexRelPredChallenge.txt\",\"r\")\n",
    "\n",
    "ru = learn_rcm_param(train_file)\n",
    "\n",
    "rcm_click_list = [simulate_rcm_click(err_I[i],ru) for i in range(len(err_I))]\n",
    "\n",
    "err_rcm_winner_list = find_winner(rcm_click_list, err_I)\n",
    "\n",
    "ERR_RCM = Counter(err_rcm_winner_list)\n",
    "\n",
    "proportion_ERR_RCM = ERR_RCM['E']/(ERR_RCM['P'] + ERR_RCM['E'])\n",
    "\n",
    "print('Winner list for first 10 queries: ',err_rcm_winner_list[:10])\n",
    "print(\"Overall counts for RCM_ERR: \",'\\t',ERR_RCM)\n",
    "print(\"Proportion of I where E > P:  \",'\\t',proportion_ERR_RCM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying SDBN on ERR: \n",
      "---------------------\n",
      "Winner list for first 10 queries:  ['P', 'NoPref', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E']\n",
      "Overall counts for SDBN_ERR:  \t Counter({'E': 16404, 'P': 6756, 'NoPref': 6243})\n",
      "Proportion of I where E > P:   \t 0.7082901554404145\n"
     ]
    }
   ],
   "source": [
    "print(\"Applying SDBN on ERR: \\n---------------------\")\n",
    "train_file = open(\"YandexRelPredChallenge.txt\",\"r\")\n",
    "\n",
    "sigma = learn_sdbm_param(train_file)\n",
    "\n",
    "sdbm_click_list = [simulate_sdbm_click(err_I[i],sigma) for i in range(len(err_I))]\n",
    "\n",
    "err_sdbm_winner_list = find_winner(sdbm_click_list, err_I)\n",
    "\n",
    "ERR_SDBM = Counter(err_sdbm_winner_list)\n",
    "\n",
    "proportion_ERR_SDBM = ERR_SDBM['E']/(ERR_SDBM['P'] + ERR_SDBM['E'])\n",
    "\n",
    "print('Winner list for first 10 queries: ',err_sdbm_winner_list[:10])\n",
    "print(\"Overall counts for SDBN_ERR: \",'\\t',ERR_SDBM)\n",
    "print(\"Proportion of I where E > P:  \",'\\t',proportion_ERR_SDBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For AP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ap_I = team_draft_interleaving(ap_pos_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying RCM on AP: \n",
      "--------------------\n",
      "Winner list for first 10 queries:  ['P', 'P', 'NoPref', 'P', 'P', 'E', 'NoPref', 'P', 'NoPref', 'E']\n",
      "Overall counts for RCM_ERR:  \t Counter({'P': 10118, 'E': 9861, 'NoPref': 7983})\n",
      "Proportion of I where E > P:   \t 0.49356824665899196\n"
     ]
    }
   ],
   "source": [
    "print(\"Applying RCM on AP: \\n--------------------\")\n",
    "train_file = open(\"YandexRelPredChallenge.txt\",\"r\")\n",
    "\n",
    "rhoo = learn_rcm_param(train_file)\n",
    "\n",
    "rcm_click_list = [simulate_rcm_click(ap_I[i],rhoo) for i in range(len(ap_I))]\n",
    "\n",
    "ap_rcm_winner_list = find_winner(rcm_click_list, ap_I)\n",
    "\n",
    "AP_RCM = Counter(ap_rcm_winner_list)\n",
    "\n",
    "proportion_AP_RCM = AP_RCM['E']/(AP_RCM['P'] + AP_RCM['E'])\n",
    "\n",
    "print('Winner list for first 10 queries: ',ap_rcm_winner_list[:10])\n",
    "print(\"Overall counts for RCM_ERR: \",'\\t',AP_RCM)\n",
    "print(\"Proportion of I where E > P:  \",'\\t',proportion_AP_RCM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying SDBN on AP: \n",
      "---------------------\n",
      "Winner list for first 10 queries:  ['NoPref', 'E', 'E', 'E', 'NoPref', 'E', 'E', 'NoPref', 'E', 'E']\n",
      "Overall counts for SDBN_ERR:  \t Counter({'E': 15366, 'P': 6748, 'NoPref': 5848})\n",
      "Proportion of I where E > P:   \t 0.6948539386813783\n"
     ]
    }
   ],
   "source": [
    "print(\"Applying SDBN on AP: \\n---------------------\")\n",
    "train_file = open(\"YandexRelPredChallenge.txt\",\"r\")\n",
    "\n",
    "sigmaa = learn_sdbm_param(train_file)\n",
    "\n",
    "sdbm_click_list = [simulate_sdbm_click(ap_I[i],sigmaa) for i in range(len(ap_I))]\n",
    "\n",
    "ap_sdbm_winner_list = find_winner(sdbm_click_list, ap_I)\n",
    "\n",
    "AP_SDBM = Counter(ap_sdbm_winner_list)\n",
    "\n",
    "proportion_AP_SDBM = AP_SDBM['E']/(AP_SDBM['P'] + AP_SDBM['E'])\n",
    "\n",
    "print('Winner list for first 10 queries: ',ap_sdbm_winner_list[:10])\n",
    "print(\"Overall counts for SDBN_ERR: \",'\\t',AP_SDBM)\n",
    "print(\"Proportion of I where E > P:  \",'\\t',proportion_AP_SDBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "From the above results it can be observed that ERR is a better Evaluation Measure\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: <font color='darkred'>Results and Analysis *(30 points)*</font>\n",
    "Compare the results of the offline experiments (i.e. the values of the ùõ•measure) with the results of the online experiment (i.e. proportion of wins), analyze them and reach your conclusions regarding their agreement.\n",
    "* Use easy to read and comprehend visuals to demonstrate the results;\n",
    "* Analyze the results on the basis of\n",
    "    * the evaluation measure used,\n",
    "    * the interleaving method used,\n",
    "    * the click model used.\n",
    "* Report and ground your conclusions.\n",
    "\n",
    "(Note 8: This is the place where you need to demonstrate your deeper understanding of what you have implemented so far; hence the large number of points assigned. Make sure you clearly do that so that the examiner of your work can grade it accordingly.)\n",
    "\n",
    "<u>Yandex Click Log File</u>:\n",
    "\n",
    "The dataset includes user sessions extracted from Yandex logs, with queries, URL rankings and clicks. To allay privacy concerns the user data is fully anonymized. So, only meaningless numeric IDs of queries, sessions, and URLs are released. The queries are grouped only by sessions and no user IDs are provided. The dataset consists of several parts. Logs represent a set of rows, where each row represents one of the possible user actions: query or click.\n",
    "\n",
    "In the case of a Query:\n",
    "\n",
    "    SessionID TimePassed TypeOfAction QueryID RegionID ListOfURLs\n",
    "\n",
    "\n",
    "In the case of a Click:\n",
    "\n",
    "    SessionID TimePassed TypeOfAction URLID\n",
    "\n",
    "\n",
    "* `SessionID` - the unique identifier of the user session.\n",
    "* `TimePassed` - the time elapsed since the beginning of the current session in standard time units.\n",
    "* `TypeOfAction` - type of user action. This may be either a query (Q), or a click (C).\n",
    "* `QueryID` - the unique identifier of the request.\n",
    "* `RegionID` - the unique identifier of the country from which a given query. This identifier may take four values.\n",
    "* `URLID` - the unique identifier of the document.\n",
    "* `ListOfURLs` - the list of documents from left to right as they have been shown to users on the page extradition Yandex (top to bottom).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision vs. average precision\n",
      "0.440397350993 0.385633681425 \t Ttest_indResult(statistic=24.810331399370998, pvalue=4.1906255886023189e-135) \n",
      "\n",
      "precision vs. nDCG\n",
      "0.440397350993 0.205250570325 \t Ttest_indResult(statistic=137.56254633997935, pvalue=0.0) \n",
      "\n",
      "precision vs. ERR\n",
      "0.440397350993 0.156888482957 \t Ttest_indResult(statistic=178.15802130708377, pvalue=0.0) \n",
      "\n",
      "average precision vs. nDCG\n",
      "0.385633681425 0.205250570325 \t Ttest_indResult(statistic=102.46682270004969, pvalue=0.0) \n",
      "\n",
      "average precision vs. ERR\n",
      "0.385633681425 0.156888482957 \t Ttest_indResult(statistic=137.7343976707223, pvalue=0.0) \n",
      "\n",
      "nDCG vs. ERR\n",
      "0.205250570325 0.156888482957 \t Ttest_indResult(statistic=44.163141900375052, pvalue=0.0) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "np.random.seed(123)\n",
    "\n",
    "diff_measure_lists = {'precision': diff_precision, 'average precision': diff_avgprec, \n",
    "                      'nDCG': diff_nDCG, 'ERR': diff_ERR}\n",
    "\n",
    "for measure_1, measure_2 in combinations(diff_measure_lists.keys(), 2):\n",
    "    print(measure_1, 'vs.', measure_2)\n",
    "    measure_1 = diff_measure_lists[measure_1]\n",
    "    measure_2 = diff_measure_lists[measure_2]\n",
    "    print(np.mean(measure_1), np.mean(measure_2), '\\t', stats.ttest_ind(measure_1, measure_2), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "explanation/analysis?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
