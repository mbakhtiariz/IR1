{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval 1#\n",
    "## Assignment 2: Retrieval models [100 points] ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will get familiar with basic and advanced information retrieval concepts. You will implement different information retrieval ranking models and evaluate their performance.\n",
    "\n",
    "We provide you with a Indri index. To query the index, you'll use a Python package ([pyndri](https://github.com/cvangysel/pyndri)) that allows easy access to the underlying document statistics.\n",
    "\n",
    "For evaluation you'll use the [TREC Eval](https://github.com/usnistgov/trec_eval) utility, provided by the National Institute of Standards and Technology of the United States. TREC Eval is the de facto standard way to compute Information Retrieval measures and is frequently referenced in scientific papers.\n",
    "\n",
    "This is a **groups-of-three assignment**, the deadline is **Wednesday, January 31st**. Code quality, informative comments and convincing analysis of the results will be considered when grading. Submission should be done through blackboard, questions can be asked on the course [Piazza](piazza.com/university_of_amsterdam/spring2018/52041inr6y/home).\n",
    "\n",
    "### Technicalities (must-read!) ###\n",
    "\n",
    "The assignment directory is organized as follows:\n",
    "   * `./assignment.ipynb` (this file): the description of the assignment.\n",
    "   * `./index/`: the index we prepared for you.\n",
    "   * `./ap_88_90/`: directory with ground-truth and evaluation sets:\n",
    "      * `qrel_test`: test query relevance collection (**test set**).\n",
    "      * `qrel_validation`: validation query relevance collection (**validation set**).\n",
    "      * `topics_title`: semicolon-separated file with query identifiers and terms.\n",
    "\n",
    "You will need the following software packages (tested with Python 3.5 inside [Anaconda](https://conda.io/docs/user-guide/install/index.html)):\n",
    "   * Python 3.5 and Jupyter\n",
    "   * Indri + Pyndri (Follow the installation instructions [here](https://github.com/nickvosk/pyndri/blob/master/README.md))\n",
    "   * gensim [link](https://radimrehurek.com/gensim/install.html)\n",
    "   * TREC Eval [link](https://github.com/usnistgov/trec_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TREC Eval primer ###\n",
    "The TREC Eval utility can be downloaded and compiled as follows:\n",
    "\n",
    "    git clone https://github.com/usnistgov/trec_eval.git\n",
    "    cd trec_eval\n",
    "    make\n",
    "\n",
    "TREC Eval computes evaluation scores given two files: ground-truth information regarding relevant documents, named *query relevance* or *qrel*, and a ranking of documents for a set of queries, referred to as a *run*. The *qrel* will be supplied by us and should not be changed. For every retrieval model (or combinations thereof) you will generate a run of the top-1000 documents for every query. The format of the *run* file is as follows:\n",
    "\n",
    "    $query_identifier Q0 $document_identifier $rank_of_document_for_query $query_document_similarity $run_identifier\n",
    "    \n",
    "where\n",
    "   * `$query_identifier` is the unique identifier corresponding to a query (usually this follows a sequential numbering).\n",
    "   * `Q0` is a legacy field that you can ignore.\n",
    "   * `$document_identifier` corresponds to the unique identifier of a document (e.g., APXXXXXXX where AP denotes the collection and the Xs correspond to a unique numerical identifier).\n",
    "   * `$rank_of_document_for_query` denotes the rank of the document for the particular query. This field is ignored by TREC Eval and is only maintained for legacy support. The ranks are computed by TREC Eval itself using the `$query_document_similarity` field (see next). However, it remains good practice to correctly compute this field.\n",
    "   * `$query_document_similarity` is a score indicating the similarity between query and document where a higher score denotes greater similarity.\n",
    "   * `$run_identifier` is an identifier of the run. This field is for your own convenience and has no purpose beyond bookkeeping.\n",
    "   \n",
    "For example, say we have two queries: `Q1` and `Q2` and we rank three documents (`DOC1`, `DOC2`, `DOC3`). For query `Q1`, we find the following similarity scores `score(Q1, DOC1) = 1.0`, `score(Q1, DOC2) = 0.5`, `score(Q1, DOC3) = 0.75`; and for `Q2`: `score(Q2, DOC1) = -0.1`, `score(Q2, DOC2) = 1.25`, `score(Q1, DOC3) = 0.0`. We can generate run using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 Q0 DOC1 1 1.0 example\n",
      "Q1 Q0 DOC3 2 0.75 example\n",
      "Q1 Q0 DOC2 3 0.5 example\n",
      "Q2 Q0 DOC2 1 1.25 example\n",
      "Q2 Q0 DOC3 2 0.0 example\n",
      "Q2 Q0 DOC1 3 -0.1 example\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def write_run(model_name, data, out_f,\n",
    "              max_objects_per_query=sys.maxsize,\n",
    "              skip_sorting=False):\n",
    "    \"\"\"\n",
    "    Write a run to an output file.\n",
    "    Parameters:\n",
    "        - model_name: identifier of run.\n",
    "        - data: dictionary mapping topic_id to object_assesments;\n",
    "            object_assesments is an iterable (list or tuple) of\n",
    "            (relevance, object_id) pairs.\n",
    "            The object_assesments iterable is sorted by decreasing order.\n",
    "        - out_f: output file stream.\n",
    "        - max_objects_per_query: cut-off for number of objects per query.\n",
    "    \"\"\"\n",
    "    for subject_id, object_assesments in data.items():\n",
    "        if not object_assesments:\n",
    "            logging.warning('Received empty ranking for %s; ignoring.',\n",
    "                            subject_id)\n",
    "\n",
    "            continue\n",
    "\n",
    "        # Probe types, to make sure everything goes alright.\n",
    "        # assert isinstance(object_assesments[0][0], float) or \\\n",
    "        #     isinstance(object_assesments[0][0], np.float32)\n",
    "        assert isinstance(object_assesments[0][1], str) or \\\n",
    "            isinstance(object_assesments[0][1], bytes)\n",
    "\n",
    "        if not skip_sorting:\n",
    "            object_assesments = sorted(object_assesments, reverse=True)\n",
    "\n",
    "        if max_objects_per_query < sys.maxsize:\n",
    "            object_assesments = object_assesments[:max_objects_per_query]\n",
    "\n",
    "        if isinstance(subject_id, bytes):\n",
    "            subject_id = subject_id.decode('utf8')\n",
    "\n",
    "        for rank, (relevance, object_id) in enumerate(object_assesments):\n",
    "            if isinstance(object_id, bytes):\n",
    "                object_id = object_id.decode('utf8')\n",
    "\n",
    "            out_f.write(\n",
    "                '{subject} Q0 {object} {rank} {relevance} '\n",
    "                '{model_name}\\n'.format(\n",
    "                    subject=subject_id,\n",
    "                    object=object_id,\n",
    "                    rank=rank + 1,\n",
    "                    relevance=relevance,\n",
    "                    model_name=model_name))\n",
    "            \n",
    "# The following writes the run to standard output.\n",
    "# In your code, you should write the runs to local\n",
    "# storage in order to pass them to trec_eval.\n",
    "write_run(\n",
    "    model_name='example',\n",
    "    data={\n",
    "        'Q1': ((1.0, 'DOC1'), (0.5, 'DOC2'), (0.75, 'DOC3')),\n",
    "        'Q2': ((-0.1, 'DOC1'), (1.25, 'DOC2'), (0.0, 'DOC3')),\n",
    "    },\n",
    "    out_f=sys.stdout,\n",
    "    max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, imagine that we know that `DOC1` is relevant and `DOC3` is non-relevant for `Q1`. In addition, for `Q2` we only know of the relevance of `DOC3`. The query relevance file looks like:\n",
    "\n",
    "    Q1 0 DOC1 1\n",
    "    Q1 0 DOC3 0\n",
    "    Q2 0 DOC3 1\n",
    "    \n",
    "We store the run and qrel in files `example.run` and `example.qrel` respectively on disk. We can now use TREC Eval to compute evaluation measures. In this example, we're only interested in Mean Average Precision and we'll only show this below for brevity. However, TREC Eval outputs much more information such as NDCG, recall, precision, etc.\n",
    "\n",
    "    $ trec_eval -m all_trec -q example.qrel example.run | grep -E \"^map\\s\"\n",
    "    > map                   \tQ1\t1.0000\n",
    "    > map                   \tQ2\t0.5000\n",
    "    > map                   \tall\t0.7500\n",
    "    \n",
    "Now that we've discussed the output format of rankings and how you can compute evaluation measures from these rankings, we'll now proceed with an overview of the indexing framework you'll use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyndri primer ###\n",
    "For this assignment you will use [Pyndri](https://github.com/cvangysel/pyndri) [[1](https://arxiv.org/abs/1701.00749)], a python interface for [Indri](https://www.lemurproject.org/indri.php). We have indexed the document collection and you can query the index using Pyndri. We will start by giving you some examples of what Pyndri can do:\n",
    "\n",
    "First we read the document collection index with Pyndri:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyndri\n",
    "\n",
    "index = pyndri.Index('index/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded index can be used to access a collection of documents in an easy manner. We'll give you some examples to get some idea of what it can do, it is up to you to figure out how to use it for the remainder of the assignment.\n",
    "\n",
    "First let's look at the number of documents, since Pyndri indexes the documents using incremental identifiers we can simply take the lowest index and the maximum document and consider the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 164597 documents in this collection.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are %d documents in this collection.\" % (index.maximum_document() - index.document_base()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the first document out of the collection and take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_document = index.document(index.maximum_document()-2)\n",
    "len(set(example_document[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a document consists of two things, a string representing the external document identifier and an integer list representing the identifiers of words that make up the document. Pyndri uses integer representations for words or terms, thus a token_id is an integer that represents a word whereas the token is the actual text of the word/term. Every id has a unique token and vice versa with the exception of stop words: words so common that there are uninformative, all of these receive the zero id.\n",
    "\n",
    "To see what some ids and their matching tokens we take a look at the dictionary of the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'new'), (2, 'percent'), (3, 'two'), (4, '1'), (5, 'people'), (6, 'million'), (7, '000'), (8, 'government'), (9, 'president'), (10, 'years'), (11, 'state'), (12, '2'), (13, 'states'), (14, 'three'), (15, 'time')]\n"
     ]
    }
   ],
   "source": [
    "token2id, id2token, _ = index.get_dictionary()\n",
    "print(list(id2token.items())[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this dictionary we can see the tokens for the (non-stop) words in our example document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['preston', 'robert', 'tisch', 'joins', 'brother', 'cbs', 'board', 'businessman', 'preston', 'robert', 'tisch', 'elected', 'board', 'cbs', 'joins', 'brother', 'cbs', 'chief', 'executive', 'laurence', 'tisch', 'communications', 'giant', 'announced', 'wednesday', 'preston', 'tisch', 'better', 'known', 'bob', 'stepped', 'march', 'serving', '19', 'months', 'postmaster', 'general', 'speculated', 'widely', 'eventually', 'join', 'cbs', 'board', 'tisch', 'brothers', 'considerable', 'economic', 'ties', 'serve', 'co', 'chief', 'executives', 'loews', 'corp', 'tobacco', 'real', 'estate', 'insurance', 'hotel', 'conglomerate', 'cbss', 'biggest', 'shareholder', 'addition', 'preston', 'president', 'laurence', 'chairman', 'loews', 'prestons', 'election', 'cbs', 'board', 'regular', 'board', 'meeting', 'announced', 'william', 's', 'paley', 'cbs', 'founder', 'chairman', 'statement', 'paley', 'quoted', 'saying', 'new', 'board', 'member', 'broad', 'experience', 'business', 'public', 'affairs', 'valuable', 'addition', 'company', 'election', 'boosts', 'membership', 'board', '14', '13', 'cbs', 'board', 'operated', '13', 'members', 'september', '1986', 'thomas', 'p', 'wyman', 'ousted', 'chairman', 'chief', 'executive', 'following', 'dramatic', 'boardroom', 'battle', 'time', 'board', 'made', 'laurence', 'tisch', 'acting', 'chief', 'executive', 'paley', 'acting', 'chairman', 'four', 'months', 'later', 'board', 'deleted', 'word', 'acting', 'job', 'titles', 'tisch', 'cbs', 'trimmed', 'costs', 'sold', 'magazine', 'records', 'divisions', 'television', 'network', 'fall', 'third', 'place', 'season', 'time', 'prime', 'time', 'ratings', 'tisch', 'installed', 'new', 'executives', 'head', 'cbs', 'broadcasting', 'news', 'operations', 'vowed', 'put', 'network', 'back', 'top', 'ratings']\n"
     ]
    }
   ],
   "source": [
    "print([id2token[word_id] for word_id in example_document[1] if word_id > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reverse can also be done, say we want to look for news about the \"University of Massachusetts\", the tokens of that query can be converted to ids using the reverse dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query by tokens: ['university', '', 'massachusetts']\n",
      "Query by ids with stopwords: [200, 0, 894]\n",
      "Query by ids without stopwords: [200, 894]\n"
     ]
    }
   ],
   "source": [
    "query_tokens = index.tokenize(\"University of Massachusetts\")\n",
    "print(\"Query by tokens:\", query_tokens)\n",
    "query_id_tokens = [token2id.get(query_token,0) for query_token in query_tokens]\n",
    "print(\"Query by ids with stopwords:\", query_id_tokens)\n",
    "query_id_tokens = [word_id for word_id in query_id_tokens if word_id > 0]\n",
    "print(\"Query by ids without stopwords:\", query_id_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally we can now match the document and query in the id space, let's see how often a word from the query occurs in our example document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document AP880914-0270 has 0 word matches with query: \"university  massachusetts\".\n",
      "Document AP880914-0270 and query \"university  massachusetts\" have a 0.0% overlap.\n"
     ]
    }
   ],
   "source": [
    "matching_words = sum([True for word_id in example_document[1] if word_id in query_id_tokens])\n",
    "print(\"Document %s has %d word matches with query: \\\"%s\\\".\" % (example_document[0], matching_words, ' '.join(query_tokens)))\n",
    "print(\"Document %s and query \\\"%s\\\" have a %.01f%% overlap.\" % (example_document[0], ' '.join(query_tokens),matching_words/float(len(example_document[1]))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is certainly not everything Pyndri can do, it should give you an idea of how to use it. Please take a look at the [examples](https://github.com/cvangysel/pyndri) as it will help you a lot with this assignment.\n",
    "\n",
    "**CAUTION**: Avoid printing out the whole index in this Notebook as it will generate a lot of output and is likely to corrupt the Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the query file\n",
    "You can parse the query file (`ap_88_89/topics_title`) using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('51', 'Airbus Subsidies'), ('52', 'South African Sanctions'), ('53', 'Leveraged Buyouts'), ('54', 'Satellite Launch Contracts'), ('55', 'Insider Trading'), ('56', 'Prime (Lending) Rate Moves, Predictions'), ('57', 'MCI'), ('58', 'Rail Strikes'), ('59', 'Weather Related Fatalities'), ('60', 'Merit-Pay vs. Seniority'), ('61', 'Israeli Role in Iran-Contra Affair'), ('62', \"Military Coups D'etat\"), ('63', 'Machine Translation'), ('64', 'Hostage-Taking'), ('65', 'Information Retrieval Systems'), ('66', 'Natural Language Processing'), ('67', 'Politically Motivated Civil Disturbances'), ('68', 'Health Hazards from Fine-Diameter Fibers'), ('69', 'Attempts to Revive the SALT II Treaty'), ('70', 'Surrogate Motherhood'), ('71', 'Border Incursions'), ('72', 'Demographic Shifts in the U.S.'), ('73', 'Demographic Shifts across National Boundaries'), ('74', 'Conflicting Policy'), ('75', 'Automation'), ('76', 'U.S. Constitution - Original Intent'), ('77', 'Poaching'), ('78', 'Greenpeace'), ('79', 'FRG Political Party Positions'), ('80', '1988 Presidential Candidates Platforms'), ('81', 'Financial crunch for televangelists in the wake of the PTL scandal'), ('82', 'Genetic Engineering'), ('83', 'Measures to Protect the Atmosphere'), ('84', 'Alternative/renewable Energy Plant & Equipment Installation'), ('85', 'Official Corruption'), ('86', 'Bank Failures'), ('87', 'Criminal Actions Against Officers of Failed Financial Institutions'), ('88', 'Crude Oil Price Trends'), ('89', '\"Downstream\" Investments by OPEC Member States'), ('90', 'Data on Proven Reserves of Oil & Natural Gas Producers'), ('91', 'U.S. Army Acquisition of Advanced Weapons Systems'), ('92', 'International Military Equipment Sales'), ('93', 'What Backing Does the National Rifle Association Have?'), ('94', 'Computer-aided Crime'), ('95', 'Computer-aided Crime Detection'), ('96', 'Computer-Aided Medical Diagnosis'), ('97', 'Fiber Optics Applications'), ('98', 'Fiber Optics Equipment Manufacturers'), ('99', 'Iran-Contra Affair'), ('100', 'Controlling the Transfer of High Technology'), ('101', 'Design of the \"Star Wars\" Anti-missile Defense System'), ('102', \"Laser Research Applicable to the U.S.'s Strategic Defense Initiative\"), ('103', 'Welfare Reform'), ('104', 'Catastrophic Health Insurance'), ('105', '\"Black Monday\"'), ('106', 'U.S. Control of Insider Trading'), ('107', 'Japanese Regulation of Insider Trading'), ('108', 'Japanese Protectionist Measures'), ('109', 'Find Innovative Companies'), ('110', 'Black Resistance Against the South African Government'), ('111', 'Nuclear Proliferation'), ('112', 'Funding Biotechnology'), ('113', 'New Space Satellite Applications'), ('114', 'Non-commercial Satellite Launches'), ('115', 'Impact of the 1986 Immigration Law'), ('116', 'Generic Drug Substitutions'), ('117', 'Capacity of the U.S. Cellular Telephone Network'), ('118', 'International Terrorists'), ('119', 'Actions Against International Terrorists'), ('120', 'Economic Impact of International Terrorism'), ('121', 'Death from Cancer'), ('122', 'RDT&E of New Cancer Fighting Drugs'), ('123', 'Research into & Control of Carcinogens'), ('124', 'Alternatives to Traditional Cancer Therapies'), ('125', 'Anti-smoking Actions by Government'), ('126', 'Medical Ethics and Modern Technology'), ('127', 'U.S.-U.S.S.R. Arms Control Agreements'), ('128', 'Privatization of State Assets'), ('129', 'Soviet Spying on the U.S.'), ('130', 'Jewish Emigration and U.S.-USSR Relations'), ('131', 'McDonnell Douglas Contracts for Military Aircraft'), ('132', '\"Stealth\" Aircraft'), ('133', 'Hubble Space Telescope'), ('134', 'The Human Genome Project'), ('135', 'Possible Contributions of Gene Mapping to Medicine'), ('136', 'Diversification by Pacific Telesis'), ('137', 'Expansion in the U.S. Theme Park Industry'), ('138', 'Iranian Support for Lebanese Hostage-takers'), ('139', \"Iran's Islamic Revolution - Domestic and Foreign Social Consequences\"), ('140', 'Political Impact of Islamic Fundamentalism'), ('141', \"Japan's Handling of its Trade Surplus with the U.S.\"), ('142', 'Impact of Government Regulated Grain Farming on International Relations'), ('143', 'Why Protect U.S. Farmers?'), ('144', 'Management Problems at the United Nations'), ('145', 'Influence of the \"Pro-Israel Lobby\"'), ('146', 'Negotiating an End to the Nicaraguan Civil War'), ('147', 'Productivity Trends in the U.S. Economy'), ('148', 'Conflict in the Horn of Africa'), ('149', 'Industrial Espionage'), ('150', 'U.S. Political Campaign Financing'), ('151', 'Coping with overcrowded prisons'), ('152', 'Accusations of Cheating by Contractors on U.S. Defense Projects'), ('153', 'Insurance Coverage which pays for Long Term Care'), ('154', 'Oil Spills'), ('155', 'Right Wing Christian Fundamentalism in U.S.'), ('156', 'Efforts to enact Gun Control Legislation'), ('157', 'Causes and treatments of multiple sclerosis (MS)'), ('158', 'Term limitations for members of the U.S. Congress'), ('159', 'Electric Car Development'), ('160', 'Vitamins - The Cure for or Cause of Human Ailments'), ('161', 'Acid Rain'), ('162', 'Automobile Recalls'), ('163', 'Vietnam Veterans and Agent Orange'), ('164', 'Generic Drugs - Illegal Activities by Manufacturers'), ('165', 'Tobacco company advertising and the young'), ('166', 'Standardized testing and cultural bias'), ('167', 'Regulation of the showing of violence and explicit sex in motion picture theaters, on television, and on video cassettes.'), ('168', 'Financing AMTRAK'), ('169', 'Cost of Garbage/Trash Removal'), ('170', 'The Consequences of Implantation of Silicone Gel Breast Devices'), ('171', \"Use of Mutual Funds in an Individual's Retirement Strategy\"), ('172', 'The Effectiveness of Medical Products and Related Programs Utilized in the Cessation of Smoking.'), ('173', 'Smoking Bans'), ('174', 'Hazardous Waste Cleanup'), ('175', 'NRA Prevention of Gun Control Legislation'), ('176', 'Real-life private investigators'), ('177', 'English as the Official Language in U.S.'), ('178', 'Dog Maulings'), ('179', 'U. S. Restaurants in Foreign Lands'), ('180', 'Ineffectiveness of U.S. Embargoes/Sanctions'), ('181', 'Abuse of the Elderly by Family Members, and Medical and Nonmedical Personnel, and Initiatives Being Taken to Minimize This Mistreatment'), ('182', 'Commercial Overfishing Creates Food Fish Deficit'), ('183', 'Asbestos Related Lawsuits'), ('184', 'Corporate Pension Plans/Funds'), ('185', 'Reform of the U.S. Welfare System'), ('186', 'Difference of Learning Levels Among Inner City and More Suburban School Students'), ('187', 'Signs of the Demise of Independent Publishing'), ('188', 'Beachfront Erosion'), ('189', 'Real Motives for Murder'), ('190', 'Instances of Fraud Involving the Use of a Computer'), ('191', 'Efforts to Improve U.S. Schooling'), ('192', 'Oil Spill Cleanup'), ('193', 'Toys R Dangerous'), ('194', 'The Amount of Money Earned by Writers'), ('195', 'Stock Market Perturbations Attributable to Computer Initiated Trading'), ('196', 'School Choice Voucher System and its effects upon the entire U.S. educational program'), ('197', 'Reform of the jurisprudence system to stop juries from granting unreasonable monetary awards'), ('198', 'Gene Therapy and Its Benefits to Humankind'), ('199', 'Legality of Medically Assisted Suicides'), ('200', 'Impact of foreign textile imports on U.S. textile industry')])\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import io\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "def parse_topics(file_or_files,\n",
    "                 max_topics=sys.maxsize, delimiter=';'):\n",
    "    assert max_topics >= 0 or max_topics is None\n",
    "\n",
    "    topics = collections.OrderedDict()\n",
    "\n",
    "    if not isinstance(file_or_files, list) and \\\n",
    "            not isinstance(file_or_files, tuple):\n",
    "        if hasattr(file_or_files, '__iter__'):\n",
    "            file_or_files = list(file_or_files)\n",
    "        else:\n",
    "            file_or_files = [file_or_files]\n",
    "\n",
    "    for f in file_or_files:\n",
    "        assert isinstance(f, io.IOBase)\n",
    "\n",
    "        for line in f:\n",
    "            assert(isinstance(line, str))\n",
    "\n",
    "            line = line.strip()\n",
    "\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            topic_id, terms = line.split(delimiter, 1)\n",
    "\n",
    "            if topic_id in topics and (topics[topic_id] != terms):\n",
    "                    logging.error('Duplicate topic \"%s\" (%s vs. %s).',\n",
    "                                  topic_id,\n",
    "                                  topics[topic_id],\n",
    "                                  terms)\n",
    "\n",
    "            topics[topic_id] = terms\n",
    "\n",
    "            if max_topics > 0 and len(topics) >= max_topics:\n",
    "                break\n",
    "\n",
    "    return topics\n",
    "\n",
    "with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "    print(parse_topics([f_topics]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement and compare lexical IR methods [35 points] ### \n",
    "\n",
    "In this task you will implement a number of lexical methods for IR using the **Pyndri** framework. Then you will evaluate these methods on the dataset we have provided using **TREC Eval**.\n",
    "\n",
    "Use the **Pyndri** framework to get statistics of the documents (term frequency, document frequency, collection frequency; **you are not allowed to use the query functionality of Pyndri**) and implement the following scoring methods in **Python**:\n",
    "\n",
    "- [TF-IDF](http://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html) and \n",
    "- [BM25](http://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html) with k1=1.2 and b=0.75. **[5 points]**\n",
    "- Language models ([survey](https://drive.google.com/file/d/0B-zklbckv9CHc0c3b245UW90NE0/view))\n",
    "    - Jelinek-Mercer (explore different values of ùõå in the range [0.1, 0.5, 0.9]). **[5 points]**\n",
    "    - Dirichlet Prior (explore different values of ùõç [500, 1000, 1500]). **[5 points]**\n",
    "    - Absolute discounting (explore different values of ùõÖ in the range [0.1, 0.5, 0.9]). **[5 points]**\n",
    "    - [Positional Language Models](http://sifaka.cs.uiuc.edu/~ylv2/pub/sigir09-plm.pdf) define a language model for each position of a document, and score a document based on the scores of its PLMs. The PLM is estimated based on propagated counts of words within a document through a proximity-based density function, which both captures proximity heuristics and achieves an effect of ‚Äúsoft‚Äù passage retrieval. Implement the PLM, all five kernels, but only the Best position strategy to score documents. Use ùõî equal to 50, and Dirichlet smoothing with ùõç optimized on the validation set (decide how to optimize this value yourself and motivate your decision in the report). **[10 points]**\n",
    "    \n",
    "Implement the above methods and report evaluation measures (on the test set) using the hyper parameter values you optimized on the validation set (also report the values of the hyper parameters). Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "For the language models, create plots showing `NDCG@10` with varying values of the parameters. You can do this by chaining small scripts using shell scripting (preferred) or execute trec_eval using Python's `subprocess`.\n",
    "\n",
    "Compute significance of the results using a [two-tailed paired Student t-test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html) **[5 points]**. Be wary of false rejection of the null hypothesis caused by the [multiple comparisons problem](https://en.wikipedia.org/wiki/Multiple_comparisons_problem). There are multiple ways to mitigate this problem and it is up to you to choose one.\n",
    "\n",
    "Analyse the results by identifying specific queries where different methods succeed or fail and discuss possible reasons that cause these differences. This is *very important* in order to understand who the different retrieval functions behave.\n",
    "\n",
    "**NOTE**: Don‚Äôt forget to use log computations in your calculations to avoid underflows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: You should structure your code around the helper functions we provide below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import time\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering statistics about 456 terms.\n",
      "Inverted index creation took 48.86119294166565 seconds.\n"
     ]
    }
   ],
   "source": [
    "with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "    queries = parse_topics([f_topics])\n",
    "\n",
    "index = pyndri.Index('index/')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "num_documents = index.maximum_document() - index.document_base()\n",
    "\n",
    "dictionary = pyndri.extract_dictionary(index)\n",
    "\n",
    "tokenized_queries = {\n",
    "    query_id: [dictionary.translate_token(token)\n",
    "               for token in index.tokenize(query_string)\n",
    "               if dictionary.has_token(token)]\n",
    "    for query_id, query_string in queries.items()}\n",
    "\n",
    "query_term_ids = set(\n",
    "    query_term_id\n",
    "    for query_term_ids in tokenized_queries.values()\n",
    "    for query_term_id in query_term_ids)\n",
    "\n",
    "print('Gathering statistics about', len(query_term_ids), 'terms.')\n",
    "\n",
    "# inverted index creation.\n",
    "\n",
    "document_lengths = {}\n",
    "unique_terms_per_document = {}\n",
    "\n",
    "inverted_index = collections.defaultdict(dict)\n",
    "collection_frequencies = collections.defaultdict(int)\n",
    "\n",
    "total_terms = 0\n",
    "\n",
    "#______________________we added a dict to keep the ext_doc_______________________\n",
    "ext_dic = {}\n",
    "\n",
    "for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "    ext_doc_id, doc_token_ids = index.document(int_doc_id)\n",
    "    \n",
    "    #--------------------------------------\n",
    "    ext_dic[int_doc_id] = ext_doc_id\n",
    "    #--------------------------------------\n",
    "    document_bow = collections.Counter(\n",
    "        token_id for token_id in doc_token_ids\n",
    "        if token_id > 0)\n",
    "    document_length = sum(document_bow.values())\n",
    "\n",
    "    document_lengths[int_doc_id] = document_length\n",
    "    total_terms += document_length\n",
    "\n",
    "    unique_terms_per_document[int_doc_id] = len(document_bow)\n",
    "\n",
    "    for query_term_id in query_term_ids:\n",
    "        assert query_term_id is not None\n",
    "\n",
    "        document_term_frequency = document_bow.get(query_term_id, 0)\n",
    "\n",
    "        if document_term_frequency == 0:\n",
    "            continue\n",
    "\n",
    "        collection_frequencies[query_term_id] += document_term_frequency\n",
    "        inverted_index[query_term_id][int_doc_id] = document_term_frequency\n",
    "\n",
    "avg_doc_length = total_terms / num_documents\n",
    "\n",
    "print('Inverted index creation took', time.time() - start_time, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf(int_document_id, query_term_id):\n",
    "    #Returns term frequency (tf_t) for a document\n",
    "    #https://docs.quantifiedcode.com/python-anti-patterns/correctness/not_using_get_to_return_a_default_value_from_a_dictionary.html\n",
    "    \n",
    "    return float(inverted_index.get(query_term_id, 0).get(int_document_id, 0))\n",
    "\n",
    "def collection_freq(query_term_id):\n",
    "    #Returns collection frequency\n",
    "    return collection_frequencies.get(query_term_id, 0)\n",
    "\n",
    "def calc_p_wc(query_term_id):\n",
    "    #Returns background probability p(w|C)\n",
    "    tf_wC = collection_freq(query_term_id)\n",
    "    C = total_terms\n",
    "    return (tf_wC/C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import Counter\n",
    "def run_retrieval(model_name, score_fn):\n",
    "    \"\"\"\n",
    "    Runs a retrieval method for all the queries and writes the TREC-friendly results in a file.\n",
    "    \n",
    "    :param model_name: the name of the model (a string)\n",
    "    :param score_fn: the scoring function (a function - see below for an example) \n",
    "    \"\"\"\n",
    "    run_out_path = 'models/{}.run'.format(model_name)\n",
    "\n",
    "    if os.path.exists(run_out_path):\n",
    "        return\n",
    "\n",
    "    retrieval_start_time = time.time()\n",
    "\n",
    "    print('Retrieving using', model_name)\n",
    "\n",
    "    data = collections.defaultdict(list)\n",
    "\n",
    "    # TODO: fill the data dictionary. \n",
    "    # The dictionary data should have the form: query_id --> (document_score, external_doc_id)\n",
    "    \n",
    "    for query_id, _ in queries.items():\n",
    "        \n",
    "        score_per_doc = Counter()\n",
    "        '''\n",
    "        We do not go through all documents, we just go through the all documents relate to query term.\n",
    "        For example if we have 5 docs and Q1 = w1 w2 and w1 is inside the doc1 & doc2, w2 is inside doc3\n",
    "        , then we go through doc1, doc2, doc3\n",
    "        '''        \n",
    "        #------------------------docs_to_check = sum of all documents for q:-------------------------\n",
    "        \n",
    "        docs_to_check = set([inverted_index[term_id].keys() for term_id in tokenized_queries[query_id]][0])\n",
    "        \n",
    "        '''\n",
    "        We check the score for every term of a query for every doc related to that query and assign some score\n",
    "        to them.\n",
    "        In tf-idf when the term is not inside the that doc, we return 0.\n",
    "        In the smoothing methods, we assign some probability to the unseen terms.\n",
    "        '''\n",
    "        #--------------------------find the score(query, doc)-----------------------------------------\n",
    "        for query_term_id in tokenized_queries[query_id]:\n",
    "            for int_doc_id in docs_to_check:\n",
    "                document_term_freq = get_tf(int_doc_id, query_term_id)\n",
    "                score_per_doc[int_doc_id] += score_fn(int_doc_id, query_term_id, document_term_freq)\n",
    "                \n",
    "        #------------------------make data set to write in run file------------------------------------        \n",
    "        for int_doc_id in score_per_doc:\n",
    "            data[query_id].append((score_per_doc[int_doc_id], ext_dic[int_doc_id]))\n",
    "\n",
    "    print('Retrieval took: ', time.time() - retrieval_start_time, 'seconds.')\n",
    "    with open(run_out_path, 'w') as f_out:\n",
    "        write_run(\n",
    "            model_name=model_name,\n",
    "            data=data,\n",
    "            out_f=f_out,\n",
    "            max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF - IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tfidf(int_document_id, query_term_id, document_term_freq):\n",
    "    \"\"\"\n",
    "    Scoring function for a document and a query term\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query_token_id: the query term id (assuming you have split the query to tokens)\n",
    "    :param document_term_freq: the document term frequency of the query term \n",
    "    \"\"\"\n",
    "    tf = np.log10(1 + document_term_freq)    \n",
    "    idf = np.log10(num_documents/len(inverted_index[query_term_id]))\n",
    "\n",
    "    return (tf * idf)\n",
    "\n",
    "# combining the two functions above: \n",
    "run_retrieval('tfidf', tfidf)\n",
    "\n",
    "# TODO implement the rest of the retrieval functions \n",
    "\n",
    "# TODO implement tools to help you with the analysis of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25(int_document_id, query_term_id, document_term_freq):\n",
    "    \n",
    "    K1 = 1.2\n",
    "    B = 0.75\n",
    "    \n",
    "    tf = document_term_freq\n",
    "    normalize_l =  document_lengths[int_document_id]/avg_doc_length\n",
    "    idf = np.log10(num_documents/len(inverted_index[query_term_id]))\n",
    "    w = ((K1+1)*tf)/ (K1*( (1-B) + B*(normalize_l) ) + tf)   \n",
    "    score = w*idf\n",
    "    \n",
    "    return score\n",
    "\n",
    "run_retrieval('bm25', bm25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jelinek-Mercer at ùõå = [0.1, 0.5, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jelinek_mercer(int_document_id, query_term_id, document_term_freq):\n",
    "    \n",
    "    tf_wd = document_term_freq\n",
    "    d = document_lengths[int_document_id]\n",
    "    p_wC = calc_p_wc(query_term_id)\n",
    "    \n",
    "    return tf_wd, d, p_wC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jelinek_mercer_01(int_document_id, query_term_id, document_term_freq, lmbda = 0.1):\n",
    "    \n",
    "    tf_wd, d, p_wC = jelinek_mercer(int_document_id, query_term_id, document_term_freq)\n",
    "    #p(w|d) = seen smoothed pbt + unseen smoothed pbt\n",
    "    pbt_lambda = (1 - lmbda)*(p_wC) + lmbda*(tf_wd/d) \n",
    "    \n",
    "    return np.log10(pbt_lambda)\n",
    "\n",
    "def jelinek_mercer_05(int_document_id, query_term_id, document_term_freq, lmbda = 0.5):\n",
    "    \n",
    "    tf_wd, d, p_wC = jelinek_mercer(int_document_id, query_term_id, document_term_freq)\n",
    "    #p(w|d) = seen smoothed pbt + unseen smoothed pbt\n",
    "    pbt_lambda = (1 - lmbda)*(p_wC) + lmbda*(tf_wd/d) \n",
    "    \n",
    "    return np.log10(pbt_lambda)\n",
    "\n",
    "def jelinek_mercer_09(int_document_id, query_term_id, document_term_freq, lmbda = 0.9):\n",
    "\n",
    "    tf_wd, d, p_wC = jelinek_mercer(int_document_id, query_term_id, document_term_freq)\n",
    "    #p(w|d) = seen smoothed pbt + unseen smoothed pbt\n",
    "    pbt_lambda = (1 - lmbda)*(p_wC) + lmbda*(tf_wd/d) \n",
    "    \n",
    "    return np.log10(pbt_lambda)\n",
    "\n",
    "\n",
    "run_retrieval('jelinek_mercer 0.1', jelinek_mercer_01)\n",
    "run_retrieval('jelinek_mercer 0.5', jelinek_mercer_05)\n",
    "run_retrieval('jelinek_mercer 0.9', jelinek_mercer_09)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Absolute Discounting at ùõÖ = [0.1, 0.5, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#formula from : https://dl.acm.org/citation.cfm?id=384019\n",
    "def absolute_discounting(int_document_id, query_term_id, document_term_freq, delta):\n",
    "    \n",
    "    tf_wd = document_term_freq\n",
    "    d = document_lengths[int_document_id]\n",
    "    #d_u = number of unique words in d\n",
    "    d_u = unique_terms_per_document[int_document_id]\n",
    "    p_wC = calc_p_wc(query_term_id)\n",
    "    \n",
    "    #seen words probability\n",
    "    seen_words = max((tf_wd - delta), 0)/d\n",
    "    \n",
    "    #unseen words probability\n",
    "    unseen_words = delta*(d_u/d)*p_wC\n",
    "\n",
    "    return seen_words, unseen_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def absolute_discounting_01(int_document_id, query_term_id, document_term_freq):\n",
    "    \n",
    "    seen_words, unseen_words = absolute_discounting(int_document_id, query_term_id, document_term_freq, delta = 0.1)\n",
    "    return (seen_words + unseen_words)\n",
    "\n",
    "def absolute_discounting_05(int_document_id, query_term_id, document_term_freq):\n",
    "    \n",
    "    seen_words, unseen_words = absolute_discounting(int_document_id, query_term_id, document_term_freq, delta = 0.5)\n",
    "    return (seen_words + unseen_words)\n",
    "\n",
    "def absolute_discounting_09(int_document_id, query_term_id, document_term_freq):\n",
    "    \n",
    "    seen_words, unseen_words = absolute_discounting(int_document_id, query_term_id, document_term_freq, delta = 0.9)\n",
    "    return (seen_words + unseen_words)\n",
    "\n",
    "\n",
    "run_retrieval('absolute_discounting 0.1', absolute_discounting_01)\n",
    "run_retrieval('absolute_discounting 0.5', absolute_discounting_05)\n",
    "run_retrieval('absolute_discounting 0.9', absolute_discounting_09)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Latent Semantic Models (LSMs) [15 points] ###\n",
    "\n",
    "In this task you will experiment with applying distributional semantics methods ([LSI](http://lsa3.colorado.edu/papers/JASIS.lsi.90.pdf) **[5 points]** and [LDA](https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf) **[5 points]**) for retrieval.\n",
    "\n",
    "You do not need to implement LSI or LDA on your own. Instead, you can use [gensim](http://radimrehurek.com/gensim/index.html). An example on how to integrate Pyndri with Gensim for word2vec can be found [here](https://github.com/cvangysel/pyndri/blob/master/examples/word2vec.py). For the remaining latent vector space models, you will need to implement connector classes (such as `IndriSentences`) by yourself.\n",
    "\n",
    "In order to use a latent semantic model for retrieval, you need to:\n",
    "   * build a representation of the query **q**,\n",
    "   * build a representation of the document **d**,\n",
    "   * calculate the similarity between **q** and **d** (e.g., cosine similarity, KL-divergence).\n",
    "     \n",
    "The exact implementation here depends on the latent semantic model you are using. \n",
    "   \n",
    "Each of these LSMs come with various hyperparameters to tune. Make a choice on the parameters, and explicitly mention the reasons that led you to these decisions. You can use the validation set to optimize hyper parameters you see fit; motivate your decisions. In addition, mention clearly how the query/document representations were constructed for each LSM and explain your choices.\n",
    "\n",
    "In this experiment, you will first obtain an initial top-1000 ranking for each query using TF-IDF in **Task 1**, and then re-rank the documents using the LSMs. Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "Perform significance testing **[5 points]** (similar as in Task 1) in the class of semantic matching methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:test\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from pprint import pprint\n",
    "import copy\n",
    "import gensim\n",
    "import logging\n",
    "import pyndri\n",
    "import pyndri.compat\n",
    "import sys\n",
    "import Cython\n",
    "import multiprocessing\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.info(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing took: \n",
      "166.08451437950134 seconds.\n"
     ]
    }
   ],
   "source": [
    "#first obtain an initial top-1000 ranking for each query using TF-IDF in Task 1\n",
    "docx = {}\n",
    "keys = inverted_index.keys()\n",
    "start_time = time.time()\n",
    "\n",
    "print('Computing took: ')\n",
    "\n",
    "for k in keys:\n",
    "    docx[k] = inverted_index[k].keys()\n",
    "\n",
    "scores = {}\n",
    "top1000_tfidf = {}\n",
    "\n",
    "#create datasets to store scores and document ids separately\n",
    "for query_id, term_idx in tokenized_queries.items():\n",
    "    scores[query_id] = []\n",
    "    docx_set = set()\n",
    "\n",
    "    for term_id in term_idx:\n",
    "        docx_set = docx_set | set(docx[term_id])\n",
    "\n",
    "#compute tfids scores\n",
    "    for d_id in docx_set:\n",
    "        result = 0\n",
    "        for term_id in term_idx:\n",
    "            result += tfidf(d_id, term_id, len(inverted_index.get(term_id, 0)))\n",
    "        scores[query_id].append((result, index.document(d_id)[0], d_id))  \n",
    "\n",
    "print(time.time() - start_time, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store top 1000 tfidf rank queries\n",
    "for query_id, term_idx in tokenized_queries.items():\n",
    "    top1000_tfidf[query_id] = sorted(scores[query_id], key = itemgetter(0), reverse = True)\n",
    "    del top1000_tfidf[query_id][1000:]\n",
    "\n",
    "#pprint(top1000_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO:gensim.corpora.dictionary:adding document #10000 to Dictionary(73866 unique tokens: ['aiding', 'caused', 'ame', 'batang', 'zamudio']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #20000 to Dictionary(101162 unique tokens: ['screeening', 'caused', 'polik', 'zamudio', 'casino']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #30000 to Dictionary(121185 unique tokens: ['screeening', 'caused', 'polik', 'zamudio', 'neighborhing']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #40000 to Dictionary(138774 unique tokens: ['screeening', 'caused', 'dumpsite', 'polik', 'zamudio']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #50000 to Dictionary(153153 unique tokens: ['screeening', 'caused', 'dumpsite', 'polik', 'zamudio']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #60000 to Dictionary(165943 unique tokens: ['screeening', 'caused', 'dumpsite', 'polik', 'zamudio']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #70000 to Dictionary(177185 unique tokens: ['screeening', 'caused', 'novetzke', 'zamudio', 'casino']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #80000 to Dictionary(188142 unique tokens: ['screeening', 'caused', '7015', 'novetzke', 'zamudio']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #90000 to Dictionary(199575 unique tokens: ['screeening', 'caused', '7015', 'novetzke', 'zamudio']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #100000 to Dictionary(211216 unique tokens: ['screeening', 'caused', '7015', 'novetzke', 'zamudio']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #110000 to Dictionary(221773 unique tokens: ['screeening', 'caused', '7015', 'novetzke', 'zamudio']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #120000 to Dictionary(231132 unique tokens: ['screeening', 'caused', '7015', 'novetzke', 'zamudio']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #130000 to Dictionary(240313 unique tokens: ['screeening', 'caused', '7015', 'novetzke', 'zamudio']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #140000 to Dictionary(248661 unique tokens: ['screeening', 'caused', '7015', 'novetzke', 'zamudio']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #150000 to Dictionary(256455 unique tokens: ['screeening', 'caused', '7015', 'novetzke', 'zamudio']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #160000 to Dictionary(263913 unique tokens: ['screeening', 'caused', '7015', 'novetzke', 'zamudio']...)\n",
      "INFO:gensim.corpora.dictionary:built Dictionary(267318 unique tokens: ['screeening', 'caused', '7015', 'novetzke', 'zamudio']...) from 164597 documents (total 42208958 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "#Use pyndri and gensim to create LSM Models\n",
    "document = index.document(index.document_base())\n",
    "\n",
    "#dictionary\n",
    "dictionary = pyndri.extract_dictionary(index)\n",
    "\n",
    "#indri to get sentences\n",
    "sentences = pyndri.compat.IndriSentences(index, dictionary)\n",
    "\n",
    "#building corpus from sentences\n",
    "dictionary_corpus = gensim.corpora.Dictionary(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('screeening', 99389),\n",
       " ('caused', 896),\n",
       " ('7015', 187357),\n",
       " ('novetzke', 95617),\n",
       " ('zamudio', 36900)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dictionary_corpus.token2id.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.corpora.indexedcorpus:loaded corpus index from models/tharangni.mm.index\n",
      "INFO:gensim.matutils:initializing corpus reader from models/tharangni.mm\n",
      "INFO:gensim.matutils:accepted corpus with 164597 documents, 267318 features, 29573706 non-zero entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using already saved BOW\n"
     ]
    }
   ],
   "source": [
    "## convert tokenized documents to vectors [DOCUMENT BOW]\n",
    "if os.path.exists('models/tharangni.mm'):\n",
    "    corpus = gensim.corpora.MmCorpus('models/tharangni.mm')\n",
    "    print('Using already saved BOW')\n",
    "else:\n",
    "    corpus = [dictionary_corpus.doc2bow(stuff) for stuff in sentences]\n",
    "    gensim.corpora.MmCorpus.serialize('models/tharangni.mm', corpus) ##<-- saves bow to disk\n",
    "    print('BOW saved to disk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(164597 documents, 267318 features, 29573706 non-zero entries)\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#representation for q (query) [QUERY BOW]\n",
    "query_bow = []\n",
    "\n",
    "#query id and term id in tokenized queries\n",
    "for q_id, t_id in tokenized_queries.items():\n",
    "    word_at_term = [id2token[t] for t in t_id]\n",
    "    query_bow.append(dictionary_corpus.doc2bow(word_at_term))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.lsimodel:using serial LSI version on this node\n",
      "INFO:gensim.models.lsimodel:updating model with new documents\n",
      "INFO:gensim.models.lsimodel:preparing a new chunk of documents\n",
      "INFO:gensim.models.lsimodel:using 100 extra samples and 2 power iterations\n",
      "INFO:gensim.models.lsimodel:1st phase: constructing (267318, 200) action matrix\n",
      "INFO:gensim.models.lsimodel:orthonormalizing (267318, 200) action matrix\n",
      "INFO:gensim.models.lsimodel:2nd phase: running dense svd on (200, 20000) matrix\n",
      "INFO:gensim.models.lsimodel:computing the final decomposition\n",
      "INFO:gensim.models.lsimodel:keeping 100 factors (discarding 16.353% of energy spectrum)\n",
      "INFO:gensim.models.lsimodel:processed documents up to #20000\n",
      "INFO:gensim.models.lsimodel:topic #0(876.656): 0.235*\"percent\" + 0.199*\"1\" + 0.199*\"new\" + 0.193*\"million\" + 0.163*\"000\" + 0.145*\"government\" + 0.144*\"two\" + 0.141*\"people\" + 0.129*\"president\" + 0.127*\"2\"\n",
      "INFO:gensim.models.lsimodel:topic #1(553.568): -0.346*\"fox\" + -0.342*\"lazy\" + -0.342*\"brown\" + -0.339*\"dogs\" + -0.339*\"quick\" + -0.338*\"jumped\" + -0.266*\"back\" + -0.261*\"sending\" + -0.261*\"wx\" + -0.261*\"0123456789\"\n",
      "INFO:gensim.models.lsimodel:topic #2(528.190): 0.443*\"percent\" + 0.301*\"1\" + 0.211*\"million\" + 0.179*\"2\" + 0.141*\"5\" + -0.137*\"president\" + -0.133*\"bush\" + 0.132*\"3\" + -0.125*\"government\" + -0.124*\"people\"\n",
      "INFO:gensim.models.lsimodel:topic #3(482.613): 0.924*\"y\" + 0.327*\"n\" + 0.107*\"republicans\" + 0.106*\"democrats\" + 0.046*\"x\" + 0.043*\"d\" + 0.036*\"republican\" + 0.036*\"democrat\" + 0.035*\"r\" + 0.027*\"smith\"\n",
      "INFO:gensim.models.lsimodel:topic #4(381.044): 0.641*\"percent\" + -0.234*\"1\" + -0.232*\"cents\" + -0.165*\"cent\" + -0.160*\"million\" + -0.139*\"futures\" + -0.138*\"lower\" + -0.130*\"higher\" + -0.124*\"new\" + 0.115*\"bush\"\n",
      "INFO:gensim.models.lsimodel:preparing a new chunk of documents\n",
      "INFO:gensim.models.lsimodel:using 100 extra samples and 2 power iterations\n",
      "INFO:gensim.models.lsimodel:1st phase: constructing (267318, 200) action matrix\n",
      "INFO:gensim.models.lsimodel:orthonormalizing (267318, 200) action matrix\n",
      "INFO:gensim.models.lsimodel:2nd phase: running dense svd on (200, 20000) matrix\n",
      "INFO:gensim.models.lsimodel:computing the final decomposition\n",
      "INFO:gensim.models.lsimodel:keeping 100 factors (discarding 17.094% of energy spectrum)\n",
      "INFO:gensim.models.lsimodel:merging projections: (267318, 100) + (267318, 100)\n",
      "INFO:gensim.models.lsimodel:keeping 100 factors (discarding 2.622% of energy spectrum)\n",
      "INFO:gensim.models.lsimodel:processed documents up to #40000\n",
      "INFO:gensim.models.lsimodel:topic #0(1231.251): 0.222*\"percent\" + 0.199*\"new\" + 0.196*\"1\" + 0.186*\"million\" + 0.150*\"000\" + 0.148*\"government\" + 0.145*\"people\" + 0.145*\"two\" + 0.132*\"president\" + 0.125*\"2\"\n",
      "INFO:gensim.models.lsimodel:topic #1(740.157): -0.420*\"percent\" + -0.319*\"1\" + -0.217*\"million\" + -0.187*\"2\" + -0.144*\"5\" + -0.136*\"3\" + 0.130*\"president\" + 0.129*\"bush\" + -0.127*\"billion\" + 0.123*\"government\"\n",
      "INFO:gensim.models.lsimodel:topic #2(710.931): -0.866*\"y\" + -0.455*\"n\" + -0.114*\"republicans\" + -0.113*\"democrats\" + -0.047*\"x\" + -0.038*\"democrat\" + -0.038*\"republican\" + -0.030*\"d\" + -0.029*\"smith\" + -0.024*\"r\"\n",
      "INFO:gensim.models.lsimodel:topic #3(555.419): 0.349*\"fox\" + 0.337*\"brown\" + 0.335*\"lazy\" + 0.333*\"dogs\" + 0.331*\"jumped\" + 0.331*\"quick\" + 0.265*\"back\" + 0.256*\"sending\" + 0.255*\"wx\" + 0.255*\"0123456789\"\n",
      "INFO:gensim.models.lsimodel:topic #4(527.693): -0.592*\"percent\" + 0.242*\"1\" + 0.231*\"cents\" + 0.161*\"cent\" + -0.159*\"bush\" + 0.138*\"lower\" + 0.135*\"futures\" + 0.130*\"new\" + 0.121*\"higher\" + 0.102*\"west\"\n",
      "INFO:gensim.models.lsimodel:preparing a new chunk of documents\n",
      "INFO:gensim.models.lsimodel:using 100 extra samples and 2 power iterations\n",
      "INFO:gensim.models.lsimodel:1st phase: constructing (267318, 200) action matrix\n",
      "INFO:gensim.models.lsimodel:orthonormalizing (267318, 200) action matrix\n",
      "INFO:gensim.models.lsimodel:2nd phase: running dense svd on (200, 20000) matrix\n",
      "INFO:gensim.models.lsimodel:computing the final decomposition\n",
      "INFO:gensim.models.lsimodel:keeping 100 factors (discarding 17.129% of energy spectrum)\n",
      "INFO:gensim.models.lsimodel:merging projections: (267318, 100) + (267318, 100)\n",
      "INFO:gensim.models.lsimodel:keeping 100 factors (discarding 2.067% of energy spectrum)\n",
      "INFO:gensim.models.lsimodel:processed documents up to #60000\n",
      "INFO:gensim.models.lsimodel:topic #0(1509.475): 0.224*\"percent\" + 0.203*\"new\" + 0.201*\"1\" + 0.187*\"million\" + 0.147*\"government\" + 0.145*\"000\" + 0.144*\"people\" + 0.144*\"two\" + 0.129*\"president\" + 0.128*\"2\"\n",
      "INFO:gensim.models.lsimodel:topic #1(912.699): -0.415*\"percent\" + -0.326*\"1\" + -0.213*\"million\" + -0.191*\"2\" + -0.150*\"5\" + -0.143*\"3\" + -0.132*\"billion\" + 0.129*\"bush\" + 0.129*\"president\" + -0.128*\"4\"\n",
      "INFO:gensim.models.lsimodel:topic #2(856.121): -0.829*\"y\" + -0.519*\"n\" + -0.117*\"republicans\" + -0.117*\"democrats\" + -0.050*\"x\" + -0.040*\"republican\" + -0.039*\"democrat\" + -0.031*\"smith\" + -0.027*\"d\" + -0.021*\"r\"\n",
      "INFO:gensim.models.lsimodel:topic #3(645.116): -0.589*\"percent\" + 0.237*\"1\" + 0.217*\"cents\" + -0.189*\"bush\" + 0.149*\"cent\" + 0.132*\"new\" + 0.130*\"lower\" + 0.124*\"futures\" + -0.116*\"president\" + -0.110*\"billion\"\n",
      "INFO:gensim.models.lsimodel:topic #4(617.250): -0.376*\"bush\" + 0.221*\"police\" + -0.219*\"president\" + 0.190*\"percent\" + -0.186*\"cents\" + 0.184*\"people\" + -0.169*\"soviet\" + 0.155*\"000\" + -0.134*\"house\" + -0.127*\"cent\"\n",
      "INFO:gensim.models.lsimodel:preparing a new chunk of documents\n",
      "INFO:gensim.models.lsimodel:using 100 extra samples and 2 power iterations\n",
      "INFO:gensim.models.lsimodel:1st phase: constructing (267318, 200) action matrix\n",
      "INFO:gensim.models.lsimodel:orthonormalizing (267318, 200) action matrix\n",
      "INFO:gensim.models.lsimodel:2nd phase: running dense svd on (200, 20000) matrix\n",
      "INFO:gensim.models.lsimodel:computing the final decomposition\n",
      "INFO:gensim.models.lsimodel:keeping 100 factors (discarding 17.210% of energy spectrum)\n",
      "INFO:gensim.models.lsimodel:merging projections: (267318, 100) + (267318, 100)\n",
      "INFO:gensim.models.lsimodel:keeping 100 factors (discarding 1.421% of energy spectrum)\n",
      "INFO:gensim.models.lsimodel:processed documents up to #80000\n",
      "INFO:gensim.models.lsimodel:topic #0(1740.586): 0.220*\"percent\" + 0.205*\"new\" + 0.199*\"1\" + 0.186*\"million\" + 0.148*\"government\" + 0.145*\"two\" + 0.145*\"people\" + 0.145*\"000\" + 0.131*\"president\" + 0.125*\"2\"\n",
      "INFO:gensim.models.lsimodel:topic #1(1046.474): -0.416*\"percent\" + -0.329*\"1\" + -0.219*\"million\" + -0.190*\"2\" + -0.147*\"5\" + -0.140*\"3\" + -0.133*\"billion\" + 0.129*\"bush\" + 0.127*\"president\" + -0.126*\"4\"\n",
      "INFO:gensim.models.lsimodel:topic #2(969.654): -0.843*\"y\" + -0.495*\"n\" + -0.117*\"republicans\" + -0.116*\"democrats\" + -0.058*\"x\" + -0.040*\"republican\" + -0.040*\"democrat\" + -0.030*\"smith\" + -0.030*\"d\" + -0.024*\"r\"\n",
      "INFO:gensim.models.lsimodel:topic #3(741.257): -0.540*\"percent\" + -0.231*\"bush\" + 0.227*\"1\" + 0.210*\"cents\" + 0.144*\"cent\" + -0.135*\"president\" + 0.131*\"new\" + 0.126*\"lower\" + -0.126*\"billion\" + -0.122*\"house\"\n",
      "INFO:gensim.models.lsimodel:topic #4(712.845): -0.382*\"bush\" + 0.282*\"percent\" + 0.225*\"police\" + -0.216*\"president\" + 0.186*\"people\" + -0.183*\"cents\" + -0.146*\"house\" + 0.143*\"000\" + -0.141*\"soviet\" + -0.126*\"new\"\n",
      "INFO:gensim.models.lsimodel:preparing a new chunk of documents\n",
      "INFO:gensim.models.lsimodel:using 100 extra samples and 2 power iterations\n",
      "INFO:gensim.models.lsimodel:1st phase: constructing (267318, 200) action matrix\n",
      "INFO:gensim.models.lsimodel:orthonormalizing (267318, 200) action matrix\n",
      "INFO:gensim.models.lsimodel:2nd phase: running dense svd on (200, 20000) matrix\n",
      "INFO:gensim.models.lsimodel:computing the final decomposition\n",
      "INFO:gensim.models.lsimodel:keeping 100 factors (discarding 15.196% of energy spectrum)\n",
      "INFO:gensim.models.lsimodel:merging projections: (267318, 100) + (267318, 100)\n",
      "INFO:gensim.models.lsimodel:keeping 100 factors (discarding 2.644% of energy spectrum)\n",
      "INFO:gensim.models.lsimodel:processed documents up to #100000\n",
      "INFO:gensim.models.lsimodel:topic #0(1941.601): 0.243*\"percent\" + 0.202*\"new\" + 0.201*\"1\" + 0.185*\"million\" + 0.146*\"000\" + 0.145*\"two\" + 0.143*\"government\" + 0.142*\"people\" + 0.130*\"president\" + 0.127*\"2\"\n",
      "INFO:gensim.models.lsimodel:topic #1(1191.696): -0.456*\"percent\" + 0.325*\"y\" + -0.283*\"1\" + 0.181*\"n\" + -0.172*\"million\" + -0.166*\"2\" + -0.129*\"5\" + -0.123*\"3\" + 0.115*\"president\" + -0.113*\"4\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.lsimodel:topic #2(1184.222): -0.792*\"y\" + -0.439*\"n\" + -0.185*\"percent\" + -0.113*\"1\" + -0.105*\"republicans\" + -0.101*\"democrats\" + -0.068*\"million\" + -0.066*\"2\" + -0.054*\"x\" + 0.052*\"people\"\n",
      "INFO:gensim.models.lsimodel:topic #3(869.029): -0.660*\"percent\" + 0.278*\"1\" + 0.210*\"cents\" + 0.203*\"million\" + -0.140*\"bush\" + 0.138*\"new\" + 0.136*\"cent\" + 0.125*\"lower\" + 0.117*\"futures\" + 0.113*\"higher\"\n",
      "INFO:gensim.models.lsimodel:topic #4(797.052): -0.427*\"bush\" + 0.252*\"police\" + -0.242*\"president\" + -0.195*\"house\" + 0.191*\"people\" + 0.166*\"000\" + -0.156*\"d\" + -0.151*\"billion\" + 0.147*\"percent\" + -0.120*\"senate\"\n",
      "INFO:gensim.models.lsimodel:preparing a new chunk of documents\n",
      "INFO:gensim.models.lsimodel:using 100 extra samples and 2 power iterations\n",
      "INFO:gensim.models.lsimodel:1st phase: constructing (267318, 200) action matrix\n",
      "INFO:gensim.models.lsimodel:orthonormalizing (267318, 200) action matrix\n",
      "INFO:gensim.models.lsimodel:2nd phase: running dense svd on (200, 20000) matrix\n",
      "INFO:gensim.models.lsimodel:computing the final decomposition\n",
      "INFO:gensim.models.lsimodel:keeping 100 factors (discarding 16.420% of energy spectrum)\n",
      "INFO:gensim.models.lsimodel:merging projections: (267318, 100) + (267318, 100)\n",
      "INFO:gensim.models.lsimodel:keeping 100 factors (discarding 2.056% of energy spectrum)\n",
      "INFO:gensim.models.lsimodel:processed documents up to #120000\n",
      "INFO:gensim.models.lsimodel:topic #0(2120.305): 0.260*\"percent\" + 0.201*\"1\" + 0.201*\"new\" + 0.184*\"million\" + 0.145*\"two\" + 0.143*\"000\" + 0.140*\"people\" + 0.140*\"government\" + 0.130*\"president\" + 0.127*\"2\"\n",
      "INFO:gensim.models.lsimodel:topic #1(1324.134): -0.554*\"percent\" + -0.277*\"1\" + -0.166*\"2\" + -0.159*\"million\" + -0.128*\"0\" + -0.127*\"5\" + -0.125*\"3\" + 0.124*\"president\" + 0.121*\"people\" + -0.115*\"4\"\n",
      "INFO:gensim.models.lsimodel:topic #2(1240.902): -0.852*\"y\" + -0.481*\"n\" + -0.116*\"republicans\" + -0.115*\"democrats\" + -0.057*\"x\" + -0.041*\"republican\" + -0.040*\"democrat\" + -0.029*\"smith\" + -0.020*\"d\" + -0.019*\"percent\"\n",
      "INFO:gensim.models.lsimodel:topic #3(989.358): -0.640*\"percent\" + 0.305*\"1\" + 0.251*\"million\" + 0.193*\"cents\" + -0.150*\"bush\" + 0.125*\"new\" + 0.120*\"cent\" + 0.115*\"lower\" + 0.108*\"higher\" + 0.107*\"futures\"\n",
      "INFO:gensim.models.lsimodel:topic #4(879.002): -0.460*\"bush\" + 0.253*\"police\" + -0.248*\"president\" + 0.176*\"people\" + -0.175*\"house\" + 0.160*\"000\" + -0.135*\"billion\" + -0.135*\"d\" + -0.121*\"dukakis\" + 0.114*\"two\"\n",
      "INFO:gensim.models.lsimodel:preparing a new chunk of documents\n",
      "INFO:gensim.models.lsimodel:using 100 extra samples and 2 power iterations\n",
      "INFO:gensim.models.lsimodel:1st phase: constructing (267318, 200) action matrix\n",
      "INFO:gensim.models.lsimodel:orthonormalizing (267318, 200) action matrix\n",
      "INFO:gensim.models.lsimodel:2nd phase: running dense svd on (200, 20000) matrix\n",
      "INFO:gensim.models.lsimodel:computing the final decomposition\n",
      "INFO:gensim.models.lsimodel:keeping 100 factors (discarding 16.281% of energy spectrum)\n",
      "INFO:gensim.models.lsimodel:merging projections: (267318, 100) + (267318, 100)\n",
      "INFO:gensim.models.lsimodel:keeping 100 factors (discarding 2.104% of energy spectrum)\n",
      "INFO:gensim.models.lsimodel:processed documents up to #140000\n",
      "INFO:gensim.models.lsimodel:topic #0(2289.802): 0.270*\"percent\" + 0.201*\"new\" + 0.200*\"1\" + 0.180*\"million\" + 0.145*\"two\" + 0.143*\"000\" + 0.140*\"people\" + 0.138*\"government\" + 0.131*\"president\" + 0.126*\"2\"\n",
      "INFO:gensim.models.lsimodel:topic #1(1443.845): -0.593*\"percent\" + -0.261*\"1\" + -0.160*\"2\" + -0.140*\"million\" + -0.132*\"0\" + 0.122*\"president\" + -0.119*\"3\" + -0.119*\"5\" + 0.118*\"people\" + -0.111*\"4\"\n",
      "INFO:gensim.models.lsimodel:topic #2(1297.430): -0.854*\"y\" + -0.476*\"n\" + -0.117*\"democrats\" + -0.117*\"republicans\" + -0.060*\"x\" + -0.043*\"republican\" + -0.040*\"democrat\" + -0.029*\"smith\" + -0.023*\"d\" + -0.022*\"yes\"\n",
      "INFO:gensim.models.lsimodel:topic #3(1094.825): -0.605*\"percent\" + 0.312*\"1\" + 0.263*\"million\" + 0.184*\"cents\" + -0.181*\"bush\" + -0.119*\"234\" + -0.119*\"567\" + 0.116*\"new\" + 0.114*\"2\" + -0.111*\"dukakis\"\n",
      "INFO:gensim.models.lsimodel:topic #4(960.243): -0.466*\"bush\" + -0.237*\"president\" + 0.235*\"police\" + -0.202*\"dukakis\" + 0.168*\"000\" + 0.158*\"people\" + -0.151*\"house\" + 0.133*\"percent\" + -0.117*\"reagan\" + -0.114*\"new\"\n",
      "INFO:gensim.models.lsimodel:preparing a new chunk of documents\n",
      "INFO:gensim.models.lsimodel:using 100 extra samples and 2 power iterations\n",
      "INFO:gensim.models.lsimodel:1st phase: constructing (267318, 200) action matrix\n",
      "INFO:gensim.models.lsimodel:orthonormalizing (267318, 200) action matrix\n",
      "INFO:gensim.models.lsimodel:2nd phase: running dense svd on (200, 20000) matrix\n",
      "INFO:gensim.models.lsimodel:computing the final decomposition\n",
      "INFO:gensim.models.lsimodel:keeping 100 factors (discarding 15.677% of energy spectrum)\n",
      "INFO:gensim.models.lsimodel:merging projections: (267318, 100) + (267318, 100)\n",
      "INFO:gensim.models.lsimodel:keeping 100 factors (discarding 1.275% of energy spectrum)\n",
      "INFO:gensim.models.lsimodel:processed documents up to #160000\n",
      "INFO:gensim.models.lsimodel:topic #0(2449.479): 0.286*\"percent\" + 0.200*\"new\" + 0.199*\"1\" + 0.178*\"million\" + 0.145*\"two\" + 0.140*\"000\" + 0.139*\"people\" + 0.136*\"government\" + 0.131*\"president\" + 0.126*\"2\"\n",
      "INFO:gensim.models.lsimodel:topic #1(1578.097): -0.637*\"percent\" + -0.228*\"1\" + -0.220*\"0\" + -0.144*\"2\" + 0.117*\"president\" + 0.115*\"people\" + -0.110*\"million\" + -0.108*\"3\" + -0.106*\"5\" + -0.102*\"4\"\n",
      "INFO:gensim.models.lsimodel:topic #2(1314.272): -0.848*\"y\" + -0.483*\"n\" + -0.119*\"democrats\" + -0.118*\"republicans\" + -0.062*\"x\" + -0.046*\"republican\" + -0.041*\"democrat\" + -0.029*\"smith\" + -0.025*\"d\" + -0.024*\"yes\"\n",
      "INFO:gensim.models.lsimodel:topic #3(1201.203): -0.534*\"percent\" + 0.332*\"1\" + 0.287*\"million\" + -0.199*\"bush\" + 0.181*\"cents\" + -0.146*\"0\" + -0.133*\"dukakis\" + 0.133*\"2\" + 0.108*\"higher\" + 0.107*\"lower\"\n",
      "INFO:gensim.models.lsimodel:topic #4(1041.856): -0.472*\"bush\" + -0.266*\"dukakis\" + 0.223*\"police\" + -0.219*\"president\" + 0.153*\"000\" + 0.147*\"people\" + -0.128*\"house\" + 0.127*\"percent\" + -0.126*\"campaign\" + -0.122*\"new\"\n",
      "INFO:gensim.models.lsimodel:preparing a new chunk of documents\n",
      "INFO:gensim.models.lsimodel:using 100 extra samples and 2 power iterations\n",
      "INFO:gensim.models.lsimodel:1st phase: constructing (267318, 200) action matrix\n",
      "INFO:gensim.models.lsimodel:orthonormalizing (267318, 200) action matrix\n",
      "INFO:gensim.models.lsimodel:2nd phase: running dense svd on (200, 4597) matrix\n",
      "INFO:gensim.models.lsimodel:computing the final decomposition\n",
      "INFO:gensim.models.lsimodel:keeping 100 factors (discarding 15.699% of energy spectrum)\n",
      "INFO:gensim.models.lsimodel:merging projections: (267318, 100) + (267318, 100)\n",
      "INFO:gensim.models.lsimodel:keeping 100 factors (discarding 0.683% of energy spectrum)\n",
      "INFO:gensim.models.lsimodel:processed documents up to #164597\n",
      "INFO:gensim.models.lsimodel:topic #0(2488.640): 0.296*\"percent\" + 0.200*\"new\" + 0.199*\"1\" + 0.177*\"million\" + 0.144*\"two\" + 0.140*\"000\" + 0.138*\"people\" + 0.135*\"government\" + 0.130*\"president\" + 0.126*\"2\"\n",
      "INFO:gensim.models.lsimodel:topic #1(1623.688): -0.657*\"percent\" + -0.245*\"0\" + -0.211*\"1\" + -0.134*\"2\" + 0.115*\"president\" + 0.114*\"people\" + -0.103*\"3\" + 0.102*\"two\" + -0.099*\"5\" + 0.097*\"government\"\n",
      "INFO:gensim.models.lsimodel:topic #2(1314.509): -0.847*\"y\" + -0.482*\"n\" + -0.120*\"democrats\" + -0.118*\"republicans\" + -0.062*\"x\" + -0.047*\"republican\" + -0.041*\"democrat\" + -0.029*\"smith\" + -0.026*\"d\" + -0.026*\"percent\"\n",
      "INFO:gensim.models.lsimodel:topic #3(1235.520): -0.497*\"percent\" + 0.339*\"1\" + 0.295*\"million\" + -0.208*\"bush\" + 0.179*\"cents\" + -0.166*\"0\" + 0.141*\"2\" + -0.139*\"dukakis\" + -0.111*\"president\" + 0.109*\"higher\"\n",
      "INFO:gensim.models.lsimodel:topic #4(1061.442): -0.474*\"bush\" + -0.272*\"dukakis\" + 0.219*\"police\" + -0.216*\"president\" + 0.155*\"000\" + 0.144*\"people\" + -0.126*\"campaign\" + -0.124*\"house\" + -0.122*\"new\" + 0.120*\"government\"\n"
     ]
    }
   ],
   "source": [
    "#train transformation model for LSI\n",
    "lsi_train = gensim.models.LsiModel(corpus=corpus, id2word=dictionary_corpus, num_topics=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:saving Projection object under models/lsi_train100.projection, separately None\n",
      "INFO:gensim.utils:storing np array 'u' to models/lsi_train100.projection.u.npy\n",
      "INFO:gensim.utils:saved models/lsi_train100.projection\n",
      "INFO:gensim.utils:saving LsiModel object under models/lsi_train100, separately None\n",
      "INFO:gensim.utils:not storing attribute projection\n",
      "INFO:gensim.utils:not storing attribute dispatcher\n",
      "INFO:gensim.utils:saved models/lsi_train100\n"
     ]
    }
   ],
   "source": [
    "DOC_lsi = lsi_train[corpus]\n",
    "QUERY_lsi = lsi_train[query_bow]\n",
    "#save model to disk\n",
    "lsi_train.save('models/lsi_train100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loading LsiModel object from models/lsi_train100\n",
      "INFO:gensim.utils:loading id2word recursively from models/lsi_train100.id2word.* with mmap=None\n",
      "INFO:gensim.utils:setting ignored attribute projection to None\n",
      "INFO:gensim.utils:setting ignored attribute dispatcher to None\n",
      "INFO:gensim.utils:loaded models/lsi_train100\n",
      "INFO:gensim.utils:loading LsiModel object from models/lsi_train100.projection\n",
      "INFO:gensim.utils:loading u from models/lsi_train100.projection.u.npy with mmap=None\n",
      "INFO:gensim.utils:loaded models/lsi_train100.projection\n"
     ]
    }
   ],
   "source": [
    "#load lsi from disk\n",
    "lsi_train = gensim.models.LsiModel.load('models/lsi_train100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2vec(representation):\n",
    "    vector = np.zeros(lsi_train.num_topics, dtype=float)\n",
    "    for element in representation:\n",
    "        vector[element[0]] = element[1]\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc processed is:  0\n",
      "Doc processed is:  5000\n",
      "Doc processed is:  10000\n",
      "Doc processed is:  15000\n",
      "Doc processed is:  20000\n",
      "Doc processed is:  25000\n",
      "Doc processed is:  30000\n",
      "Doc processed is:  35000\n",
      "Doc processed is:  40000\n",
      "Doc processed is:  45000\n",
      "Doc processed is:  50000\n",
      "Doc processed is:  55000\n",
      "Doc processed is:  60000\n",
      "Doc processed is:  65000\n",
      "Doc processed is:  70000\n",
      "Doc processed is:  75000\n",
      "Doc processed is:  80000\n",
      "Doc processed is:  85000\n",
      "Doc processed is:  90000\n",
      "Doc processed is:  95000\n",
      "Doc processed is:  100000\n",
      "Doc processed is:  105000\n",
      "Doc processed is:  110000\n",
      "Doc processed is:  115000\n",
      "Doc processed is:  120000\n",
      "Doc processed is:  125000\n",
      "Doc processed is:  130000\n",
      "Doc processed is:  135000\n",
      "Doc processed is:  140000\n",
      "Doc processed is:  145000\n",
      "Doc processed is:  150000\n",
      "Doc processed is:  155000\n",
      "Doc processed is:  160000\n"
     ]
    }
   ],
   "source": [
    "#create query and document representation\n",
    "\n",
    "document_representation = {}\n",
    "lsi_cosine_similarity = {}\n",
    "\n",
    "for doc_lsi_term, doc_lsi_id in (zip(DOC_lsi, range(num_documents))):\n",
    "    if(doc_lsi_id%5000==0):\n",
    "        print('Doc processed is: ', str(doc_lsi_id))\n",
    "    document_representation[doc_lsi_id] = doc_lsi_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query is:  ('84', [2563, 201208, 601, 351, 851, 122153])\n",
      "Query is:  ('66', [1535, 1434, 3628])\n",
      "Query is:  ('98', [91954, 176407, 851, 2324])\n",
      "Query is:  ('161', [4072, 654])\n",
      "Query is:  ('168', [2387, 23989])\n",
      "Query is:  ('195', [75, 57, 29891, 774, 121675, 235])\n",
      "Query is:  ('80', [214, 252, 789, 187515])\n",
      "Query is:  ('64', [1987, 451])\n",
      "Query is:  ('51', [5872, 3066])\n",
      "Query is:  ('58', [3454, 1920])\n",
      "Query is:  ('55', [4400, 235])\n",
      "Query is:  ('121', [168, 1042])\n",
      "Query is:  ('147', [5442, 5609, 5465, 276, 395])\n",
      "Query is:  ('94', [774, 20692, 1033])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "164597",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-735707aa6281>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop1000_tfidf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery_lsi_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0md_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocument_representation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mdocument_lsi_representation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mlsi_cosine_similarity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery_lsi_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_lsi_representation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument_lsi_representation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 164597"
     ]
    }
   ],
   "source": [
    "for query_lsi_id, query_lsi_term in (zip(tokenized_queries.items(), QUERY_lsi)):\n",
    "    print('Query is: ', str(query_lsi_id))\n",
    "    lsi_cosine_similarity[query_lsi_id[0]] = []\n",
    "    query_lsi_representation = convert2vec(query_lsi_term)\n",
    "    \n",
    "    for result in top1000_tfidf[query_lsi_id[0]]:\n",
    "        d_id = result[2]\n",
    "        d = document_representation[d_id]\n",
    "        document_lsi_representation = convert2vec(d)\n",
    "        lsi_cosine_similarity[query_lsi_id[0]].append((cosine_similarity(query_lsi_representation.reshape(1,-1), document_lsi_representation.reshape(1, -1)), index.document(d_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query_lsi_id, query_lsi_term in (zip(tokenized_queries.items(), QUERY_lsi)):\n",
    "    pprint(lsi_cosine_similarity[query_lsi_id[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3:  Word embeddings for ranking [20 points] (open-ended) ###\n",
    "\n",
    "First create word embeddings on the corpus we provided using [word2vec](http://arxiv.org/abs/1411.2738) -- [gensim implementation](https://radimrehurek.com/gensim/models/word2vec.html). You should extract the indexed documents using pyndri and provide them to gensim for training a model (see example [here](https://github.com/nickvosk/pyndri/blob/master/examples/word2vec.py)).\n",
    "   \n",
    "This is an open-ended task. It is left up you to decide how you will combine word embeddings to derive query and document representations. Note that since we provide the implementation for training word2vec, you will be graded based on your creativity on combining word embeddings for building query and document representations.\n",
    "\n",
    "Note: If you want to experiment with pre-trained word embeddings on a different corpus, you can use the word embeddings we provide alongside the assignment (./data/reduced_vectors_google.txt.tar.gz). These are the [google word2vec word embeddings](https://code.google.com/archive/p/word2vec/), reduced to only the words that appear in the document collection we use in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WORD2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Created variables\n"
     ]
    }
   ],
   "source": [
    "#creating names, constants and files\n",
    "word2vec_file = 'word2vec_model.bin'\n",
    "doc2vec_file = 'doc2vec_model.doc2vec'\n",
    "\n",
    "corpus_file = 'pyndri_corpus.mm'\n",
    "dict_file='corpdict.dict'\n",
    "\n",
    "tfidf_model_file = 'tfidf_model.model'\n",
    "tfidf_corpus_file = 'tfidf_corpus.mm'\n",
    "\n",
    "lsi_model_file = 'lsi_model.model'\n",
    "lsi_corpus_file = 'lsi_corpus.mm'\n",
    "\n",
    "lda_model_file = 'lda_model.model'\n",
    "lda_corpus_file = 'lda_corpus.mm'\n",
    "\n",
    "LSI_SHAPE = 64\n",
    "LDA_SHAPE = 32\n",
    "NN_LAYERS = 32\n",
    "\n",
    "AVAILABLE_CORES = multiprocessing.cpu_count()\n",
    "\n",
    "logging.info(\"Created variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initializing word2vec.\n",
      "INFO:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #10000, processed 2607270 words, keeping 73866 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #20000, processed 5208413 words, keeping 101162 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #30000, processed 7779447 words, keeping 121185 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #40000, processed 10402346 words, keeping 138774 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #50000, processed 12981963 words, keeping 153153 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #60000, processed 15578280 words, keeping 165943 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #70000, processed 18181273 words, keeping 177185 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #80000, processed 20777766 words, keeping 188142 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #90000, processed 23336840 words, keeping 199575 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #100000, processed 25881289 words, keeping 211216 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #110000, processed 28375782 words, keeping 221773 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #120000, processed 30893236 words, keeping 231132 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #130000, processed 33417709 words, keeping 240313 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #140000, processed 35992742 words, keeping 248661 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #150000, processed 38498286 words, keeping 256455 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #160000, processed 41037535 words, keeping 263913 word types\n",
      "INFO:gensim.models.word2vec:collected 267318 word types from a corpus of 42208958 raw words and 164597 sentences\n",
      "INFO:gensim.models.word2vec:Loading a fresh vocabulary\n",
      "INFO:gensim.models.word2vec:min_count=5 retains 102844 unique words (38% of original 267318, drops 164474)\n",
      "INFO:gensim.models.word2vec:min_count=5 leaves 41928258 word corpus (99% of original 42208958, drops 280700)\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 267318 items\n",
      "INFO:gensim.models.word2vec:sample=1e-05 downsamples 5821 most-common words\n",
      "INFO:gensim.models.word2vec:downsampling leaves estimated 18130987 word corpus (43.2% of prior 41928258)\n",
      "INFO:gensim.models.word2vec:estimated required memory for 102844 words and 32 dimensions: 77750064 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "INFO:gensim.models.word2vec:training model with 4 workers on 102844 vocabulary and 32 features, using sg=1 hs=0 sample=1e-05 negative=5 window=5\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 0.15% examples, 129761 words/s, in_qsize 0, out_qsize 2\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 0.29% examples, 129605 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 0.42% examples, 124810 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 0.56% examples, 126271 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 0.71% examples, 128007 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 0.85% examples, 129077 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 0.98% examples, 127564 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 1.14% examples, 129297 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 1.30% examples, 130893 words/s, in_qsize 1, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 1.48% examples, 134156 words/s, in_qsize 1, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 1.67% examples, 137529 words/s, in_qsize 0, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 1.85% examples, 138992 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 2.05% examples, 142086 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 2.23% examples, 144211 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 2.42% examples, 145958 words/s, in_qsize 7, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 2.62% examples, 147645 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 2.81% examples, 149399 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 3.00% examples, 150515 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 3.20% examples, 151596 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 3.39% examples, 152698 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 3.59% examples, 153496 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 3.79% examples, 154684 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 3.98% examples, 155679 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 4.16% examples, 156233 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 4.35% examples, 156927 words/s, in_qsize 7, out_qsize 3\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 4.56% examples, 157871 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 4.74% examples, 158450 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 4.92% examples, 158980 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 5.12% examples, 159681 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 5.29% examples, 159558 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 5.46% examples, 159135 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 5.63% examples, 158997 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 5.80% examples, 158855 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 5.98% examples, 158695 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 6.16% examples, 158896 words/s, in_qsize 1, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 6.33% examples, 158681 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 6.50% examples, 158507 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 6.67% examples, 158323 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 6.83% examples, 157967 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 7.02% examples, 158372 words/s, in_qsize 1, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 7.18% examples, 158185 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 7.36% examples, 158229 words/s, in_qsize 3, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 7.54% examples, 158227 words/s, in_qsize 5, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 7.71% examples, 158113 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 7.87% examples, 157958 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 8.04% examples, 158002 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 8.22% examples, 157982 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 8.41% examples, 158221 words/s, in_qsize 3, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 8.58% examples, 158027 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 8.75% examples, 158126 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 8.93% examples, 157993 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 9.11% examples, 157990 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 9.28% examples, 157908 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 9.46% examples, 157898 words/s, in_qsize 7, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 9.63% examples, 157877 words/s, in_qsize 8, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:PROGRESS: at 9.80% examples, 157824 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 9.98% examples, 157992 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 10.15% examples, 157879 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 10.34% examples, 157885 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 10.51% examples, 157886 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 10.70% examples, 157963 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 10.87% examples, 157864 words/s, in_qsize 8, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 11.05% examples, 157870 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 11.22% examples, 157815 words/s, in_qsize 7, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 11.41% examples, 157908 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 11.59% examples, 157891 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 11.77% examples, 157785 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 11.95% examples, 157697 words/s, in_qsize 7, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 12.13% examples, 157801 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 12.30% examples, 157686 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 12.49% examples, 157751 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 12.67% examples, 157702 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 12.86% examples, 157756 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 13.05% examples, 157973 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 13.24% examples, 158035 words/s, in_qsize 1, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 13.41% examples, 157900 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 13.59% examples, 157798 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 13.77% examples, 157828 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 13.96% examples, 157912 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 14.14% examples, 157866 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 14.32% examples, 157884 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 14.50% examples, 157936 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 14.68% examples, 157947 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 14.85% examples, 157816 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 15.04% examples, 157885 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 15.20% examples, 157702 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 15.39% examples, 157733 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 15.57% examples, 157754 words/s, in_qsize 3, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 15.76% examples, 157887 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 15.94% examples, 157920 words/s, in_qsize 1, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 16.13% examples, 157926 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 16.29% examples, 157846 words/s, in_qsize 3, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 16.46% examples, 157725 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 16.64% examples, 157747 words/s, in_qsize 5, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 16.81% examples, 157731 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 16.98% examples, 157653 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 17.16% examples, 157667 words/s, in_qsize 5, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 17.34% examples, 157623 words/s, in_qsize 2, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 17.53% examples, 157733 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 17.68% examples, 157568 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 17.87% examples, 157570 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 18.04% examples, 157541 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 18.23% examples, 157504 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 18.43% examples, 157733 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 18.61% examples, 157718 words/s, in_qsize 0, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 18.79% examples, 157754 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 18.98% examples, 157824 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 19.14% examples, 157667 words/s, in_qsize 7, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 19.31% examples, 157602 words/s, in_qsize 6, out_qsize 2\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 19.50% examples, 157655 words/s, in_qsize 4, out_qsize 2\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 19.69% examples, 157733 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 19.85% examples, 157639 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 20.04% examples, 157716 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 20.20% examples, 157634 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 20.37% examples, 157571 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 20.55% examples, 157604 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 20.72% examples, 157615 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 20.90% examples, 157637 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 21.08% examples, 157597 words/s, in_qsize 7, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 21.27% examples, 157731 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 21.43% examples, 157645 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 21.60% examples, 157615 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 21.78% examples, 157652 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 21.97% examples, 157680 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 22.14% examples, 157638 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 22.32% examples, 157598 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 22.49% examples, 157540 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 22.68% examples, 157636 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 22.86% examples, 157622 words/s, in_qsize 3, out_qsize 2\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 23.03% examples, 157637 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 23.21% examples, 157632 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 23.39% examples, 157664 words/s, in_qsize 5, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 23.57% examples, 157665 words/s, in_qsize 7, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 23.75% examples, 157666 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 23.92% examples, 157666 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 24.09% examples, 157651 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 24.27% examples, 157735 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 24.44% examples, 157723 words/s, in_qsize 1, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 24.61% examples, 157772 words/s, in_qsize 0, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:PROGRESS: at 24.78% examples, 157728 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 24.95% examples, 157802 words/s, in_qsize 0, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 25.13% examples, 157819 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 25.28% examples, 157645 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 25.46% examples, 157666 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 25.64% examples, 157709 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 25.81% examples, 157653 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 25.99% examples, 157661 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 26.15% examples, 157537 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 26.30% examples, 157420 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 26.49% examples, 157479 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 26.67% examples, 157494 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 26.84% examples, 157504 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 27.01% examples, 157458 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 27.18% examples, 157481 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 27.34% examples, 157312 words/s, in_qsize 0, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 27.46% examples, 157007 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 27.62% examples, 156943 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 27.77% examples, 156810 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 27.94% examples, 156830 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 28.11% examples, 156837 words/s, in_qsize 7, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 28.30% examples, 156868 words/s, in_qsize 7, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 28.48% examples, 156934 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 28.65% examples, 156925 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 28.84% examples, 157072 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 29.03% examples, 157092 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 29.20% examples, 157056 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 29.38% examples, 157068 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 29.55% examples, 157119 words/s, in_qsize 1, out_qsize 2\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 29.73% examples, 157135 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 29.89% examples, 157127 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 30.07% examples, 157141 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 30.25% examples, 157125 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 30.43% examples, 157148 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 30.63% examples, 157169 words/s, in_qsize 3, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 30.80% examples, 157104 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 30.98% examples, 157135 words/s, in_qsize 5, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 31.15% examples, 157096 words/s, in_qsize 5, out_qsize 3\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 31.33% examples, 157168 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 31.54% examples, 157273 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 31.74% examples, 157319 words/s, in_qsize 5, out_qsize 2\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 31.94% examples, 157413 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 32.13% examples, 157489 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 32.33% examples, 157595 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 32.53% examples, 157675 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 32.72% examples, 157741 words/s, in_qsize 7, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 32.92% examples, 157789 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 33.12% examples, 157912 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 33.32% examples, 157945 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 33.52% examples, 158013 words/s, in_qsize 5, out_qsize 3\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 33.71% examples, 158086 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 33.92% examples, 158192 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 34.12% examples, 158239 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 34.32% examples, 158343 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 34.52% examples, 158396 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 34.71% examples, 158454 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 34.91% examples, 158537 words/s, in_qsize 7, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 35.11% examples, 158625 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 35.31% examples, 158658 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 35.50% examples, 158686 words/s, in_qsize 6, out_qsize 3\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 35.70% examples, 158823 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 35.87% examples, 158776 words/s, in_qsize 5, out_qsize 3\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 36.08% examples, 158874 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 36.26% examples, 158907 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 36.46% examples, 158992 words/s, in_qsize 5, out_qsize 3\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 36.67% examples, 159083 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 36.86% examples, 159156 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 37.06% examples, 159259 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 37.24% examples, 159279 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 37.44% examples, 159306 words/s, in_qsize 7, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 37.63% examples, 159373 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 37.83% examples, 159435 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 38.03% examples, 159507 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 38.22% examples, 159544 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 38.42% examples, 159627 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 38.63% examples, 159712 words/s, in_qsize 7, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 38.81% examples, 159713 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 39.01% examples, 159776 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 39.20% examples, 159835 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 39.40% examples, 159877 words/s, in_qsize 7, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 39.60% examples, 159956 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 39.79% examples, 159992 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 39.99% examples, 160048 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 40.19% examples, 160110 words/s, in_qsize 5, out_qsize 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:PROGRESS: at 40.38% examples, 160190 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 40.60% examples, 160383 words/s, in_qsize 1, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 40.79% examples, 160403 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 40.98% examples, 160476 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 41.16% examples, 160505 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 41.35% examples, 160549 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 41.54% examples, 160596 words/s, in_qsize 6, out_qsize 2\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 41.74% examples, 160627 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 41.94% examples, 160685 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 42.15% examples, 160807 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 42.33% examples, 160838 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 42.52% examples, 160892 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 42.72% examples, 160963 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 42.91% examples, 161013 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 43.10% examples, 161053 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 43.29% examples, 161076 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 43.48% examples, 161111 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 43.68% examples, 161166 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 43.86% examples, 161188 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 44.06% examples, 161250 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 44.24% examples, 161283 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 44.44% examples, 161348 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 44.62% examples, 161398 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 44.81% examples, 161430 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 44.98% examples, 161467 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 45.17% examples, 161471 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 45.37% examples, 161536 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 45.56% examples, 161572 words/s, in_qsize 6, out_qsize 2\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 45.77% examples, 161652 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 45.97% examples, 161723 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 46.16% examples, 161759 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 46.36% examples, 161809 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 46.56% examples, 161922 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 46.74% examples, 161874 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 46.93% examples, 161902 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 47.12% examples, 161966 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 47.32% examples, 162001 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 47.50% examples, 162033 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 47.69% examples, 162091 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 47.89% examples, 162167 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 48.09% examples, 162242 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 48.28% examples, 162255 words/s, in_qsize 2, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 48.45% examples, 162234 words/s, in_qsize 1, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 48.65% examples, 162309 words/s, in_qsize 1, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 48.84% examples, 162371 words/s, in_qsize 1, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 49.03% examples, 162370 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 49.23% examples, 162418 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 49.41% examples, 162420 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 49.61% examples, 162486 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 49.81% examples, 162537 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 50.00% examples, 162549 words/s, in_qsize 7, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 50.20% examples, 162588 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 50.39% examples, 162614 words/s, in_qsize 6, out_qsize 2\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 50.60% examples, 162684 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 50.78% examples, 162697 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 50.97% examples, 162725 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 51.16% examples, 162746 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 51.33% examples, 162714 words/s, in_qsize 6, out_qsize 2\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 51.53% examples, 162739 words/s, in_qsize 5, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 51.74% examples, 162795 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 51.93% examples, 162817 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 52.11% examples, 162822 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 52.30% examples, 162833 words/s, in_qsize 5, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 52.50% examples, 162864 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 52.69% examples, 162877 words/s, in_qsize 7, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 52.89% examples, 162906 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 53.08% examples, 162927 words/s, in_qsize 7, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 53.27% examples, 162936 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 53.48% examples, 162980 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 53.68% examples, 163004 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 53.87% examples, 163011 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 54.06% examples, 163028 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 54.27% examples, 163084 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 54.46% examples, 163126 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 54.65% examples, 163134 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 54.84% examples, 163129 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 55.04% examples, 163149 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 55.24% examples, 163162 words/s, in_qsize 7, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 55.43% examples, 163177 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 55.61% examples, 163195 words/s, in_qsize 7, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 55.81% examples, 163222 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 56.01% examples, 163248 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 56.20% examples, 163287 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 56.39% examples, 163318 words/s, in_qsize 7, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:PROGRESS: at 56.59% examples, 163322 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 56.79% examples, 163367 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 56.98% examples, 163390 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 57.16% examples, 163394 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 57.37% examples, 163424 words/s, in_qsize 7, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 57.57% examples, 163446 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 57.76% examples, 163464 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 57.96% examples, 163501 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 58.17% examples, 163514 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 58.38% examples, 163588 words/s, in_qsize 5, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 58.58% examples, 163619 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 58.76% examples, 163617 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 58.97% examples, 163660 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 59.18% examples, 163711 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 59.36% examples, 163713 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 59.56% examples, 163739 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 59.74% examples, 163738 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 59.94% examples, 163767 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 60.13% examples, 163779 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 60.33% examples, 163839 words/s, in_qsize 1, out_qsize 2\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 60.51% examples, 163850 words/s, in_qsize 6, out_qsize 2\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 60.71% examples, 163882 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 60.90% examples, 163924 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 61.08% examples, 163917 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 61.27% examples, 163946 words/s, in_qsize 5, out_qsize 2\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 61.46% examples, 163964 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 61.64% examples, 163966 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 61.84% examples, 164013 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 62.03% examples, 164040 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 62.22% examples, 164062 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 62.40% examples, 164076 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 62.57% examples, 164049 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 62.77% examples, 164090 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 62.96% examples, 164111 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 63.14% examples, 164095 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 63.34% examples, 164132 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 63.53% examples, 164145 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 63.72% examples, 164171 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 63.92% examples, 164207 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 64.11% examples, 164218 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 64.30% examples, 164250 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 64.48% examples, 164261 words/s, in_qsize 6, out_qsize 2\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 64.69% examples, 164337 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 64.86% examples, 164338 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 65.05% examples, 164375 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 65.23% examples, 164388 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 65.44% examples, 164401 words/s, in_qsize 6, out_qsize 2\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 65.64% examples, 164438 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 65.82% examples, 164444 words/s, in_qsize 5, out_qsize 2\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 66.02% examples, 164462 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 66.21% examples, 164474 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 66.40% examples, 164489 words/s, in_qsize 7, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 66.59% examples, 164529 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 66.78% examples, 164547 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 66.96% examples, 164555 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 67.15% examples, 164597 words/s, in_qsize 5, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 67.34% examples, 164603 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 67.54% examples, 164642 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 67.72% examples, 164652 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 67.91% examples, 164691 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 68.11% examples, 164724 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 68.30% examples, 164739 words/s, in_qsize 1, out_qsize 2\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 68.49% examples, 164760 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 68.68% examples, 164791 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 68.87% examples, 164802 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 69.06% examples, 164808 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 69.26% examples, 164831 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 69.46% examples, 164855 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 69.64% examples, 164877 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 69.83% examples, 164898 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 70.02% examples, 164925 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 70.21% examples, 164943 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 70.41% examples, 164964 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 70.62% examples, 164991 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 70.81% examples, 165010 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 71.00% examples, 165008 words/s, in_qsize 5, out_qsize 3\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 71.19% examples, 165040 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 71.37% examples, 165025 words/s, in_qsize 4, out_qsize 3\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 71.59% examples, 165075 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 71.78% examples, 165074 words/s, in_qsize 5, out_qsize 2\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 72.00% examples, 165124 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 72.19% examples, 165152 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 72.39% examples, 165154 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 72.60% examples, 165175 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:PROGRESS: at 72.80% examples, 165203 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 72.99% examples, 165209 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 73.19% examples, 165241 words/s, in_qsize 4, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 73.40% examples, 165259 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 73.61% examples, 165288 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 73.80% examples, 165296 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 74.00% examples, 165334 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 74.21% examples, 165366 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 74.41% examples, 165386 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 74.60% examples, 165375 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 74.79% examples, 165394 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 74.99% examples, 165409 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 75.18% examples, 165412 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 75.38% examples, 165438 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 75.58% examples, 165459 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 75.77% examples, 165485 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 75.96% examples, 165482 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 76.15% examples, 165494 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 76.35% examples, 165530 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 76.54% examples, 165524 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 76.74% examples, 165539 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 76.92% examples, 165540 words/s, in_qsize 5, out_qsize 2\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 77.10% examples, 165550 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 77.31% examples, 165580 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 77.51% examples, 165609 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 77.70% examples, 165627 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 77.89% examples, 165602 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 78.10% examples, 165639 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 78.30% examples, 165653 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 78.49% examples, 165663 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 78.69% examples, 165685 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 78.88% examples, 165674 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 79.08% examples, 165690 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 79.27% examples, 165687 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 79.46% examples, 165700 words/s, in_qsize 5, out_qsize 2\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 79.66% examples, 165721 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 79.86% examples, 165755 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 80.05% examples, 165766 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 80.24% examples, 165784 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 80.44% examples, 165828 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 80.63% examples, 165831 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 80.83% examples, 165856 words/s, in_qsize 7, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 81.02% examples, 165881 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 81.21% examples, 165877 words/s, in_qsize 8, out_qsize 2\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 81.40% examples, 165904 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 81.60% examples, 165936 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 81.79% examples, 165952 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 81.99% examples, 165972 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 82.18% examples, 165997 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 82.36% examples, 165987 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 82.54% examples, 165995 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 82.74% examples, 166022 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 82.93% examples, 166033 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 83.13% examples, 166040 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 83.31% examples, 166039 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 83.51% examples, 166076 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 83.70% examples, 166079 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 83.90% examples, 166091 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 84.08% examples, 166096 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 84.26% examples, 166090 words/s, in_qsize 6, out_qsize 2\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 84.45% examples, 166107 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 84.63% examples, 166118 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 84.81% examples, 166111 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 84.98% examples, 166110 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 85.17% examples, 166117 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 85.36% examples, 166127 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 85.55% examples, 166139 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 85.75% examples, 166149 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 85.93% examples, 166134 words/s, in_qsize 7, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 86.14% examples, 166175 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 86.33% examples, 166194 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 86.52% examples, 166202 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 86.71% examples, 166209 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 86.89% examples, 166212 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 87.09% examples, 166249 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 87.28% examples, 166274 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 87.47% examples, 166281 words/s, in_qsize 7, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 87.67% examples, 166310 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 87.85% examples, 166295 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 88.05% examples, 166321 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 88.25% examples, 166328 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 88.44% examples, 166357 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 88.63% examples, 166373 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 88.82% examples, 166397 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:PROGRESS: at 89.01% examples, 166402 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 89.21% examples, 166425 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 89.40% examples, 166443 words/s, in_qsize 5, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 89.61% examples, 166485 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 89.79% examples, 166472 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 89.99% examples, 166500 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 90.18% examples, 166507 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 90.38% examples, 166508 words/s, in_qsize 5, out_qsize 2\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 90.58% examples, 166539 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 90.77% examples, 166549 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 90.97% examples, 166560 words/s, in_qsize 7, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 91.17% examples, 166596 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 91.36% examples, 166598 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 91.56% examples, 166604 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 91.75% examples, 166603 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 91.95% examples, 166620 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 92.14% examples, 166628 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 92.34% examples, 166653 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 92.54% examples, 166662 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 92.74% examples, 166681 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 92.95% examples, 166704 words/s, in_qsize 4, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 93.15% examples, 166719 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 93.36% examples, 166718 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 93.57% examples, 166730 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 93.76% examples, 166740 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 93.95% examples, 166758 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 94.15% examples, 166753 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 94.35% examples, 166770 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 94.55% examples, 166785 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 94.74% examples, 166779 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 94.94% examples, 166806 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 95.13% examples, 166797 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 95.34% examples, 166809 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 95.54% examples, 166832 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 95.73% examples, 166839 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 95.92% examples, 166841 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 96.12% examples, 166852 words/s, in_qsize 7, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 96.33% examples, 166902 words/s, in_qsize 0, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 96.51% examples, 166888 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 96.71% examples, 166906 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 96.89% examples, 166902 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 97.08% examples, 166905 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 97.30% examples, 166934 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 97.49% examples, 166927 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 97.68% examples, 166946 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 97.88% examples, 166954 words/s, in_qsize 6, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 98.08% examples, 166966 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 98.29% examples, 166982 words/s, in_qsize 5, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 98.49% examples, 166999 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 98.69% examples, 167028 words/s, in_qsize 6, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 98.87% examples, 167007 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 99.08% examples, 167029 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 99.26% examples, 167022 words/s, in_qsize 7, out_qsize 1\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 99.45% examples, 167022 words/s, in_qsize 5, out_qsize 3\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 99.66% examples, 167039 words/s, in_qsize 8, out_qsize 0\n",
      "INFO:gensim.models.word2vec:PROGRESS: at 99.85% examples, 167060 words/s, in_qsize 7, out_qsize 0\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:training on 211044790 raw words (90645191 effective words) took 542.5s, 167102 effective words/s\n",
      "INFO:gensim.models.keyedvectors:storing 102844x32 projection weights into word2vec_model.bin\n"
     ]
    }
   ],
   "source": [
    "logging.info('Initializing word2vec.')\n",
    "\n",
    "dictionary = pyndri.extract_dictionary(index)\n",
    "sentences = pyndri.compat.IndriSentences(index, dictionary)\n",
    "\n",
    "if not os.path.isfile(word2vec_file):\n",
    "    #train a model if it is not present\n",
    "    model = gensim.models.Word2Vec(sentences,\n",
    "                                  size = NN_LAYERS,\n",
    "                                  window = 5, # One-sided window size\n",
    "                                  min_count = 5, # Minimum word frequency\n",
    "                                  workers = AVAILABLE_CORES,\n",
    "                                  sample = 1e-5, # Sub-sample threshold\n",
    "                                  negative = 5, # Number of negative examples\n",
    "                                  sg = True)\n",
    "    model.wv.save_word2vec_format(word2vec_file, binary = True)\n",
    "    \n",
    "else:\n",
    "    #load existing model\n",
    "    word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_file, binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Learning to rank (LTR) [15 points] (open-ended) ###\n",
    "\n",
    "In this task you will get an introduction into learning to rank for information retrieval.\n",
    "\n",
    "You can explore different ways for devising features for the model. Obviously, you can use the retrieval methods you implemented in Task 1, Task 2 and Task 3 as features. Think about other features you can use (e.g. query/document length). Creativity on devising new features and providing motivation for them will be taken into account when grading.\n",
    "\n",
    "For every query, first create a document candidate set using the top-1000 documents using TF-IDF, and subsequently compute features given a query and a document. Note that the feature values of different retrieval methods are likely to be distributed differently.\n",
    "\n",
    "You are adviced to start some pointwise learning to rank algorithm e.g. logistic regression, implemented in [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "Train your LTR model using 10-fold cross validation on the test set. More advanced learning to rank algorithms will be appreciated when grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task 5: Write a report [15 points; instant FAIL if not provided] ###\n",
    "\n",
    "The report should be a PDF file created using the [sigconf ACM template](https://www.acm.org/publications/proceedings-template) and will determine a significant part of your grade.\n",
    "\n",
    "   * It should explain what you have implemented, motivate your experiments and detail what you expect to learn from them. **[10 points]**\n",
    "   * Lastly, provide a convincing analysis of your results and conclude the report accordingly. **[10 points]**\n",
    "      * Do all methods perform similarly on all queries? Why?\n",
    "      * Is there a single retrieval model that outperforms all other retrieval models (i.e., silver bullet)?\n",
    "      * ...\n",
    "\n",
    "**Hand in the report and your self-contained implementation source files.** Only send us the files that matter, organized in a well-documented zip/tgz file with clear instructions on how to reproduce your results. That is, we want to be able to regenerate all your results with minimal effort. You can assume that the index and ground-truth information is present in the same file structure as the one we have provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
