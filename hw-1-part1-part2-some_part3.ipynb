{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework is made by:\n",
    "* Masoumeh Bakhtiariziabari (11813105)\n",
    "* Marianne de Heer Kloots (11138351)\n",
    "* Tharangni Sivaji (11611065)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Part [15 pts]\n",
    "\n",
    "## 1. Hypothesis Testing ‚Äì The problem of multiple comparisons [5 points]\n",
    "Experimentation in AI often happens like this: \n",
    "1. Modify/Build an algorithm\n",
    "2. Compare the algorithm to a baseline by running a hypothesis test.\n",
    "3. If not significant, go back to step A\n",
    "4. If significant, start writing a paper. \n",
    "\n",
    "How many hypothesis tests, m, does it take to get to (with Type I error for each test = Œ±):\n",
    "* P(m<sup>th</sup> experiment gives significant result | m experiments lacking power to reject H<sub>0</sub>)?\n",
    "* P(at least one significant result | m experiments lacking power to reject H<sub>0</sub>)?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "<ol>\n",
    "<li><ul>\n",
    "        <li>P(m<sup>th</sup> experiment gives significant result | m experiments lacking power to reject H<sub>0</sub>)\n",
    "        <li>P(m<sup>th</sup> experiment gives significant result | H<sub>0</sub> is true in all m experiments)\n",
    "        <li>i.e. only the m<sup>th</sup> result is significant whereas (m-1) results are not significant\n",
    "        <li> <b>((1-Œ±)<sup>m-1</sup>)Œ± </b>\n",
    "    </ul>\n",
    "<li><ul>\n",
    "        <li>P(at least one significant result | m experiments lacking power to reject H<sub>0</sub>)\n",
    "        <li>1 - P(no significant result)\n",
    "        <li><b>1 - (1-Œ±)<sup>m</sup></b>\n",
    "    </ul>\n",
    "</ol>\n",
    "</div>\n",
    "*verify?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bias and unfairness in Interleaving experiments [10 points]\n",
    "Balance interleaving has been shown to be biased in a number of corner cases. An example was given during the lecture with two ranked lists of length 3 being interleaved, and a randomly clicking population of users that resulted in algorithm A winning ‚Öî of the time, even though in theory the percentage of wins should be 50% for both algorithms. Can you come up with a situation of two ranked lists of length 3 and a distribution of clicks over them for which Team-draft interleaving is unfair to the better algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "answer"
    ]
   },
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "*answer here*\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Part [85 pts]\n",
    "Commercial search engines use both offline and online approach in evaluating a new search algorithm: they first use an offline test collection to compare the production algorithm (P) with the new experimental algorithm (E); if *E* statistically significantly outperforms *P* with respect to the evaluation measure of their interest, the two algorithms are then compared online through an interleaving experiment.\n",
    "\n",
    "For the purpose of this homework we will assume that the evaluation measures of interest are:\n",
    "1. Binary evaluation measures\n",
    "    1. Precision at rank k,\n",
    "    2. Recall at rank k,\n",
    "    3. Average Precision,\n",
    "2. Multi-graded evaluation measures\n",
    "    1. Normalized Discounted Cumulative Gain at rank k (nDCG@k),\n",
    "    2. Expected Reciprocal Rank (ERR).\n",
    "\n",
    "Further, for the purpose of this homework we will assume that the interleaving algorithms of interest are:\n",
    "Team-Draft Interleaving (Joachims. \"Evaluating retrieval performance using clickthrough data\". Text Mining 2003.),\n",
    "Probabilistic Interleaving (Hofmann, Whiteson, and de Rijke. \"A probabilistic method for inferring preferences from clicks.\" CIKM 2011.).\n",
    " \n",
    "In an interleaving experiment the ranked results of *P* and *E* (against a user query) are interleaved in a single ranked list which is presented to a user. The user then clicks on the results and the algorithm that receives most of the clicks wins the comparison. The experiment is repeated for a number of times (impressions) and the total wins for *P* and *E* are computed. \n",
    "\n",
    "A Sign/Binomial Test is then run to examine whether the difference in wins between the two algorithms is statistically significant (or due to chance). Alternatively one can calculate the proportion of times the *E* wins and test whether this proportion, *p*, is greater than *p<sub>0</sub>=*0.5. This is called an 1-sample 1-sided proportion test.\n",
    "\n",
    "One of the key questions however is **whether offline evaluation and online evaluation outcomes agree with each other**. In this homework you will determine the degree of agreement between offline evaluation measures and interleaving outcomes, by the means of simulations. A similar analysis using actual online data can be found at Chapelle et al. ‚ÄúLarge-Scale Validation and Analysis of Interleaved Search Evaluation‚Äù."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='purple'>[Based on Lecture 1]</font>\n",
    "### Step 1: <font color='darkred'>Simulate Rankings of Relevance for *E* and *P* *(5 points)*</font>\n",
    "\n",
    "In the first step you will generate pairs of rankings of relevance, for the production *P* and experimental *E*, respectively, for a hypothetical query **q**. Assume a 3-graded relevance, i.e. `{N, R, HR}`. Construct all possible *P* and *E* ranking pairs of length 5. This step should give you about.\n",
    "\n",
    "Example:\n",
    "\n",
    "    P: {N N N N N}\n",
    "    E: {N N N N R}\n",
    "    ‚Ä¶\n",
    "    P: {HR HR HR HR R}\n",
    "    E: {HR HR HR HR HR}\n",
    "\n",
    "(Note 1: If you do not have enough computational power, sample 5000 pair uniformly at random to show your work.)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO: add comments\n",
    "from itertools import product\n",
    "from pprint import pprint\n",
    "import random\n",
    "\n",
    "algorithms = ['P', 'E']\n",
    "relevance_grades = {'N', 'HR', 'R'}\n",
    "rankings = [ranking for ranking in product(relevance_grades, repeat=5)]\n",
    "algorithm_rankings = [product(alg, rankings) for alg in algorithms]\n",
    "\n",
    "ranking_pairs = [pair for pair in product(*algorithm_rankings)]\n",
    "print(len(ranking_pairs))\n",
    "#pprint(ranking_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of combinations: 59049\n",
      "number of non-equal combinations: 58806\n",
      "[(('P', ('N', 'HR', 'R', 'N', 'N')), ('E', ('N', 'N', 'HR', 'HR', 'N'))),\n",
      " (('P', ('HR', 'R', 'HR', 'N', 'HR')), ('E', ('HR', 'HR', 'R', 'N', 'HR'))),\n",
      " (('P', ('R', 'R', 'R', 'R', 'N')), ('E', ('HR', 'N', 'N', 'R', 'R'))),\n",
      " (('P', ('HR', 'R', 'N', 'R', 'HR')), ('E', ('N', 'N', 'R', 'HR', 'R'))),\n",
      " (('P', ('HR', 'N', 'HR', 'HR', 'R')), ('E', ('HR', 'HR', 'N', 'N', 'R'))),\n",
      " (('P', ('HR', 'N', 'N', 'R', 'N')), ('E', ('R', 'R', 'HR', 'N', 'R'))),\n",
      " (('P', ('N', 'N', 'N', 'R', 'N')), ('E', ('HR', 'R', 'HR', 'N', 'HR'))),\n",
      " (('P', ('N', 'HR', 'HR', 'N', 'N')), ('E', ('N', 'N', 'R', 'R', 'N'))),\n",
      " (('P', ('HR', 'R', 'N', 'HR', 'N')), ('E', ('N', 'HR', 'R', 'R', 'HR'))),\n",
      " (('P', ('R', 'N', 'N', 'HR', 'R')), ('E', ('HR', 'HR', 'R', 'R', 'N')))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from itertools import product\n",
    "from pprint import pprint\n",
    "import random\n",
    "\n",
    "# define collections of algorithms and relevance grades\n",
    "algorithms = ['P', 'E']\n",
    "relevance_grades = ['N', 'HR', 'R']\n",
    "\n",
    "# all possible ranking sequences\n",
    "# list of rankings [('HR', 'HR', 'HR', 'HR', 'HR') ... ('N', 'N', 'N', 'N', 'N')]\n",
    "rankings = [ranking for ranking in product(relevance_grades, repeat=5)]\n",
    "\n",
    "# all algorithms paired with all rankings \n",
    "# (list of lists with elements e.g. ('P', ('HR', 'HR', 'HR', 'HR', 'HR')))\n",
    "algorithm_rankings = [list(product(alg, rankings)) for alg in algorithms]\n",
    "\n",
    "# all possible pairs of P and E with their rankings\n",
    "all_ranking_pairs = [pair for pair in product(*algorithm_rankings)]\n",
    "\n",
    "# all ranking pairs except equals\n",
    "ranking_pairs = [pair for pair in product(*algorithm_rankings) if pair[0][1] != pair[1][1]]\n",
    "\n",
    "# pretty print\n",
    "print('number of combinations:', len(all_ranking_pairs))\n",
    "print('number of non-equal combinations:', len(ranking_pairs))\n",
    "pprint(random.sample(ranking_pairs, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "**explanation/analysis:**\n",
    "We find 59049 different ranking pairs. This makes sense: each ranking pair consists of 10 relevance values (5 produced by each algorithm), all of which can take on any of the 3 grades (N, R, HR). So there should be 310 = 59049 different combinations, which matches our finding.\n",
    "We then exclude all ranking pairs which have exactly the same relevance grade sequences, which leaves us with 58806 different combinations.\n",
    "We have printed a randomly selected sample of 10 ranking pairs as an example.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: <font color='darkred'>Implement Evaluation Measures *(10 points)*</font>\n",
    "Implement 1 binary and 2 multi-graded evaluation measures out of the 7 measures mentioned above. \n",
    "\n",
    "(Note 2: Some of the aforementioned measures require the total number of relevant and highly relevant documents in the entire collection ‚Äì pay extra attention on how to find this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code\n",
    "#p@K function\n",
    "\n",
    "#import numpy as np\n",
    "def precision(k,_ranking_pairs):\n",
    "    \n",
    "    def calc_precision(i,x):\n",
    "        rel_counter = 0.0\n",
    "        prec = 0.0\n",
    "        for j in range(k):\n",
    "            if _ranking_pairs[i][x][1][j] == 'HR' or _ranking_pairs[i][x][1][j] == 'R':\n",
    "                rel_counter += 1\n",
    "                prec += rel_counter/(1.0+j)\n",
    "        return prec\n",
    "    \n",
    "    prec_list = []\n",
    "    for i in range(len(_ranking_pairs)):\n",
    "        p_prec = calc_precision(i,0)\n",
    "        e_prec = calc_precision(i,1)\n",
    "        prec_list.append((p_prec/k,e_prec/k))\n",
    "        #print(p_prec,e_prec)\n",
    "    \n",
    "    return prec_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query  18575 : \t (0.38888888888888884, 0.5555555555555555)\n",
      "query  41395 : \t (0.5555555555555555, 0.1111111111111111)\n",
      "query  19821 : \t (0.3333333333333333, 0.6666666666666666)\n",
      "query  58575 : \t (1.0, 0.1111111111111111)\n",
      "query  28808 : \t (1.0, 0.1111111111111111)\n",
      "query  9892 : \t (0.38888888888888884, 1.0)\n",
      "query  57194 : \t (1.0, 0.3333333333333333)\n",
      "query  39351 : \t (0.3333333333333333, 1.0)\n",
      "query  14794 : \t (0.16666666666666666, 0.16666666666666666)\n",
      "query  42430 : \t (0.5555555555555555, 0.38888888888888884)\n"
     ]
    }
   ],
   "source": [
    "prec = precision(3,ranking_pairs)\n",
    "\n",
    "random_query_index_list = random.sample(range(len(ranking_pairs)), 10)\n",
    "for i in random_query_index_list:\n",
    "    print(\"query \",i,\": \\t\",prec[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def average_prec(k_max,_ranking_pairs):\n",
    "    temp_list = []\n",
    "    ap_list = [[0,0] for i in range(len(_ranking_pairs))]\n",
    "    for k in range(1,k_max+1):\n",
    "        temp_list = precision(k,_ranking_pairs)\n",
    "        #print(k,temp_list)\n",
    "        for index,item in enumerate(temp_list):\n",
    "            ap_list[index] = (np.array(ap_list[index])+np.array(item)).tolist()\n",
    "    #print(ap_list)\n",
    "    for item in ap_list:\n",
    "        item[:] = [i / k_max for i in item]\n",
    "    return ap_list       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query  28613 : \t ('P', ('HR', 'HR', 'HR', 'N', 'HR')) \t 0.902 \t ('E', ('N', 'R', 'N', 'HR', 'N')) \t 0.173\n",
      "query  27209 : \t ('P', ('HR', 'HR', 'N', 'HR', 'HR')) \t 0.813 \t ('E', ('HR', 'N', 'R', 'R', 'N')) \t 0.629\n",
      "query  18276 : \t ('P', ('N', 'R', 'R', 'HR', 'N')) \t 0.300 \t ('E', ('HR', 'HR', 'R', 'N', 'HR')) \t 0.902\n",
      "query  33382 : \t ('P', ('HR', 'R', 'N', 'N', 'R')) \t 0.737 \t ('E', ('R', 'R', 'HR', 'HR', 'HR')) \t 1.000\n",
      "query  13289 : \t ('P', ('N', 'R', 'N', 'N', 'N')) \t 0.128 \t ('E', ('R', 'R', 'N', 'R', 'N')) \t 0.781\n",
      "query  35122 : \t ('P', ('HR', 'R', 'HR', 'N', 'HR')) \t 0.902 \t ('E', ('N', 'HR', 'N', 'HR', 'R')) \t 0.197\n",
      "query  15610 : \t ('P', ('N', 'R', 'HR', 'N', 'HR')) \t 0.257 \t ('E', ('HR', 'HR', 'HR', 'R', 'N')) \t 0.960\n",
      "query  10486 : \t ('P', ('N', 'HR', 'HR', 'R', 'HR')) \t 0.332 \t ('E', ('HR', 'N', 'N', 'N', 'N')) \t 0.457\n",
      "query  55175 : \t ('P', ('R', 'R', 'HR', 'N', 'R')) \t 0.902 \t ('E', ('R', 'R', 'R', 'R', 'R')) \t 1.000\n",
      "query  30044 : \t ('P', ('HR', 'HR', 'HR', 'R', 'HR')) \t 1.000 \t ('E', ('N', 'HR', 'HR', 'N', 'N')) \t 0.233\n"
     ]
    }
   ],
   "source": [
    "k_max = 5\n",
    "avg_prec = average_prec(k_max,ranking_pairs)\n",
    "\n",
    "random_query_index_list = random.sample(range(len(ranking_pairs)), 10)\n",
    "for i in random_query_index_list:\n",
    "    print(\"query \",i,\": \\t\",ranking_pairs[i][0], \"\\t\",'%.3f'%avg_prec[i][0],'\\t',ranking_pairs[i][1],'\\t','%.3f'%avg_prec[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "explanation/analysis?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------calc ideal dcg based on ground truth [10 HR, 10 R, 10 N]\n",
    "def calc_ideal_dgc(k):\n",
    "    idcg_rel = [2]*10 + [1]*10 + [0]*10\n",
    "    idcg = 0\n",
    "    for index in range(min(k, len(idcg_rel))):\n",
    "        idcg += ((2**idcg_rel[index]) - 1)/(math.log2(2+index))\n",
    "    return idcg\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I used the second formula of DCG from slide 6 of http://www.cs.cornell.edu/courses/cs4300/2013fa/lectures/evaluation-1-4pp.pdf\n",
    "#nDCG is a way to calculate this measure across many independent ____queries____(http://curtis.ml.cmu.edu/w/courses/index.php/Normalized_discounted_cumulative_gain)\n",
    "#Normalize DCG at rank n by the DCG value at rank n of the ideal ranking(stanford slide)\n",
    "\n",
    "import math\n",
    "def ndcg(max_k,query_index,method):    \n",
    "    all_dcg_k = []\n",
    "    \n",
    "    #--------------------make dcg list for all k-----------------------------\n",
    "    for r in range(0,max_k):\n",
    "        if ranking_pairs[query_index][method][1][r] == 'HR':\n",
    "            rel_num = 2\n",
    "        elif ranking_pairs[query_index][method][1][r] == 'R':\n",
    "            rel_num = 1\n",
    "        else:\n",
    "            rel_num = 0\n",
    "            \n",
    "        if len(all_dcg_k) > 0:   \n",
    "            all_dcg_k.append((((2**rel_num) - 1)/(math.log2(2+r))) + all_dcg_k[-1])\n",
    "        else:\n",
    "            all_dcg_k.append(((2**rel_num) - 1)/(math.log2(2+r)))\n",
    "            \n",
    "            \n",
    "    #----------------calculate ideal dcg------------------------------------      \n",
    "    idcg = calc_ideal_dgc(max_k)\n",
    "        \n",
    "    #--------------------------convert dcg to ndcg--------------------------   \n",
    "    if(idcg == 0):\n",
    "        all_dcg_k[:] = [0 for x in all_dcg_k]\n",
    "    else:\n",
    "        all_dcg_k[:] = [x / idcg for x in all_dcg_k]\n",
    "     \n",
    "    return all_dcg_k\n",
    "\n",
    "#-----------------------call ndcg function for all 59000 queries------------------\n",
    "ndcg_list = []\n",
    "max_k = 5\n",
    "for i in range(len(ranking_pairs)):\n",
    "    p_dcg = ndcg(max_k,i,0)\n",
    "    e_dcg = ndcg(max_k,i,1)\n",
    "    ndcg_list.append((p_dcg[-1],e_dcg[-1]))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 0.13120507751234178),\n",
       " (0.0, 0.04373502583744726),\n",
       " (0.0, 0.14606834984270645),\n",
       " (0.0, 0.27727342735504823),\n",
       " (0.0, 0.1898033756801537),\n",
       " (0.0, 0.04868944994756881),\n",
       " (0.0, 0.17989452745991058),\n",
       " (0.0, 0.09242447578501607),\n",
       " (0.0, 0.16958010263680803),\n",
       " (0.0, 0.30078518014914984)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query 33980 :\n",
      "('P', ('HR', 'R', 'N', 'HR', 'R')) \t 0.600292335865279\n",
      "('E', ('HR', 'N', 'R', 'N', 'HR')) \t 0.5268919836648939 \n",
      "\n",
      "query 56662 :\n",
      "('P', ('R', 'R', 'R', 'N', 'N')) \t 0.24090885754831726\n",
      "('E', ('N', 'HR', 'N', 'R', 'HR')) \t 0.3938807921944381 \n",
      "\n",
      "query 34987 :\n",
      "('P', ('HR', 'R', 'HR', 'N', 'N')) \t 0.5800690628219334\n",
      "('E', ('HR', 'R', 'N', 'HR', 'HR')) \t 0.6877623875401735 \n",
      "\n",
      "query 43836 :\n",
      "('P', ('R', 'N', 'R', 'N', 'HR')) \t 0.30078518014914984\n",
      "('E', ('N', 'HR', 'N', 'R', 'HR')) \t 0.3938807921944381 \n",
      "\n",
      "query 1377 :\n",
      "('P', ('N', 'N', 'N', 'HR', 'R')) \t 0.1898033756801537\n",
      "('E', ('R', 'N', 'N', 'R', 'N')) \t 0.16174285170544084 \n",
      "\n",
      "query 23929 :\n",
      "('P', ('HR', 'N', 'HR', 'R', 'R')) \t 0.6011647836954401\n",
      "('E', ('R', 'HR', 'R', 'R', 'HR')) \t 0.5634608948312462 \n",
      "\n",
      "query 38353 :\n",
      "('P', ('HR', 'R', 'R', 'HR', 'R')) \t 0.656819036744215\n",
      "('E', ('HR', 'HR', 'HR', 'N', 'N')) \t 0.7227265726449517 \n",
      "\n",
      "query 51474 :\n",
      "('P', ('R', 'HR', 'R', 'HR', 'R')) \t 0.5733697430514892\n",
      "('E', ('R', 'N', 'N', 'R', 'R')) \t 0.2054778775428881 \n",
      "\n",
      "query 1024 :\n",
      "('P', ('N', 'N', 'N', 'HR', 'HR')) \t 0.27727342735504823\n",
      "('E', ('N', 'R', 'N', 'HR', 'N')) \t 0.21739710475421564 \n",
      "\n",
      "query 41566 :\n",
      "('P', ('R', 'N', 'HR', 'N', 'N')) \t 0.28263350439468005\n",
      "('E', ('R', 'N', 'R', 'HR', 'R')) \t 0.35938347831696177 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_query_index_list = random.sample(range(len(ranking_pairs)), 10)\n",
    "for index in random_query_index_list:\n",
    "    print(\"query %d\"%index,\":\")\n",
    "    print(ranking_pairs[index][0], \"\\t\", ndcg_list[index][0])\n",
    "    print(ranking_pairs[index][1], \"\\t\",ndcg_list[index][1],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of all ERR values for P & E (should be equal):\n",
      " [ 0.55613292  0.55613292]\n",
      "mean of a random sample of 100 pairs:\n",
      " [ 0.54105143  0.57543164]\n",
      "[(0.0, 0.15),\n",
      " (0.0, 0.1),\n",
      " (0.0, 0.1875),\n",
      " (0.0, 0.225),\n",
      " (0.0, 0.2),\n",
      " (0.0, 0.125),\n",
      " (0.0, 0.175),\n",
      " (0.0, 0.175),\n",
      " (0.0, 0.25),\n",
      " (0.0, 0.2875)]\n"
     ]
    }
   ],
   "source": [
    "# ERR\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def numerical(relevance_sequence):\n",
    "    \"\"\"\n",
    "    Convert a relevance grade sequence to a sequence of numerical \n",
    "    values, based on the relevance grades given.\n",
    "    E.g. ['HR', 'HR', 'HR', 'R', 'N'] returns [2, 2, 2, 1, 0]\n",
    "    \"\"\"\n",
    "    numerical_relevance_sequence = [1 if grade == 'R' \\\n",
    "                                    else 2 if grade == 'HR' else 0 \\\n",
    "                                    for grade in relevance_sequence]\n",
    "    return numerical_relevance_sequence\n",
    "\n",
    "def R_function(g, g_max):\n",
    "    return (2**g - 1)/(2**g_max)\n",
    "\n",
    "def ERR(relevance_sequence):\n",
    "    \"\"\"\n",
    "    Compute the ERR based on Algorithm 2 in \n",
    "    https://pdfs.semanticscholar.org/7e3c/f6492128f915112ca01dcb77c766129e65cb.pdf\n",
    "    \"\"\"\n",
    "    p = 1\n",
    "    ERR = 0\n",
    "    n = len(relevance_sequence)\n",
    "    g_max = max(relevance_sequence)\n",
    "    \n",
    "    for r in range(1, n + 1):\n",
    "        g = relevance_sequence[r - 1]\n",
    "        R = R_function(g, g_max)\n",
    "        ERR = ERR + p * (R/r)\n",
    "        p = p * (1 - R)\n",
    "    return ERR\n",
    "\n",
    "ERR_list = []\n",
    "for P, E in ranking_pairs:\n",
    "    ERR_P = ERR(numerical(P[1]))\n",
    "    ERR_E = ERR(numerical(E[1]))\n",
    "    ERR_list.append((ERR_P, ERR_E))\n",
    "    \n",
    "print('mean of all ERR values for P & E (should be equal):\\n', \n",
    "      np.mean(ERR_list, axis=0))\n",
    "print('mean of a random sample of 100 pairs:\\n', \n",
    "      np.mean(random.sample(ERR_list, 100), axis=0))\n",
    "pprint(ERR_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: <font color='darkred'>Calculate the ùõ•measure *(0 points)*</font>\n",
    "For the three measures and all *P* and *E* ranking pairs constructed above calculate the difference: ùõ•measure = measure<sub>E</sub>-measure<sub>P</sub>. Consider only those pairs for which *E* outperforms *P*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def measure_diff(measure_list, _ranking_pairs, query_index_list):\n",
    "    for i in query_index_list:\n",
    "        diff = measure_list[i][1] - measure_list[i][0]\n",
    "        if diff > 0:\n",
    "            print(\"query\",i,\": \",'\\t',_ranking_pairs[i][0],'\\t',_ranking_pairs[i][1],'\\t','%.3f'%(diff))\n",
    "        else:\n",
    "            print(\"query\",i,\": \",'\\t',\"less than 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ùõ•measure for AP: \n",
      "query 17386 :  \t ('P', ('N', 'R', 'HR', 'R', 'R')) \t ('E', ('R', 'HR', 'HR', 'R', 'HR')) \t 0.668\n",
      "query 58063 :  \t less than 0\n",
      "query 46713 :  \t less than 0\n",
      "query 10805 :  \t ('P', ('N', 'HR', 'HR', 'R', 'R')) \t ('E', ('HR', 'R', 'R', 'HR', 'R')) \t 0.668\n",
      "query 44267 :  \t ('P', ('R', 'N', 'R', 'N', 'R')) \t ('E', ('R', 'R', 'N', 'R', 'R')) \t 0.228\n",
      "query 42874 :  \t less than 0\n",
      "query 3236 :  \t ('P', ('N', 'N', 'HR', 'HR', 'HR')) \t ('E', ('HR', 'N', 'HR', 'N', 'HR')) \t 0.464\n",
      "query 8651 :  \t ('P', ('N', 'HR', 'N', 'R', 'R')) \t ('E', ('R', 'N', 'R', 'N', 'R')) \t 0.388\n",
      "query 3633 :  \t less than 0\n",
      "query 57932 :  \t less than 0\n",
      "-------------------------------------------------------------------------------------------------------\n",
      " ùõ•measure for nDCG: \n",
      "query 17386 :  \t ('P', ('N', 'R', 'HR', 'R', 'R')) \t ('E', ('R', 'HR', 'HR', 'R', 'HR')) \t 0.343\n",
      "query 58063 :  \t less than 0\n",
      "query 46713 :  \t less than 0\n",
      "query 10805 :  \t ('P', ('N', 'HR', 'HR', 'R', 'R')) \t ('E', ('HR', 'R', 'R', 'HR', 'R')) \t 0.181\n",
      "query 44267 :  \t ('P', ('R', 'N', 'R', 'N', 'R')) \t ('E', ('R', 'R', 'N', 'R', 'R')) \t 0.063\n",
      "query 42874 :  \t ('P', ('R', 'N', 'HR', 'R', 'N')) \t ('E', ('N', 'HR', 'HR', 'HR', 'HR')) \t 0.330\n",
      "query 3236 :  \t ('P', ('N', 'N', 'HR', 'HR', 'HR')) \t ('E', ('HR', 'N', 'HR', 'N', 'HR')) \t 0.193\n",
      "query 8651 :  \t less than 0\n",
      "query 3633 :  \t less than 0\n",
      "query 57932 :  \t ('P', ('R', 'R', 'R', 'HR', 'R')) \t ('E', ('HR', 'N', 'HR', 'HR', 'HR')) \t 0.355\n",
      "-------------------------------------------------------------------------------------------------------\n",
      " ùõ•measure for ERR: \n",
      "query 17386 :  \t ('P', ('N', 'R', 'HR', 'R', 'R')) \t ('E', ('R', 'HR', 'HR', 'R', 'HR')) \t 0.255\n",
      "query 58063 :  \t ('P', ('R', 'R', 'R', 'HR', 'R')) \t ('E', ('R', 'R', 'HR', 'N', 'N')) \t 0.009\n",
      "query 46713 :  \t less than 0\n",
      "query 10805 :  \t ('P', ('N', 'HR', 'HR', 'R', 'R')) \t ('E', ('HR', 'R', 'R', 'HR', 'R')) \t 0.381\n",
      "query 44267 :  \t ('P', ('R', 'N', 'R', 'N', 'R')) \t ('E', ('R', 'R', 'N', 'R', 'R')) \t 0.060\n",
      "query 42874 :  \t ('P', ('R', 'N', 'HR', 'R', 'N')) \t ('E', ('N', 'HR', 'HR', 'HR', 'HR')) \t 0.002\n",
      "query 3236 :  \t ('P', ('N', 'N', 'HR', 'HR', 'HR')) \t ('E', ('HR', 'N', 'HR', 'N', 'HR')) \t 0.516\n",
      "query 8651 :  \t ('P', ('N', 'HR', 'N', 'R', 'R')) \t ('E', ('R', 'N', 'R', 'N', 'R')) \t 0.208\n",
      "query 3633 :  \t less than 0\n",
      "query 57932 :  \t ('P', ('R', 'R', 'R', 'HR', 'R')) \t ('E', ('HR', 'N', 'HR', 'HR', 'HR')) \t 0.352\n"
     ]
    }
   ],
   "source": [
    "random_query_index_list = random.sample(range(len(ranking_pairs)), 10)\n",
    "\n",
    "#for ap:\n",
    "print(\" ùõ•measure for AP: \")\n",
    "measure_diff(avg_prec, ranking_pairs, random_query_index_list)\n",
    "print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "#for nDCG:\n",
    "print(\" ùõ•measure for nDCG: \")\n",
    "measure_diff(ndcg_list, ranking_pairs, random_query_index_list)\n",
    "print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "#for ERR:\n",
    "print(\" ùõ•measure for ERR: \")\n",
    "measure_diff(ERR_list, ranking_pairs, random_query_index_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24462 27962 29376 29374\n"
     ]
    }
   ],
   "source": [
    "# code\n",
    "diff_precision = [precision_E - precision_P for precision_P, precision_E in prec if precision_E > precision_P]\n",
    "diff_avgprec = [avgprec_E - avgprec_P for avgprec_P, avgprec_E in avg_prec if avgprec_E > avgprec_P]\n",
    "diff_nDCG = [nDCG_E - nDCG_P for nDCG_P, nDCG_E in ndcg_list if nDCG_E > nDCG_P]\n",
    "diff_ERR = [ERR_E - ERR_P for ERR_P, ERR_E in ERR_list if ERR_E > ERR_P]\n",
    "print(len(diff_precision), len(diff_avgprec), len(diff_nDCG), len(diff_ERR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "explanation/analysis?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='purple'>[Based on Lecture 2]</font>\n",
    "### Step 4: <font color='darkred'>Implement Interleaving *(15 points)*</font>\n",
    "Implement 2 interleaving algorithms: (1) Team-Draft Interleaving OR Balanced Interleaving, AND (2), Probabilistic Interleaving. The interleaving algorithms should (a) given two rankings of relevance interleave them into a single ranking, and (b) given the users clicks on the interleaved ranking assign credit to the algorithms that produced the rankings.\n",
    "\n",
    "(Note 4: Note here that as opposed to a normal interleaving experiment where rankings consists of urls or docids, in our case the rankings consist of relevance labels. Hence in this case (a) you will assume that E and P return different documents, (b) the interleaved ranking will also be a ranking of labels.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Team-Draft Interleaving\n",
    "#P and E as two interleaving lists\n",
    "#as output we will have \n",
    "\n",
    "def team_draft_interleaving(_ranking_pairs):\n",
    "    all_I = []\n",
    "    for i in range(len(_ranking_pairs)):\n",
    "        A = _ranking_pairs[i][0][1]\n",
    "        B = _ranking_pairs[i][1][1]\n",
    "        team_A = []\n",
    "        team_B = []\n",
    "        I = []\n",
    "        while len(team_A) < len(A) or len(team_B) < len(B):\n",
    "            RandBit = random.getrandbits(1)\n",
    "            #pick from A\n",
    "            if len(team_A) < len(team_B) or (len(team_A) == len(team_B) and RandBit == 1):\n",
    "                I.append((A[len(team_A)],0))\n",
    "                team_A.append((I[-1],0))        \n",
    "            else:\n",
    "                #pick from B\n",
    "                I.append((B[len(team_B)],1))\n",
    "                team_B.append(I[-1])\n",
    "        all_I.append(I)\n",
    "    return all_I\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_winner(_click,_all_I):\n",
    "    winner_list = []\n",
    "    for query_index in range(len(_all_I)):\n",
    "        h_a = 0\n",
    "        h_b = 0\n",
    "        for index, item in enumerate(_all_I[query_index]):\n",
    "            if _click[query_index][index] > 0:\n",
    "                if item[1] == 0:\n",
    "                    h_a += 1\n",
    "                if item[1] == 1:\n",
    "                    h_b += 1\n",
    "        if(h_a > h_b):\n",
    "            winner_list.append('P')\n",
    "        if(h_a == h_b):\n",
    "            winner_list.append('Equal')\n",
    "        else:\n",
    "            winner_list.append('E')\n",
    "    return winner_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "explanation/analysis?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='purple'>[Based on Lecture 3]</font>\n",
    "### Step 5: <font color='darkred'>Implement User Clicks Simulation *(15 points)*</font>\n",
    "Having interleaved all the ranking pairs an online experiment could be ran. However, given that we do not have any users (and the entire homework is a big simulation) we will simulate user clicks.\n",
    "\n",
    "We have considered a number of click models including:\n",
    "1. Random Click Model (RCM)\n",
    "2. Position-Based Model (PBM)\n",
    "3. Simple Dependent Click Model (SDCM)\n",
    "4. Simple Dynamic Bayesian Network (SDBN)\n",
    "\n",
    "Consider two different click models, (a) the Random Click Model (RCM), and (b) one out of the remaining 3 aforementioned models. The parameters of some of these models can be estimated using the Maximum Likelihood Estimation (MLE) method, while others require using the Expectation-Maximization (EM) method. Implement the two models so that (a) there is a method that learns the parameters of the model given a set of training data, (b) there is a method that predicts the click probability given a ranked list of relevance labels, (c) there is a method that decides - stochastically - whether a document is clicked based on these probabilities.\n",
    "\n",
    "Having implemented the two click models, estimate the model parameters using the Yandex Click Log [[file]](https://drive.google.com/file/d/1tqMptjHvAisN1CJ35oCEZ9_lb0cEJwV0/view).\n",
    "\n",
    "(Note 6: Do not learn the attractiveness parameter *a*<sub>uq</sub>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_I = team_draft_interleaving(ranking_pairs)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn_rcm_param(_train_file):\n",
    "    c = 0\n",
    "    query_num = 0.0\n",
    "    click_num = 0.0\n",
    "    for line in _train_file:\n",
    "        info = line.split()\n",
    "        #print(info[2] == 'Q')\n",
    "        if info[2] == 'Q':\n",
    "            query_num += 1\n",
    "        if info[2] == 'C':\n",
    "            click_num += 1\n",
    "\n",
    "    doc_num = query_num*10\n",
    "    _rcm_param = click_num/doc_num\n",
    "    return _rcm_param\n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#SDBM:\n",
    "# I have to change the formula for finiding the number of last click-------------------!!!!!!\n",
    "\n",
    "def learn_sdbm_param(_train_file):\n",
    "    last_session = '0'\n",
    "    satisfy_num = 0.0\n",
    "    last_action = ''\n",
    "    click_num = 0.0\n",
    "    \n",
    "    for line in _train_file:\n",
    "        info = line.split()\n",
    "        \n",
    "        if info[2] == 'C':\n",
    "            click_num += 1\n",
    "        \n",
    "\n",
    "        if info[0] == last_session:\n",
    "            last_action = info[2]\n",
    "        else:\n",
    "            if last_action == 'C':                \n",
    "                satisfy_num += 1\n",
    "            last_session = info[0]\n",
    "    if last_action == 'C':                \n",
    "        satisfy_num += 1     \n",
    "\n",
    "        \n",
    "    sigma = satisfy_num/click_num  \n",
    "\n",
    "    return sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SDBM:\n",
    "# I have to change the formula for finiding the number of last click-------------------!!!!!!\n",
    "\n",
    "def learn_sdbm_param(_train_file):\n",
    "    for line in _train_file:\n",
    "        info = line.split()\n",
    "    \n",
    "\n",
    "    return sigma\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def sdbm_predict_click_prob(_I, _sigma):\n",
    "    prediction = []\n",
    "    attraction = {'HR':0.9,'R':0.6,'N':0.1}\n",
    "    for index, item in enumerate(_I):\n",
    "        if index == 0:\n",
    "            prediction.append(attraction[item[0]])\n",
    "        else:\n",
    "            #er = prediction[index-1]*(attraction[_I[index-1]]*(1-_sigma) + (1 - attraction[_I[index-1]]))            \n",
    "            er = prediction[index-1]*(1-_sigma * attraction[_I[index-1][0]])\n",
    "            prediction.append(attraction[item[0]] * er)\n",
    "                                       \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "explanation/analysis?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: <font color='darkred'>Simulate Interleaving Experiment *(10 points)*</font>\n",
    "Having implemented the click models, it is time to run the simulated experiment.\n",
    "\n",
    "For each of interleaved ranking run N simulations for each one of the click models implemented and measure the proportion *p* of wins for E.\n",
    "(Note 7: Some of the models above include an attractiveness parameter *a*<sub>uq</sub>. Use the relevance label to assign this parameter by setting *a*<sub>uq</sub> for a document u in the ranked list accordingly. (See [Click Models for Web Search](http://clickmodels.weebly.com/uploads/5/2/2/5/52257029/mc2015-clickmodels.pdf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate_rcm_click(_I,_ru):\n",
    "    simulated_click = []\n",
    "    for i in range(len(_I)):\n",
    "        if(random.random() < _ru):\n",
    "            simulated_click.append(1)\n",
    "        else:\n",
    "            simulated_click.append(0)\n",
    "    return simulated_click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13445559411047547\n",
      "[[('N', 1), ('N', 0), ('N', 0), ('N', 1), ('N', 1), ('N', 0), ('N', 0), ('N', 1), ('N', 0), ('HR', 1)], [('N', 1), ('N', 0), ('N', 1), ('N', 0), ('N', 1), ('N', 0), ('N', 1), ('N', 0), ('R', 1), ('N', 0)], [('N', 1), ('N', 0), ('N', 1), ('N', 0), ('N', 1), ('N', 0), ('N', 0), ('HR', 1), ('N', 1), ('N', 0)], [('N', 0), ('N', 1), ('N', 0), ('N', 1), ('N', 1), ('N', 0), ('N', 0), ('HR', 1), ('N', 0), ('HR', 1)], [('N', 0), ('N', 1), ('N', 1), ('N', 0), ('N', 0), ('N', 1), ('N', 0), ('HR', 1), ('R', 1), ('N', 0)], [('N', 0), ('N', 1), ('N', 1), ('N', 0), ('N', 1), ('N', 0), ('R', 1), ('N', 0), ('N', 1), ('N', 0)], [('N', 1), ('N', 0), ('N', 0), ('N', 1), ('N', 0), ('N', 1), ('R', 1), ('N', 0), ('N', 0), ('HR', 1)], [('N', 1), ('N', 0), ('N', 1), ('N', 0), ('N', 1), ('N', 0), ('R', 1), ('N', 0), ('R', 1), ('N', 0)], [('N', 1), ('N', 0), ('N', 0), ('N', 1), ('N', 0), ('HR', 1), ('N', 1), ('N', 0), ('N', 0), ('N', 1)], [('N', 0), ('N', 1), ('N', 1), ('N', 0), ('HR', 1), ('N', 0), ('N', 0), ('N', 1), ('HR', 1), ('N', 0)]]\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      " [1, 0, 0, 0, 1, 0, 0, 1, 1, 0],\n",
      " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],\n",
      " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [1, 0, 1, 1, 0, 0, 0, 0, 0, 0]]\n",
      "['P',\n",
      " 'E',\n",
      " 'Equal',\n",
      " 'Equal',\n",
      " 'Equal',\n",
      " 'P',\n",
      " 'E',\n",
      " 'E',\n",
      " 'P',\n",
      " 'E',\n",
      " 'P',\n",
      " 'E',\n",
      " 'Equal',\n",
      " 'P',\n",
      " 'E',\n",
      " 'P',\n",
      " 'E',\n",
      " 'P',\n",
      " 'E',\n",
      " 'Equal']\n"
     ]
    }
   ],
   "source": [
    "train_file = open(\"YandexRelPredChallenge.txt\",\"r\")\n",
    "ru = learn_rcm_param(train_file)\n",
    "print(ru)\n",
    "\n",
    "rcm_click_list = [simulate_rcm_click(all_I[i],ru) for i in range(len(all_I))]\n",
    "\n",
    "print(all_I[:10])\n",
    "pprint(rcm_click_list[:10])\n",
    "\n",
    "winner_list = find_winner(rcm_click_list, all_I)\n",
    "pprint(winner_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate_sdbm_click(_I,_sigma):\n",
    "    simulated_click = [0]*len(_I)\n",
    "    attraction = {'HR':0.9,'R':0.6,'N':0.1}\n",
    "    for index,item in enumerate(_I):\n",
    "        if random.random() < attraction[item[0]]:\n",
    "            simulated_click[index]= 1\n",
    "            if random.random() < _sigma:\n",
    "                return simulated_click\n",
    "    return simulated_click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('N', 1), ('N', 0), ('N', 0), ('N', 1), ('N', 1), ('N', 0), ('N', 0), ('N', 1), ('N', 0), ('HR', 1)], [('N', 1), ('N', 0), ('N', 1), ('N', 0), ('N', 1), ('N', 0), ('N', 1), ('N', 0), ('R', 1), ('N', 0)], [('N', 1), ('N', 0), ('N', 1), ('N', 0), ('N', 1), ('N', 0), ('N', 0), ('HR', 1), ('N', 1), ('N', 0)], [('N', 0), ('N', 1), ('N', 0), ('N', 1), ('N', 1), ('N', 0), ('N', 0), ('HR', 1), ('N', 0), ('HR', 1)], [('N', 0), ('N', 1), ('N', 1), ('N', 0), ('N', 0), ('N', 1), ('N', 0), ('HR', 1), ('R', 1), ('N', 0)], [('N', 0), ('N', 1), ('N', 1), ('N', 0), ('N', 1), ('N', 0), ('R', 1), ('N', 0), ('N', 1), ('N', 0)], [('N', 1), ('N', 0), ('N', 0), ('N', 1), ('N', 0), ('N', 1), ('R', 1), ('N', 0), ('N', 0), ('HR', 1)], [('N', 1), ('N', 0), ('N', 1), ('N', 0), ('N', 1), ('N', 0), ('R', 1), ('N', 0), ('R', 1), ('N', 0)], [('N', 1), ('N', 0), ('N', 0), ('N', 1), ('N', 0), ('HR', 1), ('N', 1), ('N', 0), ('N', 0), ('N', 1)], [('N', 0), ('N', 1), ('N', 1), ('N', 0), ('HR', 1), ('N', 0), ('N', 0), ('N', 1), ('HR', 1), ('N', 0)]]\n",
      "[[0, 1, 0, 1, 0, 0, 0, 0, 1, 1],\n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      " [0, 0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
      " [0, 0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
      " [0, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
      " [0, 0, 0, 0, 0, 0, 1, 0, 0, 1],\n",
      " [0, 0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
      " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " [0, 0, 0, 0, 1, 0, 0, 0, 1, 0]]\n",
      "['Equal',\n",
      " 'Equal',\n",
      " 'Equal',\n",
      " 'E',\n",
      " 'E',\n",
      " 'E',\n",
      " 'E',\n",
      " 'P',\n",
      " 'E',\n",
      " 'P',\n",
      " 'E',\n",
      " 'E',\n",
      " 'E',\n",
      " 'E',\n",
      " 'P',\n",
      " 'E',\n",
      " 'P',\n",
      " 'E',\n",
      " 'E',\n",
      " 'E']\n"
     ]
    }
   ],
   "source": [
    "train_file = open(\"YandexRelPredChallenge.txt\",\"r\")\n",
    "sigma = learn_sdbm_param(train_file)\n",
    "\n",
    "sdbm_click_list = [simulate_sdbm_click(all_I[i],sigma) for i in range(len(all_I))]\n",
    "\n",
    "print(all_I[:10])\n",
    "pprint(sdbm_click_list[:10])\n",
    "\n",
    "winner_list = find_winner(sdbm_click_list, all_I)\n",
    "pprint(winner_list[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "explanation/analysis?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: <font color='darkred'>Results and Analysis *(30 points)*</font>\n",
    "Compare the results of the offline experiments (i.e. the values of the ùõ•measure) with the results of the online experiment (i.e. proportion of wins), analyze them and reach your conclusions regarding their agreement.\n",
    "* Use easy to read and comprehend visuals to demonstrate the results;\n",
    "* Analyze the results on the basis of\n",
    "    * the evaluation measure used,\n",
    "    * the interleaving method used,\n",
    "    * the click model used.\n",
    "* Report and ground your conclusions.\n",
    "\n",
    "(Note 8: This is the place where you need to demonstrate your deeper understanding of what you have implemented so far; hence the large number of points assigned. Make sure you clearly do that so that the examiner of your work can grade it accordingly.)\n",
    "\n",
    "<u>Yandex Click Log File</u>:\n",
    "\n",
    "The dataset includes user sessions extracted from Yandex logs, with queries, URL rankings and clicks. To allay privacy concerns the user data is fully anonymized. So, only meaningless numeric IDs of queries, sessions, and URLs are released. The queries are grouped only by sessions and no user IDs are provided. The dataset consists of several parts. Logs represent a set of rows, where each row represents one of the possible user actions: query or click.\n",
    "\n",
    "In the case of a Query:\n",
    "\n",
    "    SessionID TimePassed TypeOfAction QueryID RegionID ListOfURLs\n",
    "\n",
    "\n",
    "In the case of a Click:\n",
    "\n",
    "    SessionID TimePassed TypeOfAction URLID\n",
    "\n",
    "\n",
    "* `SessionID` - the unique identifier of the user session.\n",
    "* `TimePassed` - the time elapsed since the beginning of the current session in standard time units.\n",
    "* `TypeOfAction` - type of user action. This may be either a query (Q), or a click (C).\n",
    "* `QueryID` - the unique identifier of the request.\n",
    "* `RegionID` - the unique identifier of the country from which a given query. This identifier may take four values.\n",
    "* `URLID` - the unique identifier of the document.\n",
    "* `ListOfURLs` - the list of documents from left to right as they have been shown to users on the page extradition Yandex (top to bottom).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision vs. average precision\n",
      "0.440397350993 0.385633681425 \t Ttest_indResult(statistic=24.810331399370998, pvalue=4.1906255886023189e-135) \n",
      "\n",
      "precision vs. nDCG\n",
      "0.440397350993 0.227499954393 \t Ttest_indResult(statistic=120.28697419447488, pvalue=0.0) \n",
      "\n",
      "precision vs. ERR\n",
      "0.440397350993 0.252210726855 \t Ttest_indResult(statistic=102.07668528704501, pvalue=0.0) \n",
      "\n",
      "average precision vs. nDCG\n",
      "0.385633681425 0.227499954393 \t Ttest_indResult(statistic=87.264614427180462, pvalue=0.0) \n",
      "\n",
      "average precision vs. ERR\n",
      "0.385633681425 0.252210726855 \t Ttest_indResult(statistic=71.148169032468587, pvalue=0.0) \n",
      "\n",
      "nDCG vs. ERR\n",
      "0.227499954393 0.252210726855 \t Ttest_indResult(statistic=-17.076224869103331, pvalue=3.2076576398410232e-65) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "np.random.seed(123)\n",
    "\n",
    "diff_measure_lists = {'precision': diff_precision, 'average precision': diff_avgprec, \n",
    "                      'nDCG': diff_nDCG, 'ERR': diff_ERR}\n",
    "\n",
    "for measure_1, measure_2 in combinations(diff_measure_lists.keys(), 2):\n",
    "    print(measure_1, 'vs.', measure_2)\n",
    "    measure_1 = diff_measure_lists[measure_1]\n",
    "    measure_2 = diff_measure_lists[measure_2]\n",
    "    print(np.mean(measure_1), np.mean(measure_2), '\\t', stats.ttest_ind(measure_1, measure_2), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "explanation/analysis?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
