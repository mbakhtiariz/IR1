{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework is made by:\n",
    "* Masoumeh Bakhtiariziabari (11813105)\n",
    "* Marianne de Heer Kloots (11138351)\n",
    "* Tharangni Sivaji (11611065)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Part [15 pts]\n",
    "\n",
    "## 1. Hypothesis Testing ‚Äì The problem of multiple comparisons [5 points]\n",
    "Experimentation in AI often happens like this: \n",
    "1. Modify/Build an algorithm\n",
    "2. Compare the algorithm to a baseline by running a hypothesis test.\n",
    "3. If not significant, go back to step A\n",
    "4. If significant, start writing a paper. \n",
    "\n",
    "How many hypothesis tests, m, does it take to get to (with Type I error for each test = Œ±):\n",
    "* P(m<sup>th</sup> experiment gives significant result | m experiments lacking power to reject H<sub>0</sub>)?\n",
    "* P(at least one significant result | m experiments lacking power to reject H<sub>0</sub>)?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "answer"
    ]
   },
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "<ol>\n",
    "<li><ul>\n",
    "        <li>P(m<sup>th</sup> experiment gives significant result | m experiments lacking power to reject H<sub>0</sub>)\n",
    "        <li>P(m<sup>th</sup> experiment gives significant result | H<sub>0</sub> is true in all m experiments)\n",
    "        <li>i.e. only the m<sup>th</sup> result is significant whereas (m-1) results are not significant\n",
    "        <li> <b>((1-Œ±)<sup>m-1</sup>)Œ± </b>\n",
    "    </ul>\n",
    "<li><ul>\n",
    "        <li>P(at least one significant result | m experiments lacking power to reject H<sub>0</sub>)\n",
    "        <li>1 - P(no significant result)\n",
    "        <li><b>1 - (1-Œ±)<sup>m</sup></b>\n",
    "    </ul>\n",
    "</ol>\n",
    "</div>\n",
    "*verify?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bias and unfairness in Interleaving experiments [10 points]\n",
    "Balance interleaving has been shown to be biased in a number of corner cases. An example was given during the lecture with two ranked lists of length 3 being interleaved, and a randomly clicking population of users that resulted in algorithm A winning ‚Öî of the time, even though in theory the percentage of wins should be 50% for both algorithms. Can you come up with a situation of two ranked lists of length 3 and a distribution of clicks over them for which Team-draft interleaving is unfair to the better algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "answer"
    ]
   },
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "*answer here*\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Part [85 pts]\n",
    "Commercial search engines use both offline and online approach in evaluating a new search algorithm: they first use an offline test collection to compare the production algorithm (P) with the new experimental algorithm (E); if *E* statistically significantly outperforms *P* with respect to the evaluation measure of their interest, the two algorithms are then compared online through an interleaving experiment.\n",
    "\n",
    "For the purpose of this homework we will assume that the evaluation measures of interest are:\n",
    "1. Binary evaluation measures\n",
    "    1. Precision at rank k,\n",
    "    2. Recall at rank k,\n",
    "    3. Average Precision,\n",
    "2. Multi-graded evaluation measures\n",
    "    1. Normalized Discounted Cumulative Gain at rank k (nDCG@k),\n",
    "    2. Expected Reciprocal Rank (ERR).\n",
    "\n",
    "Further, for the purpose of this homework we will assume that the interleaving algorithms of interest are:\n",
    "Team-Draft Interleaving (Joachims. \"Evaluating retrieval performance using clickthrough data\". Text Mining 2003.),\n",
    "Probabilistic Interleaving (Hofmann, Whiteson, and de Rijke. \"A probabilistic method for inferring preferences from clicks.\" CIKM 2011.).\n",
    " \n",
    "In an interleaving experiment the ranked results of *P* and *E* (against a user query) are interleaved in a single ranked list which is presented to a user. The user then clicks on the results and the algorithm that receives most of the clicks wins the comparison. The experiment is repeated for a number of times (impressions) and the total wins for *P* and *E* are computed. \n",
    "\n",
    "A Sign/Binomial Test is then run to examine whether the difference in wins between the two algorithms is statistically significant (or due to chance). Alternatively one can calculate the proportion of times the *E* wins and test whether this proportion, *p*, is greater than *p<sub>0</sub>=*0.5. This is called an 1-sample 1-sided proportion test.\n",
    "\n",
    "One of the key questions however is **whether offline evaluation and online evaluation outcomes agree with each other**. In this homework you will determine the degree of agreement between offline evaluation measures and interleaving outcomes, by the means of simulations. A similar analysis using actual online data can be found at Chapelle et al. ‚ÄúLarge-Scale Validation and Analysis of Interleaved Search Evaluation‚Äù."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='purple'>[Based on Lecture 1]</font>\n",
    "### Step 1: <font color='darkred'>Simulate Rankings of Relevance for *E* and *P* *(5 points)*</font>\n",
    "\n",
    "In the first step you will generate pairs of rankings of relevance, for the production *P* and experimental *E*, respectively, for a hypothetical query **q**. Assume a 3-graded relevance, i.e. `{N, R, HR}`. Construct all possible *P* and *E* ranking pairs of length 5. This step should give you about.\n",
    "\n",
    "Example:\n",
    "\n",
    "    P: {N N N N N}\n",
    "    E: {N N N N R}\n",
    "    ‚Ä¶\n",
    "    P: {HR HR HR HR R}\n",
    "    E: {HR HR HR HR HR}\n",
    "\n",
    "(Note 1: If you do not have enough computational power, sample 5000 pair uniformly at random to show your work.)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO: add comments\n",
    "from itertools import product\n",
    "from pprint import pprint\n",
    "import random\n",
    "\n",
    "algorithms = ['P', 'E']\n",
    "relevance_grades = {'N', 'HR', 'R'}\n",
    "rankings = [ranking for ranking in product(relevance_grades, repeat=5)]\n",
    "algorithm_rankings = [product(alg, rankings) for alg in algorithms]\n",
    "\n",
    "ranking_pairs = [pair for pair in product(*algorithm_rankings)]\n",
    "print(len(ranking_pairs))\n",
    "#pprint(ranking_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of combinations: 59049\n",
      "number of non-equal combinations: 58806\n",
      "[(('P', ('R', 'R', 'N', 'HR', 'N')), ('E', ('R', 'R', 'R', 'R', 'N'))),\n",
      " (('P', ('HR', 'N', 'R', 'R', 'N')), ('E', ('N', 'R', 'N', 'N', 'N'))),\n",
      " (('P', ('N', 'N', 'N', 'HR', 'HR')), ('E', ('R', 'R', 'R', 'N', 'N'))),\n",
      " (('P', ('R', 'N', 'R', 'N', 'HR')), ('E', ('N', 'R', 'HR', 'R', 'HR'))),\n",
      " (('P', ('R', 'R', 'R', 'HR', 'N')), ('E', ('HR', 'R', 'HR', 'HR', 'N'))),\n",
      " (('P', ('R', 'N', 'HR', 'N', 'N')), ('E', ('HR', 'N', 'N', 'R', 'HR'))),\n",
      " (('P', ('R', 'HR', 'R', 'HR', 'N')), ('E', ('N', 'R', 'HR', 'HR', 'N'))),\n",
      " (('P', ('HR', 'HR', 'HR', 'R', 'R')), ('E', ('N', 'HR', 'R', 'N', 'HR'))),\n",
      " (('P', ('HR', 'HR', 'N', 'N', 'R')), ('E', ('HR', 'R', 'R', 'HR', 'N'))),\n",
      " (('P', ('N', 'R', 'R', 'HR', 'N')), ('E', ('R', 'N', 'N', 'R', 'R')))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from itertools import product\n",
    "from pprint import pprint\n",
    "import random\n",
    "\n",
    "# define collections of algorithms and relevance grades\n",
    "algorithms = ['P', 'E']\n",
    "relevance_grades = ['N', 'HR', 'R']\n",
    "\n",
    "# all possible ranking sequences\n",
    "# list of rankings [('HR', 'HR', 'HR', 'HR', 'HR') ... ('N', 'N', 'N', 'N', 'N')]\n",
    "rankings = [ranking for ranking in product(relevance_grades, repeat=5)]\n",
    "\n",
    "# all algorithms paired with all rankings \n",
    "# (list of lists with elements e.g. ('P', ('HR', 'HR', 'HR', 'HR', 'HR')))\n",
    "algorithm_rankings = [list(product(alg, rankings)) for alg in algorithms]\n",
    "\n",
    "# all possible pairs of P and E with their rankings\n",
    "all_ranking_pairs = [pair for pair in product(*algorithm_rankings)]\n",
    "\n",
    "# all ranking pairs except equals\n",
    "ranking_pairs = [pair for pair in product(*algorithm_rankings) if pair[0][1] != pair[1][1]]\n",
    "\n",
    "# pretty print\n",
    "print('number of combinations:', len(all_ranking_pairs))\n",
    "print('number of non-equal combinations:', len(ranking_pairs))\n",
    "pprint(random.sample(ranking_pairs, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "explanation/analysis?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: <font color='darkred'>Implement Evaluation Measures *(10 points)*</font>\n",
    "Implement 1 binary and 2 multi-graded evaluation measures out of the 7 measures mentioned above. \n",
    "\n",
    "(Note 2: Some of the aforementioned measures require the total number of relevant and highly relevant documents in the entire collection ‚Äì pay extra attention on how to find this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code\n",
    "#p@K function\n",
    "\n",
    "#import numpy as np\n",
    "def precision(k,_ranking_pairs):\n",
    "    \n",
    "    def calc_precision(i,x):\n",
    "        rel_counter = 0.0\n",
    "        prec = 0.0\n",
    "        for j in range(k):\n",
    "            if _ranking_pairs[i][x][1][j] == 'HR' or _ranking_pairs[i][x][1][j] == 'R':\n",
    "                rel_counter += 1\n",
    "                prec += rel_counter/(1.0+j)\n",
    "        return prec\n",
    "    \n",
    "    prec_list = []\n",
    "    for i in range(len(_ranking_pairs)):\n",
    "        p_prec = calc_precision(i,0)\n",
    "        e_prec = calc_precision(i,1)\n",
    "        prec_list.append((p_prec/k,e_prec/k))\n",
    "        #print(p_prec,e_prec)\n",
    "    \n",
    "    return prec_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query  41698 : \t (0.5555555555555555, 0.38888888888888884)\n",
      "query  54058 : \t (0.6666666666666666, 0.5555555555555555)\n",
      "query  50653 : \t (1.0, 0.38888888888888884)\n",
      "query  5748 : \t (0.1111111111111111, 0.5555555555555555)\n",
      "query  55812 : \t (1.0, 1.0)\n",
      "query  20037 : \t (0.3333333333333333, 0.6666666666666666)\n",
      "query  36948 : \t (1.0, 0.3333333333333333)\n",
      "query  36638 : \t (1.0, 0.5555555555555555)\n",
      "query  5193 : \t (0.1111111111111111, 0.6666666666666666)\n",
      "query  17226 : \t (0.38888888888888884, 0.38888888888888884)\n"
     ]
    }
   ],
   "source": [
    "prec = precision(3,ranking_pairs)\n",
    "\n",
    "random_query_index_list = random.sample(range(len(ranking_pairs)), 10)\n",
    "for i in random_query_index_list:\n",
    "    print(\"query \",i,\": \\t\",prec[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def average_prec(k_max,_ranking_pairs):\n",
    "    temp_list = []\n",
    "    ap_list = [[0,0] for i in range(len(_ranking_pairs))]\n",
    "    for k in range(1,k_max+1):\n",
    "        temp_list = precision(k,_ranking_pairs)\n",
    "        #print(k,temp_list)\n",
    "        for index,item in enumerate(temp_list):\n",
    "            ap_list[index] = (np.array(ap_list[index])+np.array(item)).tolist()\n",
    "    #print(ap_list)\n",
    "    for item in ap_list:\n",
    "        item[:] = [i / k_max for i in item]\n",
    "    return ap_list       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query  27978 : \t ('P', ('HR', 'HR', 'N', 'R', 'HR')) \t 0.813 \t ('E', ('HR', 'R', 'HR', 'HR', 'R')) \t 1.000\n",
      "query  39729 : \t ('P', ('R', 'N', 'N', 'N', 'R')) \t 0.473 \t ('E', ('N', 'HR', 'HR', 'HR', 'R')) \t 0.332\n",
      "query  13126 : \t ('P', ('N', 'R', 'N', 'N', 'N')) \t 0.128 \t ('E', ('N', 'R', 'N', 'HR', 'R')) \t 0.197\n",
      "query  8253 : \t ('P', ('N', 'HR', 'N', 'R', 'HR')) \t 0.197 \t ('E', ('N', 'N', 'R', 'R', 'HR')) \t 0.121\n",
      "query  56453 : \t ('P', ('R', 'R', 'HR', 'R', 'R')) \t 1.000 \t ('E', ('N', 'R', 'HR', 'HR', 'HR')) \t 0.332\n",
      "query  28606 : \t ('P', ('HR', 'HR', 'HR', 'N', 'HR')) \t 0.902 \t ('E', ('N', 'HR', 'R', 'HR', 'R')) \t 0.332\n",
      "query  27157 : \t ('P', ('HR', 'HR', 'N', 'HR', 'HR')) \t 0.813 \t ('E', ('N', 'HR', 'R', 'R', 'R')) \t 0.332\n",
      "query  19229 : \t ('P', ('N', 'R', 'R', 'R', 'HR')) \t 0.332 \t ('E', ('HR', 'HR', 'N', 'HR', 'HR')) \t 0.813\n",
      "query  16530 : \t ('P', ('N', 'R', 'HR', 'HR', 'R')) \t 0.332 \t ('E', ('N', 'R', 'R', 'HR', 'N')) \t 0.300\n",
      "query  22294 : \t ('P', ('HR', 'N', 'HR', 'N', 'R')) \t 0.585 \t ('E', ('N', 'HR', 'N', 'HR', 'N')) \t 0.173\n"
     ]
    }
   ],
   "source": [
    "k_max = 5\n",
    "avg_prec = average_prec(k_max,ranking_pairs)\n",
    "\n",
    "random_query_index_list = random.sample(range(len(ranking_pairs)), 10)\n",
    "for i in random_query_index_list:\n",
    "    print(\"query \",i,\": \\t\",ranking_pairs[i][0], \"\\t\",'%.3f'%avg_prec[i][0],'\\t',ranking_pairs[i][1],'\\t','%.3f'%avg_prec[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "explanation/analysis?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------calc ideal dcg based on ground truth [10 HR, 10 R, 10 N]\n",
    "def calc_ideal_dgc(k):\n",
    "    idcg_rel = [2]*10 + [1]*10 + [0]*10\n",
    "    idcg = 0\n",
    "    for index in range(min(k, len(idcg_rel))):\n",
    "        idcg += ((2**idcg_rel[index]) - 1)/(math.log2(2+index))\n",
    "    return idcg\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I used the second formula of DCG from slide 6 of http://www.cs.cornell.edu/courses/cs4300/2013fa/lectures/evaluation-1-4pp.pdf\n",
    "#nDCG is a way to calculate this measure across many independent ____queries____(http://curtis.ml.cmu.edu/w/courses/index.php/Normalized_discounted_cumulative_gain)\n",
    "#Normalize DCG at rank n by the DCG value at rank n of the ideal ranking(stanford slide)\n",
    "\n",
    "import math\n",
    "def ndcg(max_k,query_index,method):    \n",
    "    all_dcg_k = []\n",
    "    \n",
    "    #--------------------make dcg list for all k-----------------------------\n",
    "    for r in range(0,max_k):\n",
    "        if ranking_pairs[query_index][method][1][r] == 'HR':\n",
    "            rel_num = 2\n",
    "        elif ranking_pairs[query_index][method][1][r] == 'R':\n",
    "            rel_num = 1\n",
    "        else:\n",
    "            rel_num = 0\n",
    "            \n",
    "        if len(all_dcg_k) > 0:   \n",
    "            all_dcg_k.append((((2**rel_num) - 1)/(math.log2(2+r))) + all_dcg_k[-1])\n",
    "        else:\n",
    "            all_dcg_k.append(((2**rel_num) - 1)/(math.log2(2+r)))\n",
    "            \n",
    "            \n",
    "    #----------------calculate ideal dcg------------------------------------      \n",
    "    idcg = calc_ideal_dgc(max_k)\n",
    "        \n",
    "    #--------------------------convert dcg to ndcg--------------------------   \n",
    "    if(idcg == 0):\n",
    "        all_dcg_k[:] = [0 for x in all_dcg_k]\n",
    "    else:\n",
    "        all_dcg_k[:] = [x / idcg for x in all_dcg_k]\n",
    "     \n",
    "    return all_dcg_k\n",
    "\n",
    "#-----------------------call ndcg function for all 59000 queries------------------\n",
    "ndcg_list = []\n",
    "max_k = 5\n",
    "for i in range(len(ranking_pairs)):\n",
    "    p_dcg = ndcg(max_k,i,0)\n",
    "    e_dcg = ndcg(max_k,i,1)\n",
    "    ndcg_list.append((p_dcg[-1],e_dcg[-1]))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 0.13120507751234178),\n",
       " (0.0, 0.04373502583744726),\n",
       " (0.0, 0.14606834984270645),\n",
       " (0.0, 0.27727342735504823),\n",
       " (0.0, 0.1898033756801537),\n",
       " (0.0, 0.04868944994756881),\n",
       " (0.0, 0.17989452745991058),\n",
       " (0.0, 0.09242447578501607),\n",
       " (0.0, 0.16958010263680803),\n",
       " (0.0, 0.30078518014914984)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query 30263 :\n",
      "('P', ('HR', 'HR', 'HR', 'R', 'R')) \t 0.8151510484299677\n",
      "('E', ('N', 'N', 'HR', 'HR', 'HR')) \t 0.4468535299918563 \n",
      "\n",
      "query 15724 :\n",
      "('P', ('N', 'R', 'HR', 'N', 'HR')) \t 0.37211393506065904\n",
      "('E', ('R', 'R', 'R', 'HR', 'N')) \t 0.3869772073910237 \n",
      "\n",
      "query 37459 :\n",
      "('P', ('HR', 'R', 'R', 'N', 'HR')) \t 0.5982207385764031\n",
      "('E', ('R', 'HR', 'N', 'HR', 'N')) \t 0.473108016335106 \n",
      "\n",
      "query 51051 :\n",
      "('P', ('R', 'HR', 'R', 'HR', 'N')) \t 0.5296347172140421\n",
      "('E', ('R', 'R', 'HR', 'R', 'HR')) \t 0.5338567867660998 \n",
      "\n",
      "query 7612 :\n",
      "('P', ('N', 'HR', 'N', 'HR', 'HR')) \t 0.49125969208957576\n",
      "('E', ('HR', 'HR', 'N', 'HR', 'N')) \t 0.6992148198508501 \n",
      "\n",
      "query 47752 :\n",
      "('P', ('R', 'HR', 'N', 'R', 'R')) \t 0.4194641422774156\n",
      "('E', ('N', 'R', 'R', 'R', 'N')) \t 0.176544905738014 \n",
      "\n",
      "query 48673 :\n",
      "('P', ('R', 'HR', 'HR', 'HR', 'N')) \t 0.642688118971914\n",
      "('E', ('N', 'HR', 'N', 'HR', 'HR')) \t 0.49125969208957576 \n",
      "\n",
      "query 13556 :\n",
      "('P', ('N', 'R', 'N', 'N', 'R')) \t 0.11506378074895644\n",
      "('E', ('N', 'N', 'N', 'HR', 'HR')) \t 0.27727342735504823 \n",
      "\n",
      "query 17848 :\n",
      "('P', ('N', 'R', 'R', 'N', 'HR')) \t 0.259060533302787\n",
      "('E', ('R', 'N', 'R', 'HR', 'N')) \t 0.3156484524795145 \n",
      "\n",
      "query 23617 :\n",
      "('P', ('HR', 'N', 'HR', 'R', 'HR')) \t 0.6886348353703347\n",
      "('E', ('HR', 'R', 'HR', 'N', 'N')) \t 0.5800690628219334 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_query_index_list = random.sample(range(len(ranking_pairs)), 10)\n",
    "for index in random_query_index_list:\n",
    "    print(\"query %d\"%index,\":\")\n",
    "    print(ranking_pairs[index][0], \"\\t\", ndcg_list[index][0])\n",
    "    print(ranking_pairs[index][1], \"\\t\",ndcg_list[index][1],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of all ERR values for P & E (should be equal):\n",
      " [ 0.55613292  0.55613292]\n",
      "mean of a random sample of 100 pairs:\n",
      " [ 0.55670573  0.55996615]\n",
      "[(0.0, 0.15),\n",
      " (0.0, 0.1),\n",
      " (0.0, 0.1875),\n",
      " (0.0, 0.225),\n",
      " (0.0, 0.2),\n",
      " (0.0, 0.125),\n",
      " (0.0, 0.175),\n",
      " (0.0, 0.175),\n",
      " (0.0, 0.25),\n",
      " (0.0, 0.2875)]\n"
     ]
    }
   ],
   "source": [
    "# ERR\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def numerical(relevance_sequence):\n",
    "    \"\"\"\n",
    "    Convert a relevance grade sequence to a sequence of numerical \n",
    "    values, based on the relevance grades given.\n",
    "    E.g. ['HR', 'HR', 'HR', 'R', 'N'] returns [2, 2, 2, 1, 0]\n",
    "    \"\"\"\n",
    "    numerical_relevance_sequence = [1 if grade == 'R' \\\n",
    "                                    else 2 if grade == 'HR' else 0 \\\n",
    "                                    for grade in relevance_sequence]\n",
    "    return numerical_relevance_sequence\n",
    "\n",
    "def R_function(g, g_max):\n",
    "    return (2**g - 1)/(2**g_max)\n",
    "\n",
    "def ERR(relevance_sequence):\n",
    "    \"\"\"\n",
    "    Compute the ERR based on Algorithm 2 in \n",
    "    https://pdfs.semanticscholar.org/7e3c/f6492128f915112ca01dcb77c766129e65cb.pdf\n",
    "    \"\"\"\n",
    "    p = 1\n",
    "    ERR = 0\n",
    "    n = len(relevance_sequence)\n",
    "    g_max = max(relevance_sequence)\n",
    "    \n",
    "    for r in range(1, n + 1):\n",
    "        g = relevance_sequence[r - 1]\n",
    "        R = R_function(g, g_max)\n",
    "        ERR = ERR + p * (R/r)\n",
    "        p = p * (1 - R)\n",
    "    return ERR\n",
    "\n",
    "ERR_list = []\n",
    "for P, E in ranking_pairs:\n",
    "    ERR_P = ERR(numerical(P[1]))\n",
    "    ERR_E = ERR(numerical(E[1]))\n",
    "    ERR_list.append((ERR_P, ERR_E))\n",
    "    \n",
    "print('mean of all ERR values for P & E (should be equal):\\n', \n",
    "      np.mean(ERR_list, axis=0))\n",
    "print('mean of a random sample of 100 pairs:\\n', \n",
    "      np.mean(random.sample(ERR_list, 100), axis=0))\n",
    "pprint(ERR_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: <font color='darkred'>Calculate the ùõ•measure *(0 points)*</font>\n",
    "For the three measures and all *P* and *E* ranking pairs constructed above calculate the difference: ùõ•measure = measure<sub>E</sub>-measure<sub>P</sub>. Consider only those pairs for which *E* outperforms *P*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def measure_diff(measure_list, _ranking_pairs, query_index_list):\n",
    "    for i in query_index_list:\n",
    "        diff = measure_list[i][1] - measure_list[i][0]\n",
    "        if diff > 0:\n",
    "            print(\"query\",i,\": \",'\\t',_ranking_pairs[i][0],'\\t',_ranking_pairs[i][1],'\\t','%.3f'%(diff))\n",
    "        else:\n",
    "            print(\"query\",i,\": \",'\\t',\"less than 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ùõ•measure for AP: \n",
      "query 4829 :  \t ('P', ('N', 'N', 'R', 'N', 'HR')) \t ('E', ('R', 'R', 'HR', 'R', 'HR')) \t 0.932\n",
      "query 30486 :  \t less than 0\n",
      "query 25467 :  \t less than 0\n",
      "query 48939 :  \t less than 0\n",
      "query 29849 :  \t less than 0\n",
      "query 33872 :  \t ('P', ('HR', 'R', 'N', 'HR', 'HR')) \t ('E', ('R', 'R', 'R', 'N', 'HR')) \t 0.089\n",
      "query 17699 :  \t less than 0\n",
      "query 42469 :  \t ('P', ('R', 'N', 'HR', 'HR', 'HR')) \t ('E', ('HR', 'HR', 'HR', 'N', 'R')) \t 0.241\n",
      "query 24746 :  \t less than 0\n",
      "query 46774 :  \t less than 0\n",
      "-------------------------------------------------------------------------------------------------------\n",
      " ùõ•measure for nDCG: \n",
      "query 4829 :  \t ('P', ('N', 'N', 'R', 'N', 'HR')) \t ('E', ('R', 'R', 'HR', 'R', 'HR')) \t 0.346\n",
      "query 30486 :  \t less than 0\n",
      "query 25467 :  \t less than 0\n",
      "query 48939 :  \t less than 0\n",
      "query 29849 :  \t less than 0\n",
      "query 33872 :  \t less than 0\n",
      "query 17699 :  \t ('P', ('N', 'R', 'R', 'N', 'HR')) \t ('E', ('N', 'HR', 'N', 'R', 'N')) \t 0.004\n",
      "query 42469 :  \t ('P', ('R', 'N', 'HR', 'HR', 'HR')) \t ('E', ('HR', 'HR', 'HR', 'N', 'R')) \t 0.207\n",
      "query 24746 :  \t less than 0\n",
      "query 46774 :  \t less than 0\n",
      "-------------------------------------------------------------------------------------------------------\n",
      " ùõ•measure for ERR: \n",
      "query 4829 :  \t ('P', ('N', 'N', 'R', 'N', 'HR')) \t ('E', ('R', 'R', 'HR', 'R', 'HR')) \t 0.313\n",
      "query 30486 :  \t less than 0\n",
      "query 25467 :  \t less than 0\n",
      "query 48939 :  \t less than 0\n",
      "query 29849 :  \t less than 0\n",
      "query 33872 :  \t less than 0\n",
      "query 17699 :  \t ('P', ('N', 'R', 'R', 'N', 'HR')) \t ('E', ('N', 'HR', 'N', 'R', 'N')) \t 0.119\n",
      "query 42469 :  \t ('P', ('R', 'N', 'HR', 'HR', 'HR')) \t ('E', ('HR', 'HR', 'HR', 'N', 'R')) \t 0.380\n",
      "query 24746 :  \t less than 0\n",
      "query 46774 :  \t less than 0\n"
     ]
    }
   ],
   "source": [
    "random_query_index_list = random.sample(range(len(ranking_pairs)), 10)\n",
    "\n",
    "#for ap:\n",
    "print(\" ùõ•measure for AP: \")\n",
    "measure_diff(avg_prec, ranking_pairs, random_query_index_list)\n",
    "print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "#for nDCG:\n",
    "print(\" ùõ•measure for nDCG: \")\n",
    "measure_diff(ndcg_list, ranking_pairs, random_query_index_list)\n",
    "print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "#for ERR:\n",
    "print(\" ùõ•measure for ERR: \")\n",
    "measure_diff(ERR_list, ranking_pairs, random_query_index_list)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#for ap:\n",
    "#for i in range(len(ranking_pairs)):\n",
    "counter = 0\n",
    "i = 0\n",
    "while i < len(ranking_pairs) and counter < 100:\n",
    "    i += 1\n",
    "    diff = avg_prec[i][1]-avg_prec[i][0]\n",
    "    if(avg_prec[i][1]-avg_prec[i][0] > 0):\n",
    "        counter += 1\n",
    "        print(i, ranking_pairs[i][0],'\\t',ranking_pairs[i][1],'\\t','%.3f'%(avg_prec[i][1]-avg_prec[i][0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#for nDCG:\n",
    "#for i in range(len(ranking_pairs)):\n",
    "counter = 0\n",
    "i = 0\n",
    "while i < len(ranking_pairs) and counter < 100:\n",
    "    i += 1\n",
    "    if(ndcg_list[i][1]-ndcg_list[i][0] > 0):\n",
    "        counter += 1\n",
    "        print(ranking_pairs[i][0],'\\t',ranking_pairs[i][1],'\\t','%.3f'%(ndcg_list[i][1]-ndcg_list[i][0]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#for ERR:\n",
    "#for i in range(len(ranking_pairs)):\n",
    "counter = 0\n",
    "i = 0\n",
    "while i < len(ranking_pairs) and counter < 100:\n",
    "    i += 1\n",
    "    if(ERR_list[i][1]-ERR_list[i][0] > 0):\n",
    "        counter += 1\n",
    "        print(ranking_pairs[i][0],'\\t',ranking_pairs[i][1],'\\t','%.3f'%(ERR_list[i][1]-ERR_list[i][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "explanation/analysis?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='purple'>[Based on Lecture 2]</font>\n",
    "### Step 4: <font color='darkred'>Implement Interleaving *(15 points)*</font>\n",
    "Implement 2 interleaving algorithms: (1) Team-Draft Interleaving OR Balanced Interleaving, AND (2), Probabilistic Interleaving. The interleaving algorithms should (a) given two rankings of relevance interleave them into a single ranking, and (b) given the users clicks on the interleaved ranking assign credit to the algorithms that produced the rankings.\n",
    "\n",
    "(Note 4: Note here that as opposed to a normal interleaving experiment where rankings consists of urls or docids, in our case the rankings consist of relevance labels. Hence in this case (a) you will assume that E and P return different documents, (b) the interleaved ranking will also be a ranking of labels.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Team-Draft Interleaving\n",
    "#P and E as two interleaving lists\n",
    "\n",
    "def team_draft_interleaving(_ranking_pairs):\n",
    "    all_I = []\n",
    "    for i in range(len(_ranking_pairs)):\n",
    "        A = _ranking_pairs[i][0][1]\n",
    "        B = _ranking_pairs[i][1][1]\n",
    "        team_A = []\n",
    "        team_B = []\n",
    "        I = []\n",
    "        while len(team_A) < len(A) or len(team_B) < len(B):\n",
    "            RandBit = random.getrandbits(1)\n",
    "            #pick from A\n",
    "            if len(team_A) < len(team_B) or (len(team_A) == len(team_B) and RandBit == 1):\n",
    "                I.append((A[len(team_A)],0))\n",
    "                team_A.append((I[-1],0))        \n",
    "            else:\n",
    "                #pick from B\n",
    "                I.append((B[len(team_B)],1))\n",
    "                team_B.append(I[-1])\n",
    "        all_I.append(I)\n",
    "    return all_I\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_winner(_click,_all_I):\n",
    "    winner_list = []\n",
    "    for query_index in range(len(_all_I)):\n",
    "        h_a = 0\n",
    "        h_b = 0\n",
    "        for index, item in enumerate(_all_I[query_index]):\n",
    "            if _click[query_index][index] > 0:\n",
    "                if item[1] == 0:\n",
    "                    h_a += 1\n",
    "                if item[1] == 1:\n",
    "                    h_b += 1\n",
    "        if(h_a > h_b):\n",
    "            winner_list.append('P')\n",
    "        if(h_a == h_b):\n",
    "            winner_list.append('Equal')\n",
    "        else:\n",
    "            winner_list.append('E')\n",
    "    return winner_list\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def find_winner(_click,_all_I):\n",
    "    winner_list = []\n",
    "    winner_percent = 0.0\n",
    "    for query_index in range(len(_all_I)):\n",
    "        h_a = 0.0\n",
    "        h_b = 0.0\n",
    "        for index, item in enumerate(_all_I[query_index]):\n",
    "            if _click[query_index][index] > 0:\n",
    "                if item[1] == 0:\n",
    "                    h_a += 1\n",
    "                if item[1] == 1:\n",
    "                    h_b += 1\n",
    "        #print('H_a = ',h_a,'H_b = ', h_b,'\\n')\n",
    "        total = h_a + h_b\n",
    "        if(h_a > h_b):\n",
    "            winner_list.append(('P', winner_percent))\n",
    "        if(h_a == h_b):\n",
    "            winner_list.append(('Equal', winner_percent))\n",
    "        else:\n",
    "            winner_list.append(('E', winner_percent))\n",
    "            \n",
    "    return winner_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58806\n",
      "58806\n",
      "[[('N', 0), ('N', 1), ('N', 0), ('N', 1), ('N', 1), ('N', 0), ('N', 1), ('N', 0), ('HR', 1), ('N', 0)], [('N', 1), ('N', 0), ('N', 1), ('N', 0), ('N', 1), ('N', 0), ('N', 0), ('N', 1), ('N', 0), ('R', 1)], [('N', 0), ('N', 1), ('N', 1), ('N', 0), ('N', 1), ('N', 0), ('HR', 1), ('N', 0), ('N', 0), ('N', 1)], [('N', 1), ('N', 0), ('N', 1), ('N', 0), ('N', 1), ('N', 0), ('HR', 1), ('N', 0), ('HR', 1), ('N', 0)], [('N', 0), ('N', 1), ('N', 0), ('N', 1), ('N', 0), ('N', 1), ('N', 0), ('HR', 1), ('R', 1), ('N', 0)], [('N', 0), ('N', 1), ('N', 0), ('N', 1), ('N', 0), ('N', 1), ('N', 0), ('R', 1), ('N', 0), ('N', 1)], [('N', 1), ('N', 0), ('N', 0), ('N', 1), ('N', 0), ('N', 1), ('N', 0), ('R', 1), ('HR', 1), ('N', 0)], [('N', 0), ('N', 1), ('N', 0), ('N', 1), ('N', 1), ('N', 0), ('N', 0), ('R', 1), ('N', 0), ('R', 1)], [('N', 0), ('N', 1), ('N', 1), ('N', 0), ('N', 0), ('HR', 1), ('N', 0), ('N', 1), ('N', 0), ('N', 1)], [('N', 1), ('N', 0), ('N', 1), ('N', 0), ('HR', 1), ('N', 0), ('N', 0), ('N', 1), ('N', 0), ('HR', 1)]]\n",
      "[[0, 1, 0, 0, 1, 0, 1, 0, 1, 0], [0, 1, 0, 0, 1, 0, 1, 0, 1, 0], [0, 1, 0, 0, 1, 0, 1, 0, 1, 0], [0, 1, 0, 0, 1, 0, 1, 0, 1, 0], [0, 1, 0, 0, 1, 0, 1, 0, 1, 0], [0, 1, 0, 0, 1, 0, 1, 0, 1, 0], [0, 1, 0, 0, 1, 0, 1, 0, 1, 0], [0, 1, 0, 0, 1, 0, 1, 0, 1, 0], [0, 1, 0, 0, 1, 0, 1, 0, 1, 0], [0, 1, 0, 0, 1, 0, 1, 0, 1, 0]]\n",
      "58806\n",
      "['E', 'P', 'E', 'E', 'E', 'Equal', 'P', 'E', 'P', 'E', 'Equal', 'P', 'E', 'P', 'E', 'Equal', 'E', 'Equal', 'P', 'E', 'P', 'E', 'P', 'E', 'P', 'E', 'Equal', 'Equal', 'Equal', 'E', 'E', 'P', 'E', 'E', 'Equal', 'Equal', 'P', 'E', 'Equal', 'Equal', 'E', 'Equal', 'Equal', 'Equal', 'P', 'E', 'Equal', 'P', 'E', 'Equal', 'Equal', 'P', 'E', 'E', 'Equal', 'E', 'E', 'E', 'Equal', 'E', 'Equal', 'P', 'E', 'Equal', 'E', 'P', 'E', 'P', 'E', 'E', 'P', 'E', 'E', 'Equal', 'Equal', 'E', 'E', 'Equal', 'E', 'P', 'E', 'Equal', 'Equal', 'Equal', 'E', 'E', 'P', 'E', 'Equal', 'P', 'E', 'Equal', 'P', 'E', 'Equal', 'Equal', 'P', 'E', 'P', 'E']\n"
     ]
    }
   ],
   "source": [
    "print(len(ranking_pairs))\n",
    "all_I = team_draft_interleaving(ranking_pairs)    \n",
    "click_list = [[random.getrandbits(1) for i in range(10)]]*len(all_I)\n",
    "pprint(len(all_I))\n",
    "print(all_I[:10])\n",
    "print(click_list[:10])\n",
    "print(len(click_list))\n",
    "winner_list = find_winner(click_list, all_I)\n",
    "print(winner_list[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "explanation/analysis?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='purple'>[Based on Lecture 3]</font>\n",
    "### Step 5: <font color='darkred'>Implement User Clicks Simulation *(15 points)*</font>\n",
    "Having interleaved all the ranking pairs an online experiment could be ran. However, given that we do not have any users (and the entire homework is a big simulation) we will simulate user clicks.\n",
    "\n",
    "We have considered a number of click models including:\n",
    "1. Random Click Model (RCM)\n",
    "2. Position-Based Model (PBM)\n",
    "3. Simple Dependent Click Model (SDCM)\n",
    "4. Simple Dynamic Bayesian Network (SDBN)\n",
    "\n",
    "Consider two different click models, (a) the Random Click Model (RCM), and (b) one out of the remaining 3 aforementioned models. The parameters of some of these models can be estimated using the Maximum Likelihood Estimation (MLE) method, while others require using the Expectation-Maximization (EM) method. Implement the two models so that (a) there is a method that learns the parameters of the model given a set of training data, (b) there is a method that predicts the click probability given a ranked list of relevance labels, (c) there is a method that decides - stochastically - whether a document is clicked based on these probabilities.\n",
    "\n",
    "Having implemented the two click models, estimate the model parameters using the Yandex Click Log [[file]](https://drive.google.com/file/d/1tqMptjHvAisN1CJ35oCEZ9_lb0cEJwV0/view).\n",
    "\n",
    "(Note 6: Do not learn the attractiveness parameter *a*<sub>uq</sub>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn_rcm_param(_train_file):\n",
    "    c = 0\n",
    "    query_num = 0.0\n",
    "    click_num = 0.0\n",
    "    for line in _train_file:\n",
    "        info = line.split()\n",
    "        #print(info[2] == 'Q')\n",
    "        if info[2] == 'Q':\n",
    "            query_num += 1\n",
    "        if info[2] == 'C':\n",
    "            click_num += 1\n",
    "\n",
    "    doc_num = query_num*10\n",
    "    _rcm_param = click_num/doc_num\n",
    "    return _rcm_param\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rcm_predict_click_prob(_I, _ru):       \n",
    "    return [_ru]*(len(_I))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from collections import Counter\n",
    "def analyse_train_file(_train_file):\n",
    "    Q_cnt= Counter()\n",
    "    \n",
    "    for line in _train_file:\n",
    "        info = line.split()\n",
    "\n",
    "        if info[2] == 'Q':\n",
    "            Q_cnt[info[3]] += 1\n",
    "\n",
    "    \n",
    "    return Q_cnt\n",
    "\n",
    "train_file = open(\"YandexRelPredChallenge.txt\",\"r\")\n",
    "Q_cnt = analyse_train_file(train_file)\n",
    "print(Q_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SDBM:\n",
    "\"\"\"\n",
    "we do not know if a user stopped examining documents because they \n",
    "were satisfied by the last-clicked document, or because they were not attracted by any document\n",
    "below it. However, if a user is assumed to be satisfied by the last-clicked document, then A and S do\n",
    "become observable.\n",
    "\"\"\"\n",
    "def learn_sdbm_param(_train_file):\n",
    "    last_session = '0'\n",
    "    satisfy_num = 0.0\n",
    "    last_action = ''\n",
    "    click_num = 0.0\n",
    "    \n",
    "    for line in _train_file:\n",
    "        info = line.split()\n",
    "        \n",
    "        if info[2] == 'C':\n",
    "            click_num += 1\n",
    "        \n",
    "\n",
    "        if info[0] == last_session:\n",
    "            last_action = info[2]\n",
    "        else:\n",
    "            if last_action == 'C':                \n",
    "                satisfy_num += 1\n",
    "            last_session = info[0]\n",
    "    if last_action == 'C':                \n",
    "        satisfy_num += 1     \n",
    "\n",
    "        \n",
    "    sigma = satisfy_num/click_num  \n",
    "\n",
    "    return sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sdbm_predict_click_prob(_I, _sigma):\n",
    "    prediction = []\n",
    "    attraction = {'HR':2,'R':1,'N':0}\n",
    "    for index, item in enumerate(_I):\n",
    "        if index == 0:\n",
    "            prediction.append(attraction[item[0]])\n",
    "        else:\n",
    "            #er = prediction[index-1]*(attraction[_I[index-1]]*(1-_sigma) + (1 - attraction[_I[index-1]]))            \n",
    "            er = prediction[index-1]*(1-_sigma * attraction[_I[index-1][0]])\n",
    "            prediction.append(attraction[item[0]] * er)\n",
    "                                       \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "explanation/analysis?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: <font color='darkred'>Simulate Interleaving Experiment *(10 points)*</font>\n",
    "Having implemented the click models, it is time to run the simulated experiment.\n",
    "\n",
    "For each of interleaved ranking run N simulations for each one of the click models implemented and measure the proportion *p* of wins for E.\n",
    "(Note 7: Some of the models above include an attractiveness parameter *a*<sub>uq</sub>. Use the relevance label to assign this parameter by setting *a*<sub>uq</sub> for a document u in the ranked list accordingly. (See [Click Models for Web Search](http://clickmodels.weebly.com/uploads/5/2/2/5/52257029/mc2015-clickmodels.pdf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13445559411047547\n",
      "[[('N', 0), ('N', 1), ('N', 1), ('N', 0), ('N', 0), ('N', 1), ('N', 1), ('N', 0), ('HR', 1), ('N', 0)], [('N', 0), ('N', 1), ('N', 0), ('N', 1), ('N', 0), ('N', 1), ('N', 1), ('N', 0), ('R', 1), ('N', 0)], [('N', 0), ('N', 1), ('N', 1), ('N', 0), ('N', 1), ('N', 0), ('N', 0), ('HR', 1), ('N', 1), ('N', 0)], [('N', 0), ('N', 1), ('N', 1), ('N', 0), ('N', 1), ('N', 0), ('HR', 1), ('N', 0), ('N', 0), ('HR', 1)], [('N', 0), ('N', 1), ('N', 0), ('N', 1), ('N', 1), ('N', 0), ('HR', 1), ('N', 0), ('R', 1), ('N', 0)], [('N', 0), ('N', 1), ('N', 1), ('N', 0), ('N', 0), ('N', 1), ('R', 1), ('N', 0), ('N', 1), ('N', 0)], [('N', 0), ('N', 1), ('N', 0), ('N', 1), ('N', 1), ('N', 0), ('N', 0), ('R', 1), ('HR', 1), ('N', 0)], [('N', 0), ('N', 1), ('N', 0), ('N', 1), ('N', 0), ('N', 1), ('R', 1), ('N', 0), ('R', 1), ('N', 0)], [('N', 1), ('N', 0), ('N', 1), ('N', 0), ('N', 0), ('HR', 1), ('N', 1), ('N', 0), ('N', 1), ('N', 0)], [('N', 0), ('N', 1), ('N', 1), ('N', 0), ('HR', 1), ('N', 0), ('N', 1), ('N', 0), ('HR', 1), ('N', 0)]]\n",
      "['Equal', 'Equal', 'Equal', 'Equal', 'Equal', 'Equal', 'Equal', 'Equal', 'Equal', 'Equal', 'Equal', 'Equal', 'Equal', 'Equal', 'Equal', 'Equal', 'Equal', 'Equal', 'Equal', 'Equal']\n"
     ]
    }
   ],
   "source": [
    "all_I = team_draft_interleaving(ranking_pairs)   \n",
    "\n",
    "train_file = open(\"YandexRelPredChallenge.txt\",\"r\")\n",
    "ru = learn_rcm_param(train_file)\n",
    "print(ru)\n",
    "\n",
    "rcm_click_list = [rcm_predict_click_prob(all_I[i],ru) for i in range(len(all_I))]\n",
    "\n",
    "print(all_I[:10])\n",
    "\n",
    "winner_list = find_winner(rcm_click_list, all_I)\n",
    "print(winner_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_I = team_draft_interleaving(ranking_pairs)   \n",
    "\n",
    "train_file = open(\"YandexRelPredChallenge.txt\",\"r\")\n",
    "sigma = learn_sdbm_param(train_file)\n",
    "print(sigma)\n",
    "print(all_I[58800])\n",
    "print(sdbm_predict_click_prob(all_I[58800],sigma))\n",
    "\n",
    "sdbm_click_list = [sdbm_predict_click_prob(all_I[i],sigma) for i in range(len(all_I))]\n",
    "\n",
    "winner_list = find_winner(sdbm_click_list, all_I)\n",
    "#print(winner_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(winner_list[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "explanation/analysis?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: <font color='darkred'>Results and Analysis *(30 points)*</font>\n",
    "Compare the results of the offline experiments (i.e. the values of the ùõ•measure) with the results of the online experiment (i.e. proportion of wins), analyze them and reach your conclusions regarding their agreement.\n",
    "* Use easy to read and comprehend visuals to demonstrate the results;\n",
    "* Analyze the results on the basis of\n",
    "    * the evaluation measure used,\n",
    "    * the interleaving method used,\n",
    "    * the click model used.\n",
    "* Report and ground your conclusions.\n",
    "\n",
    "(Note 8: This is the place where you need to demonstrate your deeper understanding of what you have implemented so far; hence the large number of points assigned. Make sure you clearly do that so that the examiner of your work can grade it accordingly.)\n",
    "\n",
    "<u>Yandex Click Log File</u>:\n",
    "\n",
    "The dataset includes user sessions extracted from Yandex logs, with queries, URL rankings and clicks. To allay privacy concerns the user data is fully anonymized. So, only meaningless numeric IDs of queries, sessions, and URLs are released. The queries are grouped only by sessions and no user IDs are provided. The dataset consists of several parts. Logs represent a set of rows, where each row represents one of the possible user actions: query or click.\n",
    "\n",
    "In the case of a Query:\n",
    "\n",
    "    SessionID TimePassed TypeOfAction QueryID RegionID ListOfURLs\n",
    "\n",
    "\n",
    "In the case of a Click:\n",
    "\n",
    "    SessionID TimePassed TypeOfAction URLID\n",
    "\n",
    "\n",
    "* `SessionID` - the unique identifier of the user session.\n",
    "* `TimePassed` - the time elapsed since the beginning of the current session in standard time units.\n",
    "* `TypeOfAction` - type of user action. This may be either a query (Q), or a click (C).\n",
    "* `QueryID` - the unique identifier of the request.\n",
    "* `RegionID` - the unique identifier of the country from which a given query. This identifier may take four values.\n",
    "* `URLID` - the unique identifier of the document.\n",
    "* `ListOfURLs` - the list of documents from left to right as they have been shown to users on the page extradition Yandex (top to bottom).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightyellow\">\n",
    "explanation/analysis?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
